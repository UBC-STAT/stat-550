[
  {
    "objectID": "schedule/slides/unit-tests.html#section",
    "href": "schedule/slides/unit-tests.html#section",
    "title": "UBC Stat550",
    "section": "Unit tests and avoiding 🪲🪲",
    "text": "Unit tests and avoiding 🪲🪲\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#i-urge-you-to-consult",
    "href": "schedule/slides/unit-tests.html#i-urge-you-to-consult",
    "title": "UBC Stat550",
    "section": "I urge you to consult:",
    "text": "I urge you to consult:\nCarnegie Mellon’s 36-750 Notes\nThank you Alex and Chris for the heavy lifting."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#bugs-happen.-all.-the.-time.",
    "href": "schedule/slides/unit-tests.html#bugs-happen.-all.-the.-time.",
    "title": "UBC Stat550",
    "section": "Bugs happen. All. The. Time.",
    "text": "Bugs happen. All. The. Time.\n\nthe crash of the Mars Climate Orbiter (1998),\na failure of the national telephone network (1990),\na deadly medical device (1985, 2000),\na massive Northeastern blackout (2003),\nthe Heartbleed, Goto Fail, Shellshock exploits (2012–2014),\na 15-year-old fMRI analysis software bug that inflated significance levels (2015),\n\n\nIt is easy to write lots of code.\nBut are we sure it’s doing the right things?\n\n\n\n\n\n\nImportant\n\n\nEffective testing tries to help."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#a-common-interactive-workflow",
    "href": "schedule/slides/unit-tests.html#a-common-interactive-workflow",
    "title": "UBC Stat550",
    "section": "A Common (Interactive) Workflow",
    "text": "A Common (Interactive) Workflow\n\nWrite a function.\nTry some reasonable values at the REPL to check that it works.\nIf there are problems, maybe insert some print statements, and modify the function.\nRepeat until things seem fine.\n\n(REPL == Read-Eval-Print-Loop, the console, or Jupyter NB)\n\nThis tends to result in lots of bugs.\nLater on, you forget which values you tried, whether they failed, how you fixed them.\nSo you make a change and maybe or maybe not try some again."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#step-1-write-functions",
    "href": "schedule/slides/unit-tests.html#step-1-write-functions",
    "title": "UBC Stat550",
    "section": "Step 1 — write functions",
    "text": "Step 1 — write functions\n\n\n\n\n\n\nWrite functions.\nLots of them.\n\n\n\n👍 Functions are testable\n👎 Scripts are not\nIt’s easy to alter the arguments and see “what happens”\nThere’s less ability to screw up environments.\n\nI’m going to mainly describe R, but the logic is very similar (if not the syntax) for python, C++, and Julia"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#understanding-signatures",
    "href": "schedule/slides/unit-tests.html#understanding-signatures",
    "title": "UBC Stat550",
    "section": "Understanding signatures",
    "text": "Understanding signatures\n\nsig(lm)\n\nfn &lt;- function(formula, data, subset, weights, na.action, method = \"qr\", model\n  = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts =\n  NULL, offset, ...)\n\nsig(`+`)\n\nfn &lt;- function(e1, e2)\n\nsig(dplyr::filter)\n\nfn &lt;- function(.data, ..., .by = NULL, .preserve = FALSE)\n\nsig(stats::filter)\n\nfn &lt;- function(x, filter, method = c(\"convolution\", \"recursive\"), sides = 2,\n  circular = FALSE, init = NULL)\n\nsig(rnorm)\n\nfn &lt;- function(n, mean = 0, sd = 1)"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#these-are-all-the-same",
    "href": "schedule/slides/unit-tests.html#these-are-all-the-same",
    "title": "UBC Stat550",
    "section": "These are all the same",
    "text": "These are all the same\n\nset.seed(12345)\nrnorm(3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(3, 0, 1)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(sd = 1, n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\nFunctions can have default values.\nYou may, but don’t have to, name the arguments\nIf you name them, you can pass them out of order (but you shouldn’t)."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#outputs-vs.-side-effects",
    "href": "schedule/slides/unit-tests.html#outputs-vs.-side-effects",
    "title": "UBC Stat550",
    "section": "Outputs vs. Side effects",
    "text": "Outputs vs. Side effects\n\n\n\nSide effects are things a function does, outputs can be assigned to variables\nA good example is the hist function\nYou have probably only seen the side effect which is to plot the histogram\n\n\nmy_histogram &lt;- hist(rnorm(1000))\n\n\n\n\n\n\n\n\n\n\n\nstr(my_histogram)\n\nList of 6\n $ breaks  : num [1:14] -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 ...\n $ counts  : int [1:13] 4 21 41 83 138 191 191 182 74 43 ...\n $ density : num [1:13] 0.008 0.042 0.082 0.166 0.276 0.382 0.382 0.364 0.148 0.086 ...\n $ mids    : num [1:13] -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ...\n $ xname   : chr \"rnorm(1000)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\nclass(my_histogram)\n\n[1] \"histogram\""
  },
  {
    "objectID": "schedule/slides/unit-tests.html#step-2-program-defensively-ensure-behaviour",
    "href": "schedule/slides/unit-tests.html#step-2-program-defensively-ensure-behaviour",
    "title": "UBC Stat550",
    "section": "Step 2 — program defensively, ensure behaviour",
    "text": "Step 2 — program defensively, ensure behaviour\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  x + 1\n}\n  \nincrementer(2)\n\n[1] 3\n\nincrementer(1:4)\n\n[1] 2 3 4 5\n\nincrementer(\"a\")\n\nError in x + 1: non-numeric argument to binary operator\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  stopifnot(is.numeric(x))\n  return(x + 1)\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): is.numeric(x) is not TRUE\n\n\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + 1\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): `x` must be numeric\n\nincrementer(2, -3) ## oops!\n\n[1] 3\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + inc_by\n}\nincrementer(2, -3)\n\n[1] -1"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#even-better",
    "href": "schedule/slides/unit-tests.html#even-better",
    "title": "UBC Stat550",
    "section": "Even better",
    "text": "Even better\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) cli::cli_abort(\"`x` must be numeric\")\n  if (!is.numeric(inc_by)) cli::cli_abort(\"`inc_by` must be numeric\")\n  x + inc_by\n}\nincrementer(\"a\")\n\nError in `incrementer()`:\n! `x` must be numeric\n\nincrementer(1:6, \"b\")\n\nError in `incrementer()`:\n! `inc_by` must be numeric"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#step-3-keep-track-of-behaviour-with-tests",
    "href": "schedule/slides/unit-tests.html#step-3-keep-track-of-behaviour-with-tests",
    "title": "UBC Stat550",
    "section": "Step 3 — Keep track of behaviour with tests",
    "text": "Step 3 — Keep track of behaviour with tests\n\nlibrary(testthat)\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) stop(\"`x` must be numeric\")\n  if (!is.numeric(inc_by)) stop(\"`inc_by` must be numeric\")\n  x + inc_by\n}\ntest_that(\"incrementer validates arguments\", {\n  expect_error(incrementer(\"a\"))\n  expect_equal(incrementer(1:3), 2:4)\n  expect_equal(incrementer(2, -3), -1)\n  expect_error(incrementer(1, \"b\"))\n  expect_identical(incrementer(1:3), 2:4)\n})\n\n── Failure: incrementer validates arguments ────────────────────────────────────\nincrementer(1:3) not identical to 2:4.\nObjects equal but not identical\n\n\nError:\n! Test failed"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#integers-are-trouble",
    "href": "schedule/slides/unit-tests.html#integers-are-trouble",
    "title": "UBC Stat550",
    "section": "Integers are trouble",
    "text": "Integers are trouble\n\nis.integer(2:4)\n\n[1] TRUE\n\nis.integer(incrementer(1:3))\n\n[1] FALSE\n\nexpect_identical(incrementer(1:3, 1L), 2:4)\nexpect_equal(incrementer(1:3, 1), 2:4)"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#unit-testing",
    "href": "schedule/slides/unit-tests.html#unit-testing",
    "title": "UBC Stat550",
    "section": "Unit testing",
    "text": "Unit testing\n\nA unit is a small bit of code (function, class, module, group of classes)\nA test calls the unit with a set of inputs, and checks if we get the expected output.\n\n\ngcd &lt;- function(x, na.rm = FALSE) {\n  if (na.rm) x &lt;- x[!is.na(x)]\n  if (anyNA(x)) return(NA)\n  stopifnot(is.numeric(x))\n  if (!rlang::is_integerish(x)) cli_abort(\"`x` must contain only integers.\")\n  if (length(x) == 1L) return(as.integer(x))\n  x &lt;- x[x != 0]\n  compute_gcd(x) # dispatch to a C++ function\n}\n\ntest_that(\"gcd works\", {\n  # corner cases\n  expect_identical(gcd(c(1, NA)), NA)\n  expect_identical(gcd(c(1, NA), TRUE), 1L)\n  expect_identical(gcd(c(1, 2, 4)), 1L)\n  # error\n  expect_error(gcd(1.3))\n  # function\n  expect_identical(gcd(c(2, 4, 6)), 2L)\n  expect_identical(gcd(c(2, 3, 7)), 1L)\n})"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#unit-testing-1",
    "href": "schedule/slides/unit-tests.html#unit-testing-1",
    "title": "UBC Stat550",
    "section": "Unit testing",
    "text": "Unit testing\nUnit testing consists of writing tests that are\n\nfocused on a small, low-level piece of code (a unit)\ntypically written by the programmer with standard tools\nfast to run (so can be run often, i.e. before every commit)."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#unit-testing-benefits",
    "href": "schedule/slides/unit-tests.html#unit-testing-benefits",
    "title": "UBC Stat550",
    "section": "Unit testing benefits",
    "text": "Unit testing benefits\nAmong others:\n\nExposing problems early\nMaking it easy to change (refactor) code without forgetting pieces or breaking things\nSimplifying integration of components\nProviding natural documentation of what the code should do\nDriving the design of new code."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#components-of-a-unit-testing-framework",
    "href": "schedule/slides/unit-tests.html#components-of-a-unit-testing-framework",
    "title": "UBC Stat550",
    "section": "Components of a Unit Testing Framework",
    "text": "Components of a Unit Testing Framework\n\n\n\nCollection of Assertions executed in sequence.\nExecuted in a self-contained environment.\nAny assertion fails  Test fails.\n\nEach test focuses on a single component.\nNamed so that you know what it’s doing.\n\n## See https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life\ntest_that(\"Conway's rules are correct\", {\n    # conway_rules(num_neighbors, alive?)\n    expect_true(conway_rules(3, FALSE))\n    expect_false(conway_rules(4, FALSE))\n    expect_true(conway_rules(2, TRUE))\n    ...\n})"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#a-test-suite",
    "href": "schedule/slides/unit-tests.html#a-test-suite",
    "title": "UBC Stat550",
    "section": "A test suite",
    "text": "A test suite\n\n\n\nCollection of related tests in a common context.\nPrepares the environment, cleans up after\n(loads some data, connects to database, necessary library,…)\nTest suites are run and the results reported, particularly failures, in a easy to parse and economical style.\nFor example, Python’s {unittest} can report like this\n\n\n\n$ python test/trees_test.py -v\n\ntest_crime_counts (__main__.DataTreeTest)\nEnsure Ks are consistent with num_points. ... ok\ntest_indices_sorted (__main__.DataTreeTest)\nEnsure all node indices are sorted in increasing order. ... ok\ntest_no_bbox_overlap (__main__.DataTreeTest)\nCheck that child bounding boxes do not overlap. ... ok\ntest_node_counts (__main__.DataTreeTest)\nEnsure that each node's point count is accurate. ... ok\ntest_oversized_leaf (__main__.DataTreeTest)\nDon't recurse infinitely on duplicate points. ... ok\ntest_split_parity (__main__.DataTreeTest)\nCheck that each tree level has the right split axis. ... ok\ntest_trange_contained (__main__.DataTreeTest)\nCheck that child tranges are contained in parent tranges. ... ok\ntest_no_bbox_overlap (__main__.QueryTreeTest)\nCheck that child bounding boxes do not overlap. ... ok\ntest_node_counts (__main__.QueryTreeTest)\nEnsure that each node's point count is accurate. ... ok\ntest_oversized_leaf (__main__.QueryTreeTest)\nDon't recurse infinitely on duplicate points. ... ok\ntest_split_parity (__main__.QueryTreeTest)\nCheck that each tree level has the right split axis. ... ok\ntest_trange_contained (__main__.QueryTreeTest)\nCheck that child tranges are contained in parent tranges. ... ok\n\n---------------------------------------------------------\nRan 12 tests in 23.932s"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#r-example",
    "href": "schedule/slides/unit-tests.html#r-example",
    "title": "UBC Stat550",
    "section": "R example",
    "text": "R example\n\ntestthat::test_local(here::here(\"../../../../../../Delphi/smoothqr/\"))\n\n✔ | F W  S  OK | Context\n\n⠏ |          0 | smooth-rq                                                      \n⠴ |          6 | smooth-rq                                                      \n✔ |         12 | smooth-rq\n\n══ Results ═════════════════════════════════════════════════════════════════════\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 12 ]"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#what-do-i-test",
    "href": "schedule/slides/unit-tests.html#what-do-i-test",
    "title": "UBC Stat550",
    "section": "What do I test?",
    "text": "What do I test?\n\n\n\nCore Principle:\n\n\nTests should be passed by a correct function, but not by an incorrect function.\n\n\n\nThe tests must apply pressure to know if things break.\n\nseveral specific inputs for which you know the correct answer\n“edge” cases, like a list of size zero or a matrix instead of a vector\nspecial cases that the function must handle, but which you might forget about months from now\nerror cases that should throw an error instead of returning an invalid answer\nprevious bugs you’ve fixed, so those bugs never return."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#what-do-i-test-1",
    "href": "schedule/slides/unit-tests.html#what-do-i-test-1",
    "title": "UBC Stat550",
    "section": "What do I test?",
    "text": "What do I test?\nMake sure that incorrect functions won’t pass (or at least, won’t pass them all).\n\nadd &lt;- function(a, b) return(4)\nadd &lt;- function(a, b) return(a * b)\n\ntest_that(\"Addition is commutative\", {\n  expect_equal(add(1, 3), add(3, 1)) # both pass this !!\n  expect_equal(add(2, 5), add(5, 2)) # neither passes this\n})\n\n\n\n\n\n\n\nTip\n\n\n\nCover all branches.\nMake sure there aren’t branches you don’t expect."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#assertions",
    "href": "schedule/slides/unit-tests.html#assertions",
    "title": "UBC Stat550",
    "section": "Assertions",
    "text": "Assertions\nAssertions are things that must be true. Failure means “Quit”.\n\nThere’s no way to recover.\nThink: passed in bad arguments.\n\n\ndef fit(data, ...):\n\n    for it in range(max_iterations):\n        # iterative fitting code here\n        ...\n\n        # Plausibility check\n        assert np.all(alpha &gt;= 0), \"negative alpha\"\n        assert np.all(theta &gt;= 0), \"negative theta\"\n        assert omega &gt; 0, \"Nonpositive omega\"\n        assert eta2 &gt; 0, \"Nonpositive eta2\"\n        assert sigma2 &gt; 0, \"Nonpositive sigma2\"\n\n    ...\n\nThe parameters have to be positive. Negative is impossible. No way to recover."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#errors",
    "href": "schedule/slides/unit-tests.html#errors",
    "title": "UBC Stat550",
    "section": "Errors",
    "text": "Errors\nErrors are for unexpected conditions that could be handled by the calling code.\n\nYou could perform some action to work around the error, fix it, or report it to the user.\n\nExample:\n\nI give you directions to my house. You get lost. You could recover.\nMaybe retrace your steps, see if you missed a sign post.\nMaybe search on Google Maps to locate your self in relation to a landmark.\nIf those fail, message me.\nIf I don’t respond, get an Uber.\nFinally, give up and go home."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#errors-1",
    "href": "schedule/slides/unit-tests.html#errors-1",
    "title": "UBC Stat550",
    "section": "Errors",
    "text": "Errors\nCode can also do this. It can try the function and catch errors to recover automatically.\nFor example:\n\nLoad some data from the internet. If the file doesn’t exist, create some.\nRun some iterative algorithm. If we haven’t converged, restart from another place.\n\nCode can fix errors without user input. It can’t fix assertions.\n\nAn input must be an integer. So round it, Warn, and proceed. Rather than fail."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#test-driven-development",
    "href": "schedule/slides/unit-tests.html#test-driven-development",
    "title": "UBC Stat550",
    "section": "Test-driven development",
    "text": "Test-driven development\nTest Driven Development (TDD) uses a short development cycle for each new feature or component:\n\nWrite tests that specify the component’s desired behavior.\nThe tests will initially fail because the component does not yet exist.\nCreate the minimal implementation that passes the test.\nRefactor the code to meet design standards, running the tests with each change to ensure correctness."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#why-work-this-way",
    "href": "schedule/slides/unit-tests.html#why-work-this-way",
    "title": "UBC Stat550",
    "section": "Why work this way?",
    "text": "Why work this way?\n\nWriting the tests may help you realize\n\nwhat arguments the function must take,\n\nwhat other data it needs,\n\nand what kinds of errors it needs to handle.\n\nThe tests define a specific plan for what the function must do.\nYou will catch bugs at the beginning instead of at the end (or never).\nTesting is part of design, instead of a lame afterthought you dread doing."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#rules-of-thumb",
    "href": "schedule/slides/unit-tests.html#rules-of-thumb",
    "title": "UBC Stat550",
    "section": "Rules of thumb",
    "text": "Rules of thumb\n\nKeep tests in separate files\n\nfrom the code they test. This makes it easy to run them separately.\n\nGive tests names\n\nTesting frameworks usually let you give the test functions names or descriptions. test_1 doesn’t help you at all, but test_tree_insert makes it easy for you to remember what the test is for.\n\nMake tests replicable\n\nIf a test involves random data, what do you do when the test fails? You need some way to know what random values it used so you can figure out why the test fails."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#rules-of-thumb-1",
    "href": "schedule/slides/unit-tests.html#rules-of-thumb-1",
    "title": "UBC Stat550",
    "section": "Rules of thumb",
    "text": "Rules of thumb\n\nUse tests instead of the REPL\n\nIf you’re building a complicated function, write the tests in advance and use them to help you while you write the function. You’ll waste time calling over and over at the REPL.\n\nAvoid testing against another’s code/package\n\nYou don’t know the ins and outs of what they do. If they change the code, your tests will fail.\n\nTest Units, not main functions\n\nYou should write small functions that do one thing. Test those. Don’t write one huge 1000-line function and try to test that.\n\nAvoid random numbers\n\nSeeds are not always portable."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#other-suggestions",
    "href": "schedule/slides/unit-tests.html#other-suggestions",
    "title": "UBC Stat550",
    "section": "Other suggestions",
    "text": "Other suggestions\n\n\nDo this\n\nfoo &lt;- function(x) {\n  if (x &lt; 0) stop(x, \" is not positive\")\n}\n\nfoo &lt;- function(x) {\n  if (x &lt; 0) message(x, \" is not positive\")\n  # not useful unless we fix it too...\n}\n\nfoo &lt;- function(x) {\n  if (x &lt; 0) warning(x, \" is not positive\")\n  # not useful unless we fix it too...\n}\n\nfoo &lt;- function(x) {\n  if (length(x) == 0)\n    rlang::abort(\"no data\", class=\"no_input_data\")\n}\n\nThese allow error handling.\n\n\nDon’t do this\n\nfoo &lt;- function(x) {\n  if (x &lt; 0) {\n    print(paste0(x, \" is not positive\"))\n    return(NULL)\n  }\n  ...\n}\n\nfoo &lt;- function(x) {\n  if (x &lt; 0) cat(\"uh oh.\")\n  ...\n}\n\nCan’t recover.\nDon’t know what went wrong."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#classes",
    "href": "schedule/slides/unit-tests.html#classes",
    "title": "UBC Stat550",
    "section": "Classes",
    "text": "Classes\n\n\n\ntib &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100), \n  y = x1 + 2 * x2 + rnorm(100)\n)\nmdl &lt;- lm(y ~ ., data = tib)\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(mdl)\n\n[1] \"lm\"\n\n\nThe class allows for the use of “methods”\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n     0.1216       1.0803       2.1038  \n\n\n\n\n\nR “knows what to do” when you print() an object of class \"lm\".\nprint() is called a “generic” function.\nYou can create “methods” that get dispatched.\nFor any generic, R looks for a method for the class.\nIf available, it calls that function."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#viewing-the-dispatch-chain",
    "href": "schedule/slides/unit-tests.html#viewing-the-dispatch-chain",
    "title": "UBC Stat550",
    "section": "Viewing the dispatch chain",
    "text": "Viewing the dispatch chain\n\nsloop::s3_dispatch(print(incrementer))\n\n=&gt; print.function\n * print.default\n\nsloop::s3_dispatch(print(tib))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(print(mdl))\n\n=&gt; print.lm\n * print.default"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#r-geeky-but-important",
    "href": "schedule/slides/unit-tests.html#r-geeky-but-important",
    "title": "UBC Stat550",
    "section": "R-Geeky But Important",
    "text": "R-Geeky But Important\nThere are lots of generic functions in R\nCommon ones are print(), summary(), and plot().\nAlso, lots of important statistical modelling concepts: residuals() coef()\n(In python, these work the opposite way: obj.residuals. The dot after the object accesses methods defined for that type of object. But the dispatch behaviour is less robust.)\n\nThe convention is that the specialized function is named method.class(), e.g., summary.lm().\nIf no specialized function is defined, R will try to use method.default().\n\nFor this reason, R programmers try to avoid . in names of functions or objects."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#annoying-example",
    "href": "schedule/slides/unit-tests.html#annoying-example",
    "title": "UBC Stat550",
    "section": "Annoying example",
    "text": "Annoying example\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n     0.1216       1.0803       2.1038  \n\nprint.lm &lt;- function(x, ...) print(\"This is an linear model.\")\nprint(mdl)\n\n[1] \"This is an linear model.\"\n\n\n\nOverwrote the method in the global environment."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#wherefore-methods",
    "href": "schedule/slides/unit-tests.html#wherefore-methods",
    "title": "UBC Stat550",
    "section": "Wherefore methods?",
    "text": "Wherefore methods?\n\nThe advantage is that you don’t have to learn a totally new syntax to grab residuals or plot things\nYou just use residuals(mdl) whether mdl has class lm could have been done two centuries ago, or a Batrachian Emphasis Machine which won’t be invented for another five years.\nThe one draw-back is the help pages for the generic methods tend to be pretty vague\nCompare ?summary with ?summary.lm."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#different-environments",
    "href": "schedule/slides/unit-tests.html#different-environments",
    "title": "UBC Stat550",
    "section": "Different environments",
    "text": "Different environments\n\nThese are often tricky, but are very common.\nMost programming languages have this concept in one way or another.\nIn R code run in the Console produces objects in the “Global environment”\nYou can see what you create in the “Environment” tab.\nBut there’s lots of other stuff.\nMany packages are automatically loaded at startup, so you have access to the functions and data inside those package Environments.\n\nFor example mean(), lm(), plot(), iris (technically iris is lazy-loaded, meaning it’s not in memory until you call it, but it is available)"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#section-1",
    "href": "schedule/slides/unit-tests.html#section-1",
    "title": "UBC Stat550",
    "section": "",
    "text": "Other packages require you to load them with library(pkg) before their functions are available.\nBut, you can call those functions by prefixing the package name ggplot2::ggplot().\nYou can also access functions that the package developer didn’t “export” for use with ::: like dplyr:::as_across_fn_call()\n\n\nThat is all about accessing “objects in package environments”"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#other-issues-with-environments",
    "href": "schedule/slides/unit-tests.html#other-issues-with-environments",
    "title": "UBC Stat550",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nAs one might expect, functions create an environment inside the function.\n\nz &lt;- 1\nfun &lt;- function(x) {\n  z &lt;- x\n  print(z)\n  invisible(z)\n}\nfun(14)\n\n[1] 14\n\n\nNon-trivial cases are data-masking environments.\n\ntib &lt;- tibble(x1 = rnorm(100),  x2 = rnorm(100),  y = x1 + 2 * x2)\nmdl &lt;- lm(y ~ x2, data = tib)\nx2\n\nError in eval(expr, envir, enclos): object 'x2' not found\n\n\n\nlm() looks “inside” the tib to find y and x2\nThe data variables are added to the lm() environment"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#other-issues-with-environments-1",
    "href": "schedule/slides/unit-tests.html#other-issues-with-environments-1",
    "title": "UBC Stat550",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nWhen Knit, .Rmd files run in their OWN environment.\nThey are run from top to bottom, with code chunks depending on previous\nThis makes them reproducible.\nJupyter notebooks don’t do this. 😱\nObjects in your local environment are not available in the .Rmd\nObjects in the .Rmd are not available locally.\n\n\n\n\n\n\nTip\n\n\nThe most frequent error I see is:\n\nrunning chunks individually, 1-by-1, and it works\nKnitting, and it fails\n\nThe reason is almost always that the chunks refer to objects in the Global Environment that don’t exist in the .Rmd"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#section-2",
    "href": "schedule/slides/unit-tests.html#section-2",
    "title": "UBC Stat550",
    "section": "",
    "text": "This error also happens because:\n\nlibrary() calls were made globally but not in the .Rmd\n\nso the packages aren’t loaded\n\npaths to data or other objects are not relative to the .Rmd in your file system\n\nthey must be\n\nCareful organization and relative paths will help to avoid some of these."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#how-to-fix-code",
    "href": "schedule/slides/unit-tests.html#how-to-fix-code",
    "title": "UBC Stat550",
    "section": "How to fix code",
    "text": "How to fix code\n\nIf you’re using a function in a package, start with ?function to see the help\n\nMake sure you’re calling the function correctly.\nTry running the examples.\npaste the error into Google (this is my first step when you ask me)\nGo to the package website if it exists, and browse around\n\nIf your .Rmd won’t Knit\n\nDid you make the mistake on the last slide?\nDid it Knit before? Then the bug is in whatever you added.\nDid you never Knit it? Why not?\nCall rstudioapi::restartSession(), then run the Chunks 1-by-1"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#section-3",
    "href": "schedule/slides/unit-tests.html#section-3",
    "title": "UBC Stat550",
    "section": "",
    "text": "Adding browser()\n\nOnly useful with your own functions.\nOpen the script with the function, and add browser() to the code somewhere\nThen call your function.\nThe execution will Stop where you added browser() and you’ll have access to the local environment to play around"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#reproducible-examples",
    "href": "schedule/slides/unit-tests.html#reproducible-examples",
    "title": "UBC Stat550",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\n\n\n\n\n\nQuestion I get uncountably often that I hate:\n\n\n“I ran the code like you had on Slide 39, but it didn’t work.”\n\n\n\n\nIf you want to ask me why the code doesn’t work, you need to show me what’s wrong.\n\n\n\n\n\n\n\nDon’t just share a screenshot!\n\n\nUnless you get lucky, I won’t be able to figure it out from that.\nAnd I can’t copy-paste into Google.\n\n\n\nWhat you need is a Reproducible Example or reprex. This is a small chunk of code that\n\nruns in it’s own environment\nand produces the error."
  },
  {
    "objectID": "schedule/slides/unit-tests.html#reproducible-examples-how-it-works",
    "href": "schedule/slides/unit-tests.html#reproducible-examples-how-it-works",
    "title": "UBC Stat550",
    "section": "Reproducible examples, How it works",
    "text": "Reproducible examples, How it works\n\nOpen a new .R script.\nPaste your buggy code in the file (no need to save)\nEdit your code to make sure it’s “enough to produce the error” and nothing more. (By rerunning the code a few times.)\nCopy your code.\nCall reprex::reprex() from the console. This will run your code in a new environment and show the result in the Viewer tab. Does it create the error you expect?\nIf it creates other errors, that may be the problem. You may fix the bug on your own!\nIf it doesn’t have errors, then your global environment is Farblunget.\nThe Output is now on your clipboard. Share that.\n\n\n\n\n\n\n\nNote\n\n\nBecause Reprex runs in it’s own environment, it doesn’t have access to any of the libraries you loaded or the stuff in your global environment. You’ll have to load these things in the script. That’s the point"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#practice",
    "href": "schedule/slides/unit-tests.html#practice",
    "title": "UBC Stat550",
    "section": "Practice",
    "text": "Practice\nGradient ascent.\n\nSuppose we want to find \\(\\max_x f(x)\\).\nWe repeat the update \\(x \\leftarrow x + \\gamma f'(x)\\) until convergence, for some \\(\\gamma &gt; 0\\).\n\nPoisson likelihood.\n\nRecall the likelihood: \\(L(\\lambda; y_1,\\ldots,y_n) = \\prod_{i=1}^n \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}I(y_i \\in 0,1,\\ldots)\\)\n\nGoal: find the MLE for \\(\\lambda\\) using gradient ascent"
  },
  {
    "objectID": "schedule/slides/unit-tests.html#deliverables-2-r-scripts",
    "href": "schedule/slides/unit-tests.html#deliverables-2-r-scripts",
    "title": "UBC Stat550",
    "section": "Deliverables, 2 R scripts",
    "text": "Deliverables, 2 R scripts\n\nA function that evaluates the log likelihood. (think about sufficiency, ignorable constants)\nA function that evaluates the gradient of the log likelihood.\nA function that does the optimization.\n\nShould take in data, the log likelihood and the gradient.\nUse the loglikelihood to determine convergence.\nPass in any other necessary parameters with reasonable defaults.\n\nA collection of tests that make sure your functions work.\n\n\\[\n\\begin{aligned}\nL(\\lambda; y_1,\\ldots,y_n) &= \\prod_{i=1}^n \\frac{\\lambda^{y_i} \\exp(-\\lambda)}{y_i!}I(y_i \\in 0,1,\\ldots)\\\\\nx &\\leftarrow x + \\gamma f'(x)\n\\end{aligned}\n\\]\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/syllabus.html#section",
    "href": "schedule/slides/syllabus.html#section",
    "title": "UBC Stat550",
    "section": "Introduction and Second half pivot",
    "text": "Introduction and Second half pivot\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/syllabus.html#about-me",
    "href": "schedule/slides/syllabus.html#about-me",
    "title": "UBC Stat550",
    "section": "About me",
    "text": "About me\n\n\n\nDaniel J. McDonald\ndaniel@stat.ubc.ca\nhttp://dajmcdon.github.io\nAssociate Professor, Department of Statistics\n\n\n\nMoved to UBC in mid-March 2020, 2 days before the border closed\nPreviously a Stats Prof at Indiana University for 8 years"
  },
  {
    "objectID": "schedule/slides/syllabus.html#no-more-canvas",
    "href": "schedule/slides/syllabus.html#no-more-canvas",
    "title": "UBC Stat550",
    "section": "No More Canvas!!",
    "text": "No More Canvas!!\nSee the website:\nhttps://ubc-stat.github.io/stat-550/\n\n\n\nYou’ll find\n\nannouncements\nschedule\nlecture slides / notes\n\n(Grades still on Canvas)"
  },
  {
    "objectID": "schedule/slides/syllabus.html#course-communication",
    "href": "schedule/slides/syllabus.html#course-communication",
    "title": "UBC Stat550",
    "section": "Course communication",
    "text": "Course communication\n\n\nWebsite:\nhttps://ubc-stat.github.io/stat-550\n\nHosted on GitHub.\nLinks to slides and all materials\nSyllabus is there. Be sure to read it. (same idea as before)\n\nSlack:\n\nThis is our discussion board.\nNote that this data is hosted on servers outside of Canada. You may wish to use a pseudonym to protect your privacy.\nWe’ll use a Channel in the UBC-Stat Workspace\n\n\nGithub organization\n\nLinked from the website.\nThis is where you complete/submit assignments/projects/in-class-work\nThis is also hosted on US servers https://github.com/Stat550-2022"
  },
  {
    "objectID": "schedule/slides/syllabus.html#why-these",
    "href": "schedule/slides/syllabus.html#why-these",
    "title": "UBC Stat550",
    "section": "Why these?",
    "text": "Why these?\n\nYes, some data is hosted on servers in the US.\nBut in the real world, no one uses Canvas/Piazza, so why not learn things they do use?\nCanvas is dumb and hard to organize.\nGitHub is free and actually useful.\nMuch easier to communicate, “grade” or comment on your work\nMuch more DS friendly\nNote that MDS uses both of these, the department uses both, etc.\nMore on all this later.\n\n\nSlack help from MDS — features and rules"
  },
  {
    "objectID": "schedule/slides/syllabus.html#what-are-the-goals-of-stat-550",
    "href": "schedule/slides/syllabus.html#what-are-the-goals-of-stat-550",
    "title": "UBC Stat550",
    "section": "What are the goals of Stat 550?",
    "text": "What are the goals of Stat 550?\n\n1. Prepare you to do the consulting practicum (Stat 551)\n\n2. You’re a captive audience, so I can teach you some skills you’ll need for\n\nMSc Thesis/Project or PhD research\nEmployment in Data Science / Statistics.\nThese are often things that will help with the first as well"
  },
  {
    "objectID": "schedule/slides/syllabus.html#prepare-you-for-the-consulting-practicum-stat-551",
    "href": "schedule/slides/syllabus.html#prepare-you-for-the-consulting-practicum-stat-551",
    "title": "UBC Stat550",
    "section": "1. Prepare you for the consulting practicum (Stat 551)",
    "text": "1. Prepare you for the consulting practicum (Stat 551)\n\nunderstand how the data was collected\nimplications of the collection process for analysis\norganize data for analysis\ndetermine appropriate methods for analysis that answer’s the client’s questions\ninterpret the results\npresent and communicate the results\n\n\n\nIn most courses you get nice clean data. Getting to “nice clean data” is non-trivial\nIn most courses things are “IID”, negligible missingness\nUsually, the question is formed in statistical langauge, here, you are responsible for “translating”\nInterpretation has to be “translated back”\nPresentation skills — important everywhere"
  },
  {
    "objectID": "schedule/slides/syllabus.html#some-skills-youll-need",
    "href": "schedule/slides/syllabus.html#some-skills-youll-need",
    "title": "UBC Stat550",
    "section": "2. Some skills you’ll need",
    "text": "2. Some skills you’ll need\n\nVersion control\nReproducible reports\nWriting experience: genre is important\nPresentation skills\nBetter coding practice\nDocumentation"
  },
  {
    "objectID": "schedule/slides/syllabus.html#computing",
    "href": "schedule/slides/syllabus.html#computing",
    "title": "UBC Stat550",
    "section": "Computing",
    "text": "Computing\n\nAll work done in R/RMarkdown.\nNo you can’t use Python. Or Stata or SPSS.\nNo you can’t use Jupyter Notebooks.\nAll materials on Github.\nYou will learn to use Git/GitHub/RStudio/Rmarkdown.\nSlack for discussion/communication"
  },
  {
    "objectID": "schedule/slides/syllabus.html#getting-setup",
    "href": "schedule/slides/syllabus.html#getting-setup",
    "title": "UBC Stat550",
    "section": "Getting setup",
    "text": "Getting setup\n\nAdd to Slack Channel: https://ubc-stat.slack.com/archives/C04QUDNJG9X\nGithub account: https://github.com/\nAdd to the Github Org — tell me your account\nRStudio synchronization\n\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/presentations.html#section",
    "href": "schedule/slides/presentations.html#section",
    "title": "UBC Stat550",
    "section": "Giving presentations",
    "text": "Giving presentations\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/presentations.html#structure",
    "href": "schedule/slides/presentations.html#structure",
    "title": "UBC Stat550",
    "section": "Structure",
    "text": "Structure\n\nStrategy (applies to papers too)\nDos and don’ts\nPersonal preferences"
  },
  {
    "objectID": "schedule/slides/presentations.html#genre",
    "href": "schedule/slides/presentations.html#genre",
    "title": "UBC Stat550",
    "section": "Genre",
    "text": "Genre\nTalks take many forms (like papers)\n\nDepartment seminar\nShort conference presentation\nClass lecture\n...\n\nCalibrate your talk to the Genre and the Audience\n\n\nA job talk takes much more work than a class presentation\nFor context, after much practice, it takes me about 1 hour per minute of presentation length, depending on the amount of polish.\nMy course lectures take about 4x the target duration.\nGeneral ideas are the same for all styles."
  },
  {
    "objectID": "schedule/slides/presentations.html#audience",
    "href": "schedule/slides/presentations.html#audience",
    "title": "UBC Stat550",
    "section": "Audience",
    "text": "Audience\n\nThink about who you are talking to\n\nStatisticians?\nStudents?\nPotential employer?\nPeople with PhD’s but in other disciplines?\nYour grandma?\n\nRegardless of the audience, I think of dividing the talk roughly in 3rds."
  },
  {
    "objectID": "schedule/slides/presentations.html#your-audience-for-your-in-class-talk",
    "href": "schedule/slides/presentations.html#your-audience-for-your-in-class-talk",
    "title": "UBC Stat550",
    "section": "(Your audience for your in-class talk)",
    "text": "(Your audience for your in-class talk)\n\n2/3 of the time, the client.\nYou’re teaching them this topic.\nThink “someone who took 1 or 2 classes in statistics”\n1/3 of the time, your classmates.\nWhat are the details they need to know that they don’t know?\nThink carefully how to structure for that breakdown."
  },
  {
    "objectID": "schedule/slides/presentations.html#content",
    "href": "schedule/slides/presentations.html#content",
    "title": "UBC Stat550",
    "section": "Content",
    "text": "Content\nEach part is a little mini-talk\n\nStarts with the general idea\nDevelops a few details. Strategy: problem/solution or question/answer\nEnds with a takeaway\n\nBut these parts are recalibrated to the audience.\n\nYour Grandma doesn’t want to see math.\nYour employer might, but doesn’t want to hear about \\(\\sigma\\)-fields.\nStatisticians don’t want to see proofs (but might want a sketch).\n..."
  },
  {
    "objectID": "schedule/slides/presentations.html#story-structure",
    "href": "schedule/slides/presentations.html#story-structure",
    "title": "UBC Stat550",
    "section": "Story structure",
    "text": "Story structure\n\n\n\n\n\n\nWhat I often see…\n\n\nOnce upon a time, a young MSc student went into the woods of theory and found some trees.\nFirst they looked at one tree, it was oak.\nThen the looked at the next tree, it was maple.\nThen they wondered if trees could talk.\nAfter three months of wandering, they saw a house…\n\n\n\n\n\n\n\n\n\n\nThe attention grabber\n\n\nAxe-wielding woodsman saves student from wolf attack!"
  },
  {
    "objectID": "schedule/slides/presentations.html#better-structure",
    "href": "schedule/slides/presentations.html#better-structure",
    "title": "UBC Stat550",
    "section": "Better structure",
    "text": "Better structure\n\n(Enough details to give the headline.)\nHeadline result.\nHow do we know the result is real. What are the details of computation, inference, methodology.\nDemonstration with empirics."
  },
  {
    "objectID": "schedule/slides/presentations.html#you-should-consider",
    "href": "schedule/slides/presentations.html#you-should-consider",
    "title": "UBC Stat550",
    "section": "You should consider…",
    "text": "You should consider…\n\nAttention span diminishes quickly.\nWhat are the 3-5 takeaways?\nHit your main result at the beginning: this is what I can do that I couldn’t before."
  },
  {
    "objectID": "schedule/slides/presentations.html#the-ideal-map",
    "href": "schedule/slides/presentations.html#the-ideal-map",
    "title": "UBC Stat550",
    "section": "The ideal map",
    "text": "The ideal map\nMap out what you’ve done.\n\nWhat did you find?\nWhat are the implications?\nWhy does audience care?\nHow do we do it?\n\n\nAvoid wandering in the wilderness:\n\nFirst we did this;\nBut that didn’t work, so we tried …\nBut then we added …\nFinally we got to the beach …\nAnd the water was nice …"
  },
  {
    "objectID": "schedule/slides/presentations.html#words",
    "href": "schedule/slides/presentations.html#words",
    "title": "UBC Stat550",
    "section": "Words",
    "text": "Words\n\n\nToo many words on a slide is bad\n\nBullet points\nToo densely concentrated are bad\nAre bad\nAre hard to focus on\n\n\nEmpty space is your friend\n\nLorem markdownum et moras et ponendi odores, neu magna per! Tyria meo iungitur videt, frigore terras rogis Anienis poteram, dant. His vallem arma corpore vident nunc nivibus avus, dea. Spatium luce certa cupiunt, lina. Amabam opem, Iovis fecundaque et parum.\nAede virum annis audit modo: meus ramis videri: nec quod insidiisque Aonio tenuem, AI. Trames Iason: nocent hortatus lacteus praebita paternos petit, Paridis aptus prius ut origo furiisque. Mercibus sis nullo aliudve Amathunta sufficit ululatibus, praevalidusque segnis et Dryopen."
  },
  {
    "objectID": "schedule/slides/presentations.html#images",
    "href": "schedule/slides/presentations.html#images",
    "title": "UBC Stat550",
    "section": "Images",
    "text": "Images\n\n\nPictures are good\n\nFlow charts are good.\n\nCareful use of colour is good.\n\nSize is good.\n\ntoo much variation is distracting\n\n\n\n\n\nHow long did you stare at the cat?"
  },
  {
    "objectID": "schedule/slides/presentations.html#graphics",
    "href": "schedule/slides/presentations.html#graphics",
    "title": "UBC Stat550",
    "section": "Graphics",
    "text": "Graphics\n\n\n\n\n\n\n\n\nImportant\n\n\nDefaults are almost always terrible."
  },
  {
    "objectID": "schedule/slides/presentations.html#issues-with-the-preceding",
    "href": "schedule/slides/presentations.html#issues-with-the-preceding",
    "title": "UBC Stat550",
    "section": "Issues with the preceding",
    "text": "Issues with the preceding\n\nColours are awful\nGrey background is distracting\nText size is too small\nLegend position on the side is strange?\nNumbers on the y-axis are nonesense\nWith barchart, y-axis should start at 0.\n.png vs .svg"
  },
  {
    "objectID": "schedule/slides/presentations.html#graphics-1",
    "href": "schedule/slides/presentations.html#graphics-1",
    "title": "UBC Stat550",
    "section": "Graphics",
    "text": "Graphics"
  },
  {
    "objectID": "schedule/slides/presentations.html#again",
    "href": "schedule/slides/presentations.html#again",
    "title": "UBC Stat550",
    "section": "Again",
    "text": "Again\n\n\n\n\n\n\n\nTip\n\n\nI like this, but ~10% of men are colour blind (including some faculty in this department)."
  },
  {
    "objectID": "schedule/slides/presentations.html#simulation",
    "href": "schedule/slides/presentations.html#simulation",
    "title": "UBC Stat550",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "schedule/slides/presentations.html#jargon",
    "href": "schedule/slides/presentations.html#jargon",
    "title": "UBC Stat550",
    "section": "Jargon",
    "text": "Jargon\n\nBe wary of acronyms (MLE, BLUP, RKHS)\nAgain, think of your audience. MLE is fine for any statistician.\nOthers need definitions in words and written on the slide\nSame for math notation \\(\\bar{X},\\ \\mu,\\ \\sigma,\\ \\mathbf{UDV}^\\top\\)\nAnd for applied work e.g. SNP\nBest Linear Unbiased Predictor"
  },
  {
    "objectID": "schedule/slides/presentations.html#things-i-hate",
    "href": "schedule/slides/presentations.html#things-i-hate",
    "title": "UBC Stat550",
    "section": "Things I hate",
    "text": "Things I hate\n Saying “I’m not going to talk about …”  “I’m happy to discuss … later if you’d like”.\n Wiggling your laser pointer at every word. Highlight important things with pretty colours. Use pointer sparingly.\n Playing with your collar, your pockets, your water bottle…\n Staring at your slides …\n Displaying the total number of slides as in 6/85 in the lower right hand corner …\n Running over time. Skipping 6 slides to desperately make the time limit.\n Using the default themes:"
  },
  {
    "objectID": "schedule/slides/presentations.html#never-use-tables-of-numbers",
    "href": "schedule/slides/presentations.html#never-use-tables-of-numbers",
    "title": "UBC Stat550",
    "section": "Never use tables of numbers",
    "text": "Never use tables of numbers\n\nEconomists do this all the time for inexplicable reasons\nI rarely put these in papers either\nIf I’m not going to talk about it, it doesn’t go on the slide\nThere’s no way I’m going to read off the number, certainly not to 4 decimal places\nUse a graph"
  },
  {
    "objectID": "schedule/slides/presentations.html#use-graphs-but",
    "href": "schedule/slides/presentations.html#use-graphs-but",
    "title": "UBC Stat550",
    "section": "Use graphs, but",
    "text": "Use graphs, but\n\nA graph with 3 dots should be a table of 3 numbers.\nBut why do you have only 3 numbers?\nAny table can be a better graph.\n\n\n\n\n\n\n\nAsk yourself:\n\n\nIs this the best way to display the data? Have I summarized too much?"
  },
  {
    "objectID": "schedule/slides/presentations.html#example-made-up-simulation-results",
    "href": "schedule/slides/presentations.html#example-made-up-simulation-results",
    "title": "UBC Stat550",
    "section": "Example: Made up simulation results",
    "text": "Example: Made up simulation results\nRan 50 simulations."
  },
  {
    "objectID": "schedule/slides/presentations.html#example-made-up-simulation-results-1",
    "href": "schedule/slides/presentations.html#example-made-up-simulation-results-1",
    "title": "UBC Stat550",
    "section": "Example: Made up simulation results",
    "text": "Example: Made up simulation results\nRan 50 simulations."
  },
  {
    "objectID": "schedule/slides/presentations.html#example-made-up-simulation-results-2",
    "href": "schedule/slides/presentations.html#example-made-up-simulation-results-2",
    "title": "UBC Stat550",
    "section": "Example: Made up simulation results",
    "text": "Example: Made up simulation results\nRan 50 simulations."
  },
  {
    "objectID": "schedule/slides/presentations.html#things-you-should-do",
    "href": "schedule/slides/presentations.html#things-you-should-do",
    "title": "UBC Stat550",
    "section": "Things you should do",
    "text": "Things you should do\n Number your slides\n Have lots of prepared backup slides (details, answers to potential questions, further analysis)\n Practice a lot. Practice in front of others. Practice the beginning more than the rest.\n BE EXCITED. You worked hard on this. All results are cool. Play them up. You did something good and you want to tell everyone about how awesome you are. Own it.\n Take credit. Say “I showed this” not “It can be shown”."
  },
  {
    "objectID": "schedule/slides/presentations.html#things-that-are-debatable",
    "href": "schedule/slides/presentations.html#things-that-are-debatable",
    "title": "UBC Stat550",
    "section": "Things that are debatable",
    "text": "Things that are debatable\n\nMath talks tend to be “chalkboard”\nCS talks tend to be “sales pitch”\nStats is in the middle.\nI lean toward details with elements of salesmanship\nIf I hear your talk, I want to be able to “do” what you created. This is hard without some math.\nThis also colours my decisions about software.\n\n\n\n\n\n\n\n\nNote\n\n\nJeff Bezos banned Powerpoint from Amazon presentations"
  },
  {
    "objectID": "schedule/slides/presentations.html#closing-suggestions",
    "href": "schedule/slides/presentations.html#closing-suggestions",
    "title": "UBC Stat550",
    "section": "Closing suggestions",
    "text": "Closing suggestions\n1. Slow down\n\nGet a bottle of water before the talk.\nDrink it to pause on (pre-planned) key slides.\nThis will help you relax.\nIt will also give the audience a few seconds to get the hard stuff into their head.\n\n2. Cut back\n\nMost of your slides probably have too many words.\nAnd too much “filler” –&gt; Kill the filler"
  },
  {
    "objectID": "schedule/slides/presentations.html#closing-suggestions-1",
    "href": "schedule/slides/presentations.html#closing-suggestions-1",
    "title": "UBC Stat550",
    "section": "Closing suggestions",
    "text": "Closing suggestions\n3. Try to move\n\nIt’s good to move physically, engage the audience\nTry to make eye contact with the whole room\nRecord yourself once to see if you do anything extraneous\n\n4. Have fun."
  },
  {
    "objectID": "schedule/slides/presentations.html#example-talks",
    "href": "schedule/slides/presentations.html#example-talks",
    "title": "UBC Stat550",
    "section": "Example talks:",
    "text": "Example talks:\n\nTeaching PCA\nShort research talk about Latent Infections\n\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/organization.html#section",
    "href": "schedule/slides/organization.html#section",
    "title": "UBC Stat550",
    "section": "Organization and reports",
    "text": "Organization and reports\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/organization.html#topics-for-today",
    "href": "schedule/slides/organization.html#topics-for-today",
    "title": "UBC Stat550",
    "section": "Topics for today",
    "text": "Topics for today\n\nOrganizing your file system\nWriting reports that mix output and text\n(Avoiding buggy code)"
  },
  {
    "objectID": "schedule/slides/organization.html#the-guiding-theme",
    "href": "schedule/slides/organization.html#the-guiding-theme",
    "title": "UBC Stat550",
    "section": "The guiding theme",
    "text": "The guiding theme"
  },
  {
    "objectID": "schedule/slides/organization.html#i-urge-you-to-consult",
    "href": "schedule/slides/organization.html#i-urge-you-to-consult",
    "title": "UBC Stat550",
    "section": "I urge you to consult:",
    "text": "I urge you to consult:\nKarl Broman’s Notes"
  },
  {
    "objectID": "schedule/slides/organization.html#some-guiding-principles",
    "href": "schedule/slides/organization.html#some-guiding-principles",
    "title": "UBC Stat550",
    "section": "Some guiding principles",
    "text": "Some guiding principles\n\nAvoid naming by date.\n\nYour file system already knows the date.\nSometimes projects take a while.\nYou can add this inside a particular report: Last updated: 2022-01-07\n\nIf you’re going to use a date anywhere, do YYYY-MM-DD or YYYYMMDD not DD-MMM-YY\nThis is a process\nDon’t get tied down\nBut don’t reorganize every time you find a better system\nCustomize to your needs, preferences"
  },
  {
    "objectID": "schedule/slides/organization.html#organizing-your-stuff",
    "href": "schedule/slides/organization.html#organizing-your-stuff",
    "title": "UBC Stat550",
    "section": "Organizing your stuff",
    "text": "Organizing your stuff\n├── Advising\n│   ├── arash\n│   ├── gian-carlo\n├── CV\n├── Computing\n│   ├── batchtools.slurm.tmpl\n│   ├── computecanada_notes.md\n│   ├── FKF\n│   └── ghclass\n├── Grants\n│   ├── B&E JSM 2010\n│   ├── CANSSI RRP 2020\n│   ├── NSERC 2020\n├── LettersofRec\n├── Manuscripts\n|   ├── learning-white-matter\n|   ├── rt-est\n│   ├── zzzz Old\n├── Referee reports\n├── Talks\n│   ├── JobTalk2020\n│   ├── ubc-stat-covid-talk\n│   └── utoronto-grad-advice\n├── Teaching\n│   ├── stat-406\n│   ├── stat-550\n│   ├── zzzz CMU TA\n│   └── zzzz booth\n└── Website"
  },
  {
    "objectID": "schedule/slides/organization.html#inside-a-project",
    "href": "schedule/slides/organization.html#inside-a-project",
    "title": "UBC Stat550",
    "section": "Inside a project",
    "text": "Inside a project\n.\n├── README.md\n├── Summary of Goals.rtf\n├── cluster_output\n├── code\n├── data\n├── dsges-github.Rproj\n├── manuscript\n└── waldman-triage\n\nInclude a README\nIdeally have a MAKEFILE\nUnder version control, shared with collaborator"
  },
  {
    "objectID": "schedule/slides/organization.html#basic-principles",
    "href": "schedule/slides/organization.html#basic-principles",
    "title": "UBC Stat550",
    "section": "Basic principles",
    "text": "Basic principles\n\nBe consistent – directory structure; names\n\nall project files in 1 directory, not multiples\n\nAlways separate raw from processed data\nAlways separate code from data\nIt should be obvious what code created what files, and what the dependencies are. (MAKEFILE forces this)\nNo hand-editing of data files\nDon’t use spaces in file names\nIn code, use relative paths, not absolute paths\n\n../blah not ~/blah or /users/dajmcdon/Documents/Work/proj-1/blah\nThe {here} package in R is great for this"
  },
  {
    "objectID": "schedule/slides/organization.html#problem-coordinating-with-collaborators",
    "href": "schedule/slides/organization.html#problem-coordinating-with-collaborators",
    "title": "UBC Stat550",
    "section": "Problem: Coordinating with collaborators",
    "text": "Problem: Coordinating with collaborators\n\nWhere to put data that multiple people will work with?\nWhere to put intermediate/processed data?\nWhere to indicate the code that created those processed data files?\nHow to divvy up tasks and know who did what?\nNeed to agree on directory structure and file naming conventions\n\nGitHub is (I think) the ideal solution, but not always feasible."
  },
  {
    "objectID": "schedule/slides/organization.html#problem-collaborators-who-dont-use-github",
    "href": "schedule/slides/organization.html#problem-collaborators-who-dont-use-github",
    "title": "UBC Stat550",
    "section": "Problem: Collaborators who don’t use GitHub",
    "text": "Problem: Collaborators who don’t use GitHub\n\nUse GitHub yourself\nCopy files to/from some shared space\n\nIdeally, in an automated way (Dropbox, S3 Bucket)\nAvoid Word at all costs. Google Docs if needed.\nWord and Git do not mix\nLast resort: Word file in Dropbox. Everything else nicely organized on your end. Rmd file with similar structure to Manuscript that does the analysis.\n\nCommit their changes.\n\n\nOverleaf has Git built in (paid tier). I don’t like Overleaf. Costs money, the viewer is crap and so is the editor. I suggest you avoid it."
  },
  {
    "objectID": "schedule/slides/organization.html#using-rmarkdownquartojupyter-for-most-things",
    "href": "schedule/slides/organization.html#using-rmarkdownquartojupyter-for-most-things",
    "title": "UBC Stat550",
    "section": "Using Rmarkdown/Quarto/Jupyter for most things",
    "text": "Using Rmarkdown/Quarto/Jupyter for most things\nYour goal is to Avoid at all costs:\n\n“How did I create this plot?”\n“Why did I decide to omit those six samples?”\n“Where (on the web) did I find these data?”\n“What was that interesting gene/feature/predictor?”\n\n\nReally useful resource:\n\nEmily Reiderer RmdDD\nTalk Slides"
  },
  {
    "objectID": "schedule/slides/organization.html#when-i-begin-a-new-project",
    "href": "schedule/slides/organization.html#when-i-begin-a-new-project",
    "title": "UBC Stat550",
    "section": "When I begin a new project",
    "text": "When I begin a new project\n\nCreate a directory structure\n\ncode/\npapers/\nnotes/ (maybe?)\nREADME.md\ndata/ (maybe?)\n\nWrite scripts in the code/ directory\nTODO items in the README\nUse Rmarkdown/Quarto/Jupyter for reports, render to .pdf"
  },
  {
    "objectID": "schedule/slides/organization.html#as-the-project-progresses",
    "href": "schedule/slides/organization.html#as-the-project-progresses",
    "title": "UBC Stat550",
    "section": "As the project progresses…",
    "text": "As the project progresses…\nReorganize\n\nSome script files go to a package (thorougly tested), all that remains is for the paper\nThese now load the package and run simulations or analyses (that take a while)\nMaybe add a directory that contains dead-ends (code or text or …)\nAdd manuscript/. I try to go for main.tex and Supplement.Rmd\nSupplement.Rmd runs anything necessary in code/ and creates all figures in the main doc and the supplement. Also generates any online supplementary material\nSometimes, just manuscript/main.Rmd\nSometimes main.tex just inputs intro.tex, methods.tex, etc."
  },
  {
    "objectID": "schedule/slides/organization.html#the-old-manuscript-starting-in-school-persisting-too-long",
    "href": "schedule/slides/organization.html#the-old-manuscript-starting-in-school-persisting-too-long",
    "title": "UBC Stat550",
    "section": "The old manuscript (starting in School, persisting too long)",
    "text": "The old manuscript (starting in School, persisting too long)\n\nWrite lots of LaTeX, R code in separate files\nNeed a figure. Run R code, get figure, save as .pdf.\nRecompile LaTeX. Axes are unreadable. Back to R, rerun R code, …\nRecompile LaTeX. Can’t distinguish lines. Back to R, rerun R code, …\nCollaborator wants changes to the simulation. Edit the code. Rerun figure script, doesn’t work. More edits….Finally Recompile.\nReviewer “what if n is bigger”. Hope I can find the right location. But the code isn’t functions. Something breaks …\nEtc, etc."
  },
  {
    "objectID": "schedule/slides/organization.html#now",
    "href": "schedule/slides/organization.html#now",
    "title": "UBC Stat550",
    "section": "Now:",
    "text": "Now:\n\nR package with documented code, available on GitHub.\n\nOne script to run the analysis, one to gather the results.\n\nOne .Rmd file to take in the results, do preprocessing, generate all figures.\n\nLaTeX file on Journal style.\n\nThe optimal\nSame as above but with a MAKEFILE to automatically run parts of 1–4 as needed"
  },
  {
    "objectID": "schedule/slides/organization.html#evolution-of-presentations",
    "href": "schedule/slides/organization.html#evolution-of-presentations",
    "title": "UBC Stat550",
    "section": "Evolution of presentations",
    "text": "Evolution of presentations\n\nLaTeX + Beamer (similar to the manuscript):\n\nWrite lots of LaTeX, R code in separate files\nNeed a figure. Run R code, get figure, save as .pdf.\nRinse and repeat.\n\nCourse slides in Rmarkdown + Slidy\nSeminars in Rmarkdown + Beamer (with lots of customization)\nSeminars in Rmarkdown + Xaringan\nEverything in Quarto\n\n\n\n\n\n\n\n\nEasy to use.\nEasy to customize (defaults are not great)\nWELL DOCUMENTED"
  },
  {
    "objectID": "schedule/slides/organization.html#takeaways",
    "href": "schedule/slides/organization.html#takeaways",
    "title": "UBC Stat550",
    "section": "Takeaways",
    "text": "Takeaways\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/grad-school.html#section",
    "href": "schedule/slides/grad-school.html#section",
    "title": "UBC Stat550",
    "section": "Skills for graduate students",
    "text": "Skills for graduate students\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/grad-school.html#something-happens-in-graduate-school",
    "href": "schedule/slides/grad-school.html#something-happens-in-graduate-school",
    "title": "UBC Stat550",
    "section": "Something happens in graduate school",
    "text": "Something happens in graduate school\n\nAs undergrads, you took lots of classes\nYou didn’t care that much about all of them\nSure, you wanted good grades, but you may not have always wanted to really learn the material\nAnd you probably didn’t try to go in depth beyond the requirements\n\n\n\nThat has to change in grad school\nEven if you don’t want a to be a professor, to get a PhD, to do an MSc thesis.\nThis is the material that you have decided you will use for the rest of your life\n\n\n\n\nIf you disagree, then we should talk"
  },
  {
    "objectID": "schedule/slides/grad-school.html#side-discussion-on-reading-for-research",
    "href": "schedule/slides/grad-school.html#side-discussion-on-reading-for-research",
    "title": "UBC Stat550",
    "section": "Side discussion on “Reading for research”",
    "text": "Side discussion on “Reading for research”\n\nYou should “read” regularly: set aside an 2-3 hours every week\nStay up-to-date on recent research, determine what you find interesting\nWhat do people care about? What does it take to write journal articles?"
  },
  {
    "objectID": "schedule/slides/grad-school.html#what-is-read",
    "href": "schedule/slides/grad-school.html#what-is-read",
    "title": "UBC Stat550",
    "section": "What is “read”?",
    "text": "What is “read”?\n\nStart with titles, then abstracts, then intro+conclusion\nEach is a filter to determine how far to go\nPass 3 filters, read the paper (should take about ~30 minutes)\nDon’t get bogged down in notation, proofs\nOrganize your documents somehow, make notes in the margins, etc\nAfter you read it, you should be able to tell me what they show, why it’s important, why it’s novel\nIf you can, figure out how they show something. This is hard."
  },
  {
    "objectID": "schedule/slides/grad-school.html#how-to-find-and-organize-papers",
    "href": "schedule/slides/grad-school.html#how-to-find-and-organize-papers",
    "title": "UBC Stat550",
    "section": "How to find and organize papers",
    "text": "How to find and organize papers\n\narXiv, AOS, JASA, JCGS have RSS feeds, email lists etc\nFind a statistician you like who filters\nFollow reading groups\nConference proceedings\nBecome an IMS member, SSC member (ASA costs money:( )\nBibDesk, Zotero"
  },
  {
    "objectID": "schedule/slides/grad-school.html#ideal-outcome",
    "href": "schedule/slides/grad-school.html#ideal-outcome",
    "title": "UBC Stat550",
    "section": "Ideal outcome",
    "text": "Ideal outcome\n\nIf you need to learn something, you can teach yourself\nKnow how to find the basics on the internet\nKnow how to go in depth with real sources\nCollect a set of resources that you can turn to regularly\nIf you need to read a book, you can\nIf you need to pick up a new coding language, you can\n\n\n\n\n\n\n\n\nWhat this doesn’t mean\n\n\nYou are not expected to have all the answers at the tips of your fingers.\n\n\n\nBut you should get progressively good at finding them.\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#section",
    "href": "schedule/slides/cluster-computing.html#section",
    "title": "UBC Stat550",
    "section": "Cluster computing (at UBC)",
    "text": "Cluster computing (at UBC)\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#ubc-hpc",
    "href": "schedule/slides/cluster-computing.html#ubc-hpc",
    "title": "UBC Stat550",
    "section": "UBC HPC",
    "text": "UBC HPC\n3 potentially useful systems:\n\nDepartment VM\nUBC ARC Sockeye\nDigital Research Alliance of Canada\n\nI’ve only used 1 and 3. I mainly use 3.\nAccessing\nAs far as I know, access for students requires “faculty” support\n\nEmail The/Binh.\nPossible you can access without a faculty PI.\nEmail your advisor to ask for an account.\n\nThe rest of this will focus on 3."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#prerequisites",
    "href": "schedule/slides/cluster-computing.html#prerequisites",
    "title": "UBC Stat550",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n\n\nCommand line interface (Terminal on Mac)\n(optional) helpful to have ftp client. (Cyberduck)\nGlobus Connect. File transfer approved by DRAC\n\n\nUseful CL commands\n\ncd ~/path/to/directory\n\ncp file/to/copy.txt copied/to/copy1.txt\n\nrm file/to/delete.txt\n\nrm -r dir/to/delete/\n\nls -a # list all files"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#how-to-connect",
    "href": "schedule/slides/cluster-computing.html#how-to-connect",
    "title": "UBC Stat550",
    "section": "How to connect",
    "text": "How to connect\nLogin to a system:\n\nssh dajmcdon@cedar.alliancecan.ca\n\n\nUpon login, you’re on a “head” or “login” node.\nJobs &gt; 30min will be killed.\nYou can continuously run short interactive jobs."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#rule-1",
    "href": "schedule/slides/cluster-computing.html#rule-1",
    "title": "UBC Stat550",
    "section": "Rule 1",
    "text": "Rule 1\n\n\n\n\n\n\nTip\n\n\nIf you’re doing work for school: run it on one of these machines.\n\n\n\n\nYes, there is overhead to push data over and pull results back.\nBut DRAC/Sockeye is much faster than your machine.\nAnd this won’t lock up your laptop for 4 hours while you run the job.\nIt’s also a good experience.\nYou can log out and leave the job running. Just log back in to see if it’s done (you should always have some idea how long it will take)"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#modules",
    "href": "schedule/slides/cluster-computing.html#modules",
    "title": "UBC Stat550",
    "section": "Modules",
    "text": "Modules\n\nOnce you connect with ssh:\nThere are no Applications loaded.\nYou must tell the system what you want.\nThe command is module load r or module load sas\nIf you find yourself using the same modules all the time:\n\n\nmodule load StdEnv/2023 r gurobi python # stuff I use\n\nmodule save my_modules # save loaded modules\n\nmodule restore my_modules # on login, load the usual set"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#running-something-interactively",
    "href": "schedule/slides/cluster-computing.html#running-something-interactively",
    "title": "UBC Stat550",
    "section": "Running something interactively",
    "text": "Running something interactively\n\nLogin\nLoad modules\nRequest interactive compute\n\n\nsalloc --time=1:0:0 --ntasks=1 --account=def-dajmcdon --mem-per-cpu=4096M\n# allocate 1 hour on 1 cpu with 4Gb RAM\n\n\nFor the user def-dajmcdon (that’s me, accounts start with def-)\n\nThen I would start R\n\nr\n\nAnd run whatever I want. If it takes more than an hour or needs more than 4GB of memory, it’ll quit."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#interactive-jobs",
    "href": "schedule/slides/cluster-computing.html#interactive-jobs",
    "title": "UBC Stat550",
    "section": "Interactive jobs",
    "text": "Interactive jobs\n\nOnce started they’ll just go\nYou can do whatever else you want on your machine\nBut you can’t kill the connection\nSo don’t close your laptop and walk away\nThis is not typically the best use of this resource.\nBetter is likely syzygy.\n\nAlthough, syzygy has little memory and little storage, so it won’t do intensive tasks\n\nI think your home dir is limited to 1GB"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#big-memory-jobs",
    "href": "schedule/slides/cluster-computing.html#big-memory-jobs",
    "title": "UBC Stat550",
    "section": "Big memory jobs",
    "text": "Big memory jobs\n\nPossible you can do this interactively, but discouraged\n\n\n\n\n\n\n\nExample\n\n\n\nNeuroscience project\nDataset is about 10GB\nPeak memory usage during analysis is about 24GB\nCan’t do this on my computer\nWant to offload onto DRAC\n\n\n\n\n\nWrite a R / python script that does the whole analysis and saves the output.\nYou need to ask DRAC to run the script for you."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#the-scheduler",
    "href": "schedule/slides/cluster-computing.html#the-scheduler",
    "title": "UBC Stat550",
    "section": "The scheduler",
    "text": "The scheduler\n\nYou can log in to DRAC and “do stuff”\nBut resources are limited.\nThere’s a process that determines who gets resources when.\nTechnically the salloc command we used before requested some resources.\nIt may “sit” until the resources you want are available, but probably not long.\nAnything else has to go through the scheduler.\nDRAC uses the slurm scheduler"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-script",
    "href": "schedule/slides/cluster-computing.html#example-script",
    "title": "UBC Stat550",
    "section": "Example script",
    "text": "Example script\n\n#!/bin/bash\n\n#SBATCH --account=def-dajmcdon\n#SBATCH --job-name=dlbcl-suffpcr\n#SBATCH --output=%x-%j.out\n#SBATCH --error=%x-%j.out\n#SBATCH --time=10:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=32G\n\nRscript -e 'source(\"dlbcl-nocv.R\")'\n\n\nThis asks for 10 hours of compute time with 32GB of memory\nThe job-name / output / error fields are for convenience.\nIf unspecified, I’ll end up with files named things like jobid60607-60650934.out"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#submitting-and-other-useful-commands",
    "href": "schedule/slides/cluster-computing.html#submitting-and-other-useful-commands",
    "title": "UBC Stat550",
    "section": "Submitting and other useful commands",
    "text": "Submitting and other useful commands\n\nSuppose that slurm script is saved as dlbcl-slurm.sh\n\n\nsbatch dlbcl-slurm.sh # submit the job to the scheduler\n\nsqueue -u $USER # show status of your jobs ($USER is an env variable)\n\nscancel -u $USER # cancel all your jobs\n\nscancel -t PENDING -u $USER # cancel all your pending jobs\n\n\n\n\n\n\n\nImportant\n\n\n\nJobs inherit environment variables. So if you load modules, then submit, your modules are available to run.\nOn Cedar, jobs cannot run from ~/. It must be run from ~/scratch/ or ~/projects/."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#types-of-jobs",
    "href": "schedule/slides/cluster-computing.html#types-of-jobs",
    "title": "UBC Stat550",
    "section": "Types of jobs",
    "text": "Types of jobs\n\nBig jobs (need lots of RAM)\nGPU jobs (you want deep learning, I don’t know how to do this)\nOther jobs with internal parallelism (I almost never do this)\nEmbarrassingly parallel jobs (I do this all the time)"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#simple-parallelization",
    "href": "schedule/slides/cluster-computing.html#simple-parallelization",
    "title": "UBC Stat550",
    "section": "Simple parallelization",
    "text": "Simple parallelization\n\nMost of my major computing needs are “embarrassingly parallel”\nI want to run a few algorithms on a bunch of different simulated datasets under different parameter configurations.\nPerhaps run the algos on some real data too.\nR has packages which are good for parallelization (snow, snowfall, Rmpi, parallel)\nThis is how I originally learned to do parallel computing. But these packages are not good for the cluster\nThey’re fine for your machine, but we’ve already decided we’re not going to do that anymore."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-of-the-bad-parallelism",
    "href": "schedule/slides/cluster-computing.html#example-of-the-bad-parallelism",
    "title": "UBC Stat550",
    "section": "Example of the bad parallelism",
    "text": "Example of the bad parallelism\nTorque script\n\n#!/bin/bash  \n#PBS -l nodes=8:ppn=8,walltime=200:00:00\n#PBS -m abe\n#PBS -n ClusterPermute \n#PBS -j oe \nmpirun -np 64 -machinefile $PBS_NODEFILE R CMD BATCH ClusterPermute.R\n\n\nTorque is a different scheduler. UBC ARC Sockeye uses Torque. Looks much like Slurm.\nHere, ClusterPermute.R uses Rmpi to do “parallel lapply”\nSo I asked for 8 processors on each of 8 nodes."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-of-the-bad-parallelism-1",
    "href": "schedule/slides/cluster-computing.html#example-of-the-bad-parallelism-1",
    "title": "UBC Stat550",
    "section": "Example of the bad parallelism",
    "text": "Example of the bad parallelism\nTorque script\n\n#!/bin/bash  \n#PBS -l nodes=8:ppn=8,walltime=200:00:00\n#PBS -m abe\n#PBS -n ClusterPermute \n#PBS -j oe \nmpirun -np 64 -machinefile $PBS_NODEFILE R CMD BATCH ClusterPermute.R\n\nProblem\n\nThe scheduler has to find 8 nodes with 8 available processors before this job will start.\nThis often takes a while, sometimes days.\nBut the jobs don’t need those things to happen at the same time."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#batchtools",
    "href": "schedule/slides/cluster-computing.html#batchtools",
    "title": "UBC Stat550",
    "section": "{batchtools}",
    "text": "{batchtools}\n\nUsing R (or python) to parallelize is inefficient when there’s a scheduler in the middle.\nBetter is to actually submit 64 different jobs each requiring 1 node\nThen each can get out of the queue whenever a processor becomes available.\nBut that would seem to require writing 64 different slurm scripts\n{batchtools} does this for you, all in R\n\nIt automates writing/submitting slurm / torque scripts.\nIt automatically stores output, and makes it easy to collect.\nIt generates lots of jobs.\nAll this from R directly.\n\n\nIt’s easy to port across machines / schedulers.\nI can test parts (or even run) it on my machine without making changes for the cluster."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#setup-batchtools",
    "href": "schedule/slides/cluster-computing.html#setup-batchtools",
    "title": "UBC Stat550",
    "section": "Setup {batchtools}",
    "text": "Setup {batchtools}\n\nCreate a directory where all your jobs will live (in subdirectories). Mine is ~/\nIn that directory, you need a template file. Mine is ~/.batchtools.slurm.tmpl (next slide)\nCreate a configuration file which lives in your home directory. You must name it ~/.batchtools.conf.R.\n\n\n# ~/.batchtools.conf.R\ncluster.functions &lt;- makeClusterFunctionsSlurm()"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#batchtools.slurm.tmpl",
    "href": "schedule/slides/cluster-computing.html#batchtools.slurm.tmpl",
    "title": "UBC Stat550",
    "section": "~/.batchtools.slurm.tmpl",
    "text": "~/.batchtools.slurm.tmpl\n\n#!/bin/bash\n\n## Job Resource Interface Definition\n##\n## ntasks [integer(1)]:       Number of required tasks,\n##                            Set larger than 1 if you want to further parallelize\n##                            with MPI within your job.\n## ncpus [integer(1)]:        Number of required cpus per task,\n##                            Set larger than 1 if you want to further parallelize\n##                            with multicore/parallel within each task.\n## walltime [integer(1)]:     Walltime for this job, in seconds.\n##                            Must be at least 60 seconds for Slurm to work properly.\n## memory   [integer(1)]:     Memory in megabytes for each cpu.\n##                            Must be at least 100 (when I tried lower values my\n##                            jobs did not start at all).\n##\n## Default resources can be set in your .batchtools.conf.R by defining the variable\n## 'default.resources' as a named list.\n\n&lt;%\n# relative paths are not handled well by Slurm\nlog.file = fs::path_expand(log.file)\n-%&gt;\n\n#SBATCH --account=def-dajmcdon\n#SBATCH --mail-user=daniel@stat.ubc.ca\n#SBATCH --mail-type=ALL\n#SBATCH --job-name=&lt;%= job.name %&gt;\n#SBATCH --output=&lt;%= log.file %&gt;\n#SBATCH --error=&lt;%= log.file %&gt;\n#SBATCH --time=&lt;%= resources$walltime %&gt;\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=&lt;%= resources$ncpus %&gt;\n#SBATCH --mem-per-cpu=&lt;%= resources$memory %&gt;\n&lt;%= if (array.jobs) sprintf(\"#SBATCH --array=1-%i\", nrow(jobs)) else \"\" %&gt;\n\n## Run R:\n## we merge R output with stdout from SLURM, which gets then logged via --output option\nRscript -e 'batchtools::doJobCollection(\"&lt;%= uri %&gt;\")'\n\n\nWhen I’m ready to run, I’ll call something like:\n\nbatchtools::submitJobs(\n  job.ids, \n  resources = list(ncpus=1, walltime=\"24:00:00\", memory=\"32G\")\n)"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#workflow",
    "href": "schedule/slides/cluster-computing.html#workflow",
    "title": "UBC Stat550",
    "section": "Workflow",
    "text": "Workflow\nSee the vignette: vignette(\"batchtools\")\nor the\nwebsite\n\nCreate a folder to hold your code. Mine usually contains 2 files, one to set up/run the experiment, one to collect results. Code needed to run the experiment lives in an R package.\nWrite a script to setup the experiment and submit.\nWait.\nCollect your results. Copy back to your machine etc."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-1-use-genetics-data-to-predict-viral-load",
    "href": "schedule/slides/cluster-computing.html#example-1-use-genetics-data-to-predict-viral-load",
    "title": "UBC Stat550",
    "section": "Example 1: Use genetics data to predict viral load",
    "text": "Example 1: Use genetics data to predict viral load\n\nAn “extra” example in a methods paper to appease reviewers\nMethod is:\n\napply a special version of PCA to a big (wide) data set\nDo OLS using the top few PCs\n\nThis is “principle components regression” with sparse principle components.\nGot 413 COVID patients, measure “viral load” and gene expression\n9435 differentially expressed genes.\nThe method needs to form a 10K x 10K matrix multiple times and do an approximate SVD. Requires 32GB memory. Compute time is ~6 hours.\nTwo tuning parameters: \\(\\lambda\\) and number of PCs\nWant to do CV to choose, and then use those on the whole data, describe selected genes."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-1-use-genetics-data-to-predict-viral-load-1",
    "href": "schedule/slides/cluster-computing.html#example-1-use-genetics-data-to-predict-viral-load-1",
    "title": "UBC Stat550",
    "section": "Example 1: Use genetics data to predict viral load",
    "text": "Example 1: Use genetics data to predict viral load\n\nlibrary(batchtools)\n\nreg &lt;- makeExperimentRegistry(\"spcr-genes\", packages = c(\"tidyverse\", \"suffpcr\"))\nx &lt;- readRDS(here::here(\"suffpcr-covid\", \"covid_x.rds\"))\ny &lt;- readRDS(here::here(\"suffpcr-covid\", \"covid_y.rds\"))\n\nsubsample = function(data, job, ratio, ...) {\n  n &lt;- nrow(data$x)\n  train &lt;- sample(n, floor(n * ratio))\n  test &lt;- setdiff(seq_len(n), train)\n  list(test = test, train = train)\n}\n\naddProblem(\"cv\", data = list(x = x, y = y), fun = subsample)\naddProblem(\"full\", data = list(x = x, y = y))\n\naddAlgorithm(\n  \"spcr_cv\",\n  fun = function(job, data, instance, ...) { # args are required\n    fit &lt;- suffpcr(\n      data$x[instance$train, ], data$y[instance$train], \n      lambda_min = 0, lambda_max = 1, ...\n    )\n    valid_err &lt;- colMeans(\n      (\n        data$y[instance$test] - \n         as.matrix(predict(fit, newdata = data$x[instance$test, ]))\n      )^2,\n      na.rm = TRUE\n    )\n    return(list(fit = fit, valid_err = valid_err))\n  }\n)\n\naddAlgorithm(\n  \"spcr_full\",\n  fun = function(job, data, instance, ...) {\n    suffpcr(data$x, data$y, lambda_max = 1, lambda_min = 0, ...)\n  }\n)\n\n## Experimental design\npdes_cv &lt;- list(cv = data.frame(ratio = .75))\npdes_full &lt;- list(full = data.frame())\nades_cv &lt;- list(spcr_cv = data.frame(d = c(3, 5, 15)))\nades_full &lt;- list(spcr_full = data.frame(d = c(3, 5, 15)))\n\naddExperiments(pdes_cv, ades_cv, repls = 5L)\naddExperiments(pdes_full, ades_full)\n\nsubmitJobs(\n  findJobs(), \n  resources = list(ncpus = 1, walltime = \"8:00:00\", memory = \"32G\")\n)\n\nEnd up with 18 jobs."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-2-predicting-future-covid-cases",
    "href": "schedule/slides/cluster-computing.html#example-2-predicting-future-covid-cases",
    "title": "UBC Stat550",
    "section": "Example 2: Predicting future COVID cases",
    "text": "Example 2: Predicting future COVID cases\n\nTake a few very simple models and demonstrate that some choices make a big difference in accuracy.\nAt each time \\(t\\), download COVID cases as observed on day \\(t\\) for a bunch of locations\nEstimate a few different models for predicting days \\(t+1,\\ldots,t+k\\)\nStore point and interval forecasts.\nDo this for \\(t\\) every week over a year."
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#example-2-predicting-future-covid-cases-1",
    "href": "schedule/slides/cluster-computing.html#example-2-predicting-future-covid-cases-1",
    "title": "UBC Stat550",
    "section": "Example 2: Predicting future COVID cases",
    "text": "Example 2: Predicting future COVID cases\n\nfcasters &lt;- list.files(here::here(\"code\", \"forecasters\"))\nfor (fcaster in fcasters) source(here::here(\"code\", \"forecasters\", fcaster))\nregistry_path &lt;- here::here(\"data\", \"forecast-experiments\")\nsource(here::here(\"code\", \"common-pars.R\"))\n\n# Setup the data ----------------------------------------------------\nreg &lt;- makeExperimentRegistry(\n  registry_path,\n  packages = c(\"tidyverse\", \"covidcast\"),\n  source = c(\n    here::here(\"code\", \"forecasters\", fcasters), \n    here::here(\"code\", \"common-pars.R\")\n  )\n)\n\ngrab_data &lt;- function(data, job, forecast_date, ...) {\n  dat &lt;- covidcast_signals(\n    data_sources, signals, as_of = forecast_date, \n    geo_type = geo_type, start_day = \"2020-04-15\") %&gt;% \n    aggregate_signals(format = \"wide\") \n  names(dat)[3:5] &lt;- c(\"value\", \"num\", \"covariate\") # assumes 2 signals\n  dat %&gt;% \n    filter(!(geo_value %in% drop_geos)) %&gt;% \n    group_by(geo_value) %&gt;% \n    arrange(time_value)\n}\naddProblem(\"covidcast_proper\", fun = grab_data, cache = TRUE)\n\n# Algorithm wrappers -----------------------------------------------------\nbaseline &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, value) %&gt;% \n    group_modify(prob_baseline, ...)\n}\nar &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, time_value, value) %&gt;% \n    group_modify(prob_ar, ...)\n}\nqar &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, time_value, value) %&gt;% \n    group_modify(quant_ar, ...)\n}\ngam &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, time_value, value) %&gt;%\n    group_modify(safe_prob_gam_ar, ...)\n}\nar_cov &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    group_modify(prob_ar_cov, ...)\n}\njoint &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, time_value, value) %&gt;% \n    joint_ar(...)\n}\ncorrected_ar &lt;- function(data, job, instance, ...) {\n  instance %&gt;% \n    dplyr::select(geo_value, time_value, num) %&gt;% \n    rename(value = num) %&gt;% \n    corrections_single_signal(cparams) %&gt;% \n    group_modify(prob_ar, ...)\n}\n\naddAlgorithm(\"baseline\", baseline)\naddAlgorithm(\"ar\", ar)\naddAlgorithm(\"qar\", qar)\naddAlgorithm(\"gam\", gam)\naddAlgorithm(\"ar_cov\", ar_cov)\naddAlgorithm(\"joint_ar\", joint)\naddAlgorithm(\"corrections\", corrected_ar)\n\n# Experimental design -----------------------------------------------------\nproblem_design &lt;- list(covidcast_proper = data.frame(forecast_date = forecast_dates))\nalgorithm_design &lt;- list(\n  baseline = CJ(\n    train_window = train_windows, min_train_window = min(train_windows), ahead = aheads\n  ),\n  ar = CJ(\n    train_window = train_windows, min_train_window = min(train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  qar = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads\n  ),\n  gam = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads, df = gam_df\n  ),\n  ar_cov = CJ(\n    train_window = train_windows, min_train_window = min(train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  joint_ar = CJ(\n    train_window = joint_train_windows, min_train_window = min(joint_train_windows), \n    lags = lags_list, ahead = aheads\n  ),\n  corrections = CJ(\n    train_window = train_windows, min_train_window = min(train_windows),\n    lags = lags_list, ahead = aheads\n  )\n)\n\naddExperiments(problem_design, algorithm_design)\nids &lt;- unwrap(getJobPars()) %&gt;% \n  select(job.id, forecast_date) %&gt;% \n  mutate(chunk = as.integer(as.factor(forecast_date))) %&gt;% \n  select(-forecast_date)\n\n## ~13000 jobs, we don't want to submit that many since they run fast\n## Chunk them into groups by forecast_date (to download once for the group)\n## Results in 68 chunks\n\nsubmitJobs(\n  ids, \n  resources = list(ncpus = 1, walltime = \"4:00:00\", memory = \"16G\")\n)"
  },
  {
    "objectID": "schedule/slides/cluster-computing.html#takeaways",
    "href": "schedule/slides/cluster-computing.html#takeaways",
    "title": "UBC Stat550",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nBenefits of this workflow:\n\nDon’t lock up your computer\nStuff runs much faster\nCan easily scale up to many jobs\nLogs are stored for debugging\nForces you to think about the design\nNo overhead to store results\nEasy to add more experiments later, adjust parameters, etc.\n\n\n\nCosts:\n\nI only know how to do this in R\nOverhead of moving between machines\nSome headaches to understand the syntax\n\n\n\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/handouts/report-formatting.html",
    "href": "schedule/handouts/report-formatting.html",
    "title": "Formatting consulting reports",
    "section": "",
    "text": "Note\n\n\n\nThis advice comes from Prof. Harry Joe."
  },
  {
    "objectID": "schedule/handouts/report-formatting.html#client-report-format",
    "href": "schedule/handouts/report-formatting.html#client-report-format",
    "title": "Formatting consulting reports",
    "section": "Client Report Format",
    "text": "Client Report Format\nThe itemized list given below should work for most reports. The report should be as short as possible with only relevant material; avoid digression and do not mention ideas that were considered but discarded.\nPerhaps start with an outline (bulleted list) of take-home messages (conclusions, statistical advice); then an outline or bulleted list of items needed to support the take-home messages. If the ordering of sections is as given below, write an outline for each section and then convert to paragraphs. This should help to avoid tangential sentences (material not needed to support the statistical advice).\nThe format and most guidelines below are also good for term project reports, presentations, and research articles/reports.\nReports for regular clients: 4 to 6 pages; an appendix of several additional pages (e.g. sample R code) could be added if relevant."
  },
  {
    "objectID": "schedule/handouts/report-formatting.html#ordering-of-sections-of-report.",
    "href": "schedule/handouts/report-formatting.html#ordering-of-sections-of-report.",
    "title": "Formatting consulting reports",
    "section": "Ordering of sections of report.",
    "text": "Ordering of sections of report.\n\nSuggested Outline\n\nAbstract or executive summary with the “big picture” (motivation) and the most important advice (results).\nIntroduction. Clearly state the scientific objective(s) of the study.\nData description and collection. Provide the important details on the nature of the data and how they were collected. Statistical issues or questions. State the statistical questions the client wants answered (or should want to have answered).\nProposed statistical methods and results.\n\nDescriptive tools: Suggest appropriate tables and figures for summarizing the data; include example figures relevant to the project (use simulated data if you have to)\nAnalytical techniques: Avoid using formulas to the maximal extent possible (supply relevant references instead); explain ideas along with an example and perhaps sample output relevant to the project\nAny statistically significant result should be supported with appropriate graphs or tables.\n\nConclusion. Summarize your recommendations.\nFurther reading. Include references that you know are at the level your client will understand.\nAppendix. If the client wants formulas and computer code, this is the best place to put it\n\n\n\nAdditional advice\n\nDon’t put lengthy computer code or output in the body of your report. If you want to provide some details on how to carry out the analysis you are recommending, put this in the appendix. Ideally, this would be in the software the client plans to use.\nDon’t refer the client to references for explanations or examples without having given your own.\nDo communicate clearly. Avoiding formulas and using relevant examples and figures to back up your explanations are probably the best way to ensure your client will understand your advice.\nDo not use too many digits in your report. Typically numerical output of statistical software requires truncation of the number of decimal places.\nThe captions of graphs and tables should be self-contained enough so that the meaning is clear without reading the body of the report.\nUnits of all variables should be included in summaries etc."
  },
  {
    "objectID": "schedule/handouts/report-formatting.html#suggestions-for-consulting-reports-for-non-statisticians",
    "href": "schedule/handouts/report-formatting.html#suggestions-for-consulting-reports-for-non-statisticians",
    "title": "Formatting consulting reports",
    "section": "Suggestions for Consulting Reports for Non-Statisticians",
    "text": "Suggestions for Consulting Reports for Non-Statisticians\n\nOverall style\n\nReport should be written to your client, so must use language your client can understand. Use plain English and avoid statistical technical terms, such as “generalized linear model” (better would be binary regression or count regression or ordinal regression depending on the response type) and “hierarchical model”.\nReport should focus on motivation, explanation and interpretation. Your client needs to understand (conceptually) the nature of the analyses you are suggesting/presenting, and it is most important to convey clearly what interpretations are possible based on your suggested analyses.\nSections should be organized to discuss simpler approaches first and more complicated approaches later, using the discussion of the simpler approaches to build up to the more complicated approaches. For example, for regression, could start with one predictor before multiple predictors.\nReport should not include verbatim copy of computer output (for example, R markdown is inappropriate except possibly in the appendix).\nReport should include only essential tables and figures (not everything that you produced from your code). Include only what is needed to support the advice/conclusions.\nNever say that an assumption or hypothesis has been verified. You can say that an assumption of statistical procedure can be assessed with some graphical method.\nIf assumptions about a statistical procedure are included, be precise in stating them without using notation. For example, for the paired-t procedure, saying “the data are assumed to be normally distributed” is imprecise; the precise assumption is that “differences of .. and .. are assumed to be normally distributed”\n\n\n\nAnalysis Strategies\n\nAlways carry out detailed exploratory analyses before any formal analyses.\nAvoid over-reliance on hypothesis testing; estimation via confidence intervals or out-of-sample assessment of prediction ability are much more informative.\nBe careful with pseudo-replication.\nFor statistical methodology, it is better to understand methods based on statistical theory from your previous courses. Avoid use of a search engine for a methodology to solve a problem. It is not acceptable to only provide R (or SAS/python) functions/code as advice for a client. Anything software must be backed up with brief explanations of the methodology in non-technical language. Because there are errors and unclear documentation in many R functions (including non-standard uses of lm(), glm(), anova() etc), when you are using an R function for the first time, please verify most of the output via direct coding of a few lines of R code. Note that output of maximum likelihood estimation can be verified via nlm() (nonlinear minimization function), with input of the negative log-likelihood.\nFor reference of statistical methodology to a client, provide a book reference rather than a web site.\nIf you plan to continue to do research in MSc or PhD program, you will be better prepared if you come up with your own ideas for statistical methods for different data situations and write your own code. That is, if you can develop and code existing methods on your own, this is a step to developing and coding new methods in original statistical research.\n\n\n\nInterpretation of Results\n\nBe very careful when discussing statistical inference based on observational data, when there is no randomization or sampling basis for carrying out the inference. To what population does the inference apply? Keep in mind that your inference is entirely model-based in such situations.\nYour summary section should include a frank and careful discussion of any concerns you may have about your recommended approaches to the client’s problem.\n\n\n\nProofreading and notation\n\nGeneral: Proofread your report very carefully for clarity of wording, optimal organization, and grammatical errors before you submit it for review. Use a spellchecker to check for spelling errors.\nAvoid abbreviations in text. If an abbreviation is needed, show the full term before the first occurrence of the abbreviation.\nIn this course, the word ‘data’ should be considered as plural for grammatical considerations. (The singular form in Latin is ‘datum’).\nIf necessary, information will be posted on correct use of mathematical notation without overloading symbols and abusing notation for functions and random variables."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": " Lecture slides and handouts",
    "section": "",
    "text": "Term 2 runs 8 January to 13 April."
  },
  {
    "objectID": "schedule/index.html#lecture-slides",
    "href": "schedule/index.html#lecture-slides",
    "title": " Lecture slides and handouts",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nGiving presentations\nGit and GitHub\nTips for organization\nUnit tests\nBootstrap\nTime series\nCluster computing\nModel selection\nRegularized linear models\nSkills for graduate students"
  },
  {
    "objectID": "schedule/index.html#handouts",
    "href": "schedule/index.html#handouts",
    "title": " Lecture slides and handouts",
    "section": "Handouts",
    "text": "Handouts\n\nRecommended books and sources\nFormatting consulting reports"
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "UBC Stat550",
    "section": "Course goals",
    "text": "Course goals\nThis graduate-level course is designed to serve two overlapping purposes: (1) help students prepare for the Consulting Practicum Course (STAT 551) and/or work with ASDa and (2) give students appropriate professional skills for research, thesis writing, other projects. The major goals are to\n\nRead, understand, explain academic statistical concepts to non-statisticians\nGive high-quality presentations\nLearn how to convert subject-matter questions into statistical ones.\nCommunicate statistical conclusions carefully and rigorously, in writing and presentations\nUse reasonable statistical workflows\nConduct quality computer experiments\nUse version control effectively\nWrite verifiable, reproducible code\n\nClass time will contain some lectures and some guest lectures as well as student presentations and workshops."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "Important\n\n\n\nSee Canvas for the full syllabus."
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": " Syllabus",
    "section": "Course info",
    "text": "Course info\nCo-Instructors:\nLang Wu and Daniel McDonald\nOffice hours:\nI’m happy to arrange time as needed.\nLectures:\nTue/Thu 14:30 - 16:00"
  },
  {
    "objectID": "syllabus.html#important-considerations",
    "href": "syllabus.html#important-considerations",
    "title": " Syllabus",
    "section": "Important considerations",
    "text": "Important considerations\n\nUniversity policies\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here.\n\n\nAcademic honesty and standards\nUBC Vancouver Statement\nAcademic honesty is essential to the continued functioning of the University of British Columbia as an institution of higher learning and research. All UBC students are expected to behave as honest and responsible members of an academic community. Breach of those expectations or failure to follow the appropriate policies, principles, rules, and guidelines of the University with respect to academic honesty may result in disciplinary action.\nFor the full statement, please see the 2020/21 Vancouver Academic Calendar\n\n\nAcademic Concessions\nThese are handled according to UBC policy. Please see\n\nUBC student services\nUBC Vancouver Academic Calendar\nFaculty of Science Concessions\n\n\n\nTake care of yourself\nCourse work at this level can be intense, and I encourage you to take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. I struggle with these issues too, and I try hard to set aside time for things that make me happy (cooking, playing/listening to music, exercise, going for walks).\nAll of us benefit from support during times of struggle. If you are having any problems or concerns, do not hesitate to speak with me. There are also many resources available on campus that can provide help and support. Asking for support sooner rather than later is almost always a good idea.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support. UBC Counseling Services is here to help: call 604 822 3811 or visit their website. Consider also reaching out to a friend, faculty member, or family member you trust to help get you the support you need."
  },
  {
    "objectID": "schedule/handouts/reference-books.html",
    "href": "schedule/handouts/reference-books.html",
    "title": "Recommended books",
    "section": "",
    "text": "These are Prof. Harry Joe’s suggestions for resources on different statistical topics that students might not be exposed to before Stat 550/551.\n\nThe student can also learn about the topic and not just about how an R function works.\nBooks with R in the title are generally not good for learning the theory.\nA book reference with theory and good examples is better than a URL.\n\n\nANOVA for different experimental designs (includes random effects, unbalanced).\n\nStatistical Analysis of Designed Experiments, A C. Tamhane (2009), Wiley.\n\nLogistic regression and diagnostics.\n\nApplied Logistic Regression, 2nd ed, Hosmer and Lemeshow (2000), Wiley.\n\nSurvival or time-to-event data.\n\nStatistical Models and Methods for Lifetime Data, 2nd ed, J.F. Lawless (2003), Wiley.\n\nTime series including multivariate.\n\nTime Series Analysis, Univariate and multivariate methods, 2nd ed., W.W.S. Wei (1990), Addison.\n\nFactor models as part of multivariate statistics.\n\nApplied Multivariate Statistical Analysis, 5th ed, Johnson and Wichern (2002), Prentice Hall (not e-available).\n\nSEM = Structural equation models.\n\nLinear Causal Modeling with Structural Equations, S.A. Mulaik (2009), Chapman&Hall/CRC.\n\nMeta-analysis.\n\nMeta-Analysis of Controlled Clinical Trials, A. Whitehead (2002), Wiley.\n\nNumerical maximum likelihood.\n\nNonlinear Parameter Optimization Using R Tools, J.C. Nash (2014), Wiley.\n\nItem response, validity and reliability of instruments for abstract attributes.\n\nTest Theory: A Unified Treatment, R.D. McDonald (1999), Routledge.\n\nSample size calculation. e.g. non-central t, non-central F etc.,. and general approach.\n\nStatistical power analysis : a simple and general model for traditional and modern hypothesis tests, 4th edition. Kevin R. Murphy, Brett Myors, Allen Wolach (2014), Routledge.\n\n\n\n\n\nThis book covers many cases. However the principle of sample size calculation can be shown better in a few pages based on examples that show the derivation of the distribution of the test statistic under a point null hypothesis and under a point alternative hypothesis.\n\nSurvey sampling\n\nSampling Design and Analysis, Second Edition. Sharon L. Lohr, CRC 2019.\n\n\nEspecially Chapter 8 Nonresponse; Section 8.3 Callbacks and Two-Phase Sampling.\n\n\n\n\n\nTRC Market Research, Non-Response Bias In Survey Sampling.\n\nMachine learning procedures\n\nElements of Statistical Learning, 2nd edition. Hastie, Tibshirani, and Friedman (2009), Springer."
  },
  {
    "objectID": "schedule/slides/bootstrap.html#section",
    "href": "schedule/slides/bootstrap.html#section",
    "title": "UBC Stat550",
    "section": "The bootstrap",
    "text": "The bootstrap\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#in-statistics",
    "href": "schedule/slides/bootstrap.html#in-statistics",
    "title": "UBC Stat550",
    "section": "In statistics…",
    "text": "In statistics…\nThe “bootstrap” works. And well.\nIt’s good for “second-level” analysis.\n\n“First-level” analyses are things like \\(\\hat\\beta\\), \\(\\hat y\\), an estimator of the center (a median), etc.\n“Second-level” are things like \\(\\Var{\\hat\\beta}\\), a confidence interval for \\(\\hat y\\), or a median, etc.\n\nYou usually get these “second-level” properties from “the sampling distribution of an estimator”\n\nBut what if you don’t know the sampling distribution? Or you’re skeptical of the CLT argument?"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#refresher-on-sampling-distributions",
    "href": "schedule/slides/bootstrap.html#refresher-on-sampling-distributions",
    "title": "UBC Stat550",
    "section": "Refresher on sampling distributions",
    "text": "Refresher on sampling distributions\n\nIf \\(X_i\\) are iid Normal \\((0,\\sigma^2)\\), then \\(\\Var{\\bar{X}} = \\sigma^2 / n\\).\nIf \\(X_i\\) are iid and \\(n\\) is big, then \\(\\Var{\\bar{X}} \\approx \\Var{X_1} / n\\).\nIf \\(X_i\\) are iid Binomial \\((m, p)\\), then \\(\\Var{\\bar{X}} = mp(1-p) / n\\)"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#example-of-unknown-sampling-distribution",
    "href": "schedule/slides/bootstrap.html#example-of-unknown-sampling-distribution",
    "title": "UBC Stat550",
    "section": "Example of unknown sampling distribution",
    "text": "Example of unknown sampling distribution\nI estimate a LDA on some data.\nI get a new \\(x_0\\) and produce \\(\\hat{Pr}(y_0 =1 \\given x_0)\\).\nCan I get a 95% confidence interval for \\(Pr(y_0=1 \\given x_0)\\)?\n\nThe bootstrap gives this to you."
  },
  {
    "objectID": "schedule/slides/bootstrap.html#procedure",
    "href": "schedule/slides/bootstrap.html#procedure",
    "title": "UBC Stat550",
    "section": "Procedure",
    "text": "Procedure\n\nResample your training data w/ replacement.\nCalculate a LDA on this sample.\nProduce a new prediction, call it \\(\\widehat{Pr}_b(y_0 =1 \\given x_0)\\).\nRepeat 1-3 \\(b = 1,\\ldots,B\\) times.\nCI: \\(\\left[2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(1-\\alpha/2),\\ 2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(\\alpha/2)\\right]\\)\n\n\\(\\hat{F}\\) is the “empirical” distribution of the bootstraps."
  },
  {
    "objectID": "schedule/slides/bootstrap.html#very-basic-example",
    "href": "schedule/slides/bootstrap.html#very-basic-example",
    "title": "UBC Stat550",
    "section": "Very basic example",
    "text": "Very basic example\n\nLet \\(X_i\\sim Exponential(1/5)\\). The pdf is \\(f(x) = \\frac{1}{5}e^{-x/5}\\)\nI know if I estimate the mean with \\(\\bar{X}\\), then by the CLT (if \\(n\\) is big),\n\n\\[\\frac{\\sqrt{n}(\\bar{X}-E[X])}{s} \\approx N(0, 1).\\]\n\nThis gives me a 95% confidence interval like \\[\\bar{X} \\pm 2 \\frac{s}{\\sqrt{n}}\\]\nBut I don’t want to estimate the mean, I want to estimate the median."
  },
  {
    "objectID": "schedule/slides/bootstrap.html#now-what",
    "href": "schedule/slides/bootstrap.html#now-what",
    "title": "UBC Stat550",
    "section": "Now what",
    "text": "Now what\n\nI give you a sample of size 500, you give me the sample median.\nHow do you get a CI?\nYou can use the bootstrap!\n\n\nset.seed(2022-11-01)\nx &lt;- rexp(n, 1 / 5)\n(med &lt;- median(x)) # sample median\n\n[1] 3.669627\n\nB &lt;- 100\nalpha &lt;- 0.05\nbootMed &lt;- function() median(sample(x, replace = TRUE)) # resample, and get the median\nFhat &lt;- replicate(B, bootMed()) # repeat B times, \"empirical distribution\"\nCI &lt;- 2 * med - quantile(Fhat, probs = c(1 - alpha / 2, alpha / 2))"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#slightly-harder-example",
    "href": "schedule/slides/bootstrap.html#slightly-harder-example",
    "title": "UBC Stat550",
    "section": "Slightly harder example",
    "text": "Slightly harder example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = Hwt ~ 0 + Bwt, data = fatcats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.9293 -1.0460 -0.1407  0.8298 16.2536 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nBwt  3.81895    0.07678   49.74   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.549 on 143 degrees of freedom\nMultiple R-squared:  0.9454,    Adjusted R-squared:  0.945 \nF-statistic:  2474 on 1 and 143 DF,  p-value: &lt; 2.2e-16\n\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "href": "schedule/slides/bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "title": "UBC Stat550",
    "section": "When we fit models, we examine diagnostics",
    "text": "When we fit models, we examine diagnostics\n\n\n\n\n\n\n\n\n\n\n\nThe tails are too fat, I don’t believe that CI…\n\nWe bootstrap\n\nB &lt;- 500\nbhats &lt;- double(B)\nalpha &lt;- .05\nfor (b in 1:B) {\n  samp &lt;- sample(1:nrow(fatcats), replace = TRUE)\n  newcats &lt;- fatcats[samp, ] # new data\n  bhats[b] &lt;- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n\n   97.5%     2.5% \n3.654977 3.955927 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#an-alternative",
    "href": "schedule/slides/bootstrap.html#an-alternative",
    "title": "UBC Stat550",
    "section": "An alternative",
    "text": "An alternative\n\nSo far, I didn’t use any information about the data-generating process.\nWe’ve done the non-parametric bootstrap\nThis is easiest, and most common for most cases.\n\n\nBut there’s another version\n\nYou could try a “parametric bootstrap”\nThis assumes knowledge about the DGP"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#same-data",
    "href": "schedule/slides/bootstrap.html#same-data",
    "title": "UBC Stat550",
    "section": "Same data",
    "text": "Same data\n\n\nNon-parametric bootstrap\nSame as before\n\nB &lt;- 500\nbhats &lt;- double(B)\nalpha &lt;- .05\nfor (b in 1:B) {\n  samp &lt;- sample(1:nrow(fatcats), replace = TRUE)\n  newcats &lt;- fatcats[samp, ] # new data\n  bhats[b] &lt;- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # NP Bootstrap CI\n  quantile(bhats, probs = c(1-alpha/2, alpha/2))\n\n   97.5%     2.5% \n3.673559 3.970251 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %  97.5 %\nBwt 3.667178 3.97073\n\n\n\nParametric bootstrap\n\nAssume that the linear model is TRUE.\nThen, \\(\\texttt{Hwt}_i = \\widehat{\\beta}\\times \\texttt{Bwt}_i + \\widehat{e}_i\\), \\(\\widehat{e}_i \\approx \\epsilon_i\\)\nThe \\(\\epsilon_i\\) is random \\(\\longrightarrow\\) just resample \\(\\widehat{e}_i\\).\n\n\nB &lt;- 500\nbhats &lt;- double(B)\nalpha &lt;- .05\ncats.lm &lt;- lm(Hwt ~ 0 + Bwt, data = fatcats)\nnewcats &lt;- fatcats\nfor (b in 1:B) {\n  samp &lt;- sample(residuals(cats.lm), replace = TRUE)\n  newcats$Hwt &lt;- predict(cats.lm) + samp # new data\n  bhats[b] &lt;- coef(lm(Hwt ~ 0 + Bwt, data = newcats)) \n}\n\n2 * coef(cats.lm) - # Parametric Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha/2, alpha/2))\n\n   97.5%     2.5% \n3.665531 3.961896"
  },
  {
    "objectID": "schedule/slides/bootstrap.html#bootstrap-error-sources",
    "href": "schedule/slides/bootstrap.html#bootstrap-error-sources",
    "title": "UBC Stat550",
    "section": "Bootstrap error sources",
    "text": "Bootstrap error sources\nSimulation error:\nusing only \\(B\\) samples to estimate \\(F\\) with \\(\\hat{F}\\).\nStatistical error:\nour data depended on a sample from the population. We don’t have the whole population so we make an error by using a sample\n(Note: this part is what always happens with data, and what the science of statistics analyzes.)\nSpecification error:\nIf we use the parametric bootstrap, and our model is wrong, then we are overconfident.\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/git.html#section",
    "href": "schedule/slides/git.html#section",
    "title": "UBC Stat550",
    "section": "Version control",
    "text": "Version control\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/git.html#why-version-control",
    "href": "schedule/slides/git.html#why-version-control",
    "title": "UBC Stat550",
    "section": "Why version control?",
    "text": "Why version control?\n\nMuch of this lecture is based on material from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "schedule/slides/git.html#why-version-control-1",
    "href": "schedule/slides/git.html#why-version-control-1",
    "title": "UBC Stat550",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWords of wisdom\n\n\nYour closest collaborator is you six months ago, but you don’t reply to emails.\n– Paul Wilson"
  },
  {
    "objectID": "schedule/slides/git.html#why-git",
    "href": "schedule/slides/git.html#why-git",
    "title": "UBC Stat550",
    "section": "Why Git",
    "text": "Why Git\n\n\n\nYou could use something like Box or Dropbox\nThese are poor-man’s version control\nGit is much more appropriate\nIt works with large groups\nIt’s very fast\nIt’s much better at fixing mistakes\nTech companies use it (so it’s in your interest to have some experience)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis will hurt, but what doesn’t kill you, makes you stronger."
  },
  {
    "objectID": "schedule/slides/git.html#overview",
    "href": "schedule/slides/git.html#overview",
    "title": "UBC Stat550",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "schedule/slides/git.html#aside-on-built-in-command-line",
    "href": "schedule/slides/git.html#aside-on-built-in-command-line",
    "title": "UBC Stat550",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "schedule/slides/git.html#typical-workflow",
    "href": "schedule/slides/git.html#typical-workflow",
    "title": "UBC Stat550",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\ngit clone https://github.com/stat550-2021/lecture-slides.git\n\nCreate a branch\n\ngit branch &lt;branchname&gt;\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\ngit add &lt;name/of/tracked/file&gt;\n\nCommit your changes\n\ngit commit -m \"Some explanatory message\"\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to GitHub\n\ngit push\ngit push -u origin &lt;branchname&gt;"
  },
  {
    "objectID": "schedule/slides/git.html#what-should-be-tracked",
    "href": "schedule/slides/git.html#what-should-be-tracked",
    "title": "UBC Stat550",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nDefinitely\n\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\n\n\n\nPossibly\n\nlogs, jupyter notebooks, images (that won’t change), …\n\n\n\n\nQuestionable\n\nprocessed data, static pdfs, …\n\n\n\n\nDefinitely not\n\nfull data, continually updated pdfs, other things compiled from source code, …"
  },
  {
    "objectID": "schedule/slides/git.html#what-things-to-track",
    "href": "schedule/slides/git.html#what-things-to-track",
    "title": "UBC Stat550",
    "section": "What things to track",
    "text": "What things to track\n\nYou decide what is “versioned”.\nA file called .gitignore tells git files or types to never track\n\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n\nShortcut to track everything (use carefully):\n\ngit add ."
  },
  {
    "objectID": "schedule/slides/git.html#whats-a-pr",
    "href": "schedule/slides/git.html#whats-a-pr",
    "title": "UBC Stat550",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/git.html#some-things-to-be-aware-of",
    "href": "schedule/slides/git.html#some-things-to-be-aware-of",
    "title": "UBC Stat550",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 9 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "schedule/slides/git.html#the-maindevelopbranch-workflow",
    "href": "schedule/slides/git.html#the-maindevelopbranch-workflow",
    "title": "UBC Stat550",
    "section": "The main/develop/branch workflow",
    "text": "The main/develop/branch workflow\n\nWhen working on your own\n\nDon’t NEED branches (but you should use them, really)\nI make a branch if I want to try a modification without breaking what I have.\n\nWhen working on a large team with production grade software\n\nmain is protected, released version of software (maybe renamed to release)\ndevelop contains things not yet on main, but thoroughly tested\nOn a schedule (once a week, once a month) develop gets merged to main\nYou work on a feature branch off develop to build your new feature\nYou do a PR against develop. Supervisors review your contributions\n\n\n\nI and many DS/CS/Stat faculty use this workflow with my lab."
  },
  {
    "objectID": "schedule/slides/git.html#protection",
    "href": "schedule/slides/git.html#protection",
    "title": "UBC Stat550",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance\n\n\n\n\n\n\n\nTip\n\n\nI suggest (require?) you adopt the “production” version for your HW 2"
  },
  {
    "objectID": "schedule/slides/git.html#operations-in-rstudio",
    "href": "schedule/slides/git.html#operations-in-rstudio",
    "title": "UBC Stat550",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\nCovers:\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\nCommand line versions (of the same)\ngit add &lt;name/of/file&gt;\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b &lt;name/of/branch&gt;"
  },
  {
    "objectID": "schedule/slides/git.html#other-useful-stuff-but-command-line-only",
    "href": "schedule/slides/git.html#other-useful-stuff-but-command-line-only",
    "title": "UBC Stat550",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\ngit config user.name --global \"Daniel J. McDonald\"\ngit config user.email --global \"daniel@stat.ubc.ca\"\ngit config core.editor --global nano \n# or emacs or ... (default is vim)\nStaging\ngit add name/of/file # stage 1 file\ngit add . # stage all\nCommitting\n# stage/commit simultaneously\ngit commit -am \"message\" \n\n# open editor to write long commit message\ngit commit \nPushing\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n\n\nBranching\n# switch to branchname, error if uncommitted changes\ngit checkout branchname \n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\nCheck the status\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs"
  },
  {
    "objectID": "schedule/slides/git.html#commit-messages",
    "href": "schedule/slides/git.html#commit-messages",
    "title": "UBC Stat550",
    "section": "Commit messages",
    "text": "Commit messages\n\n\n\n\n\n\n\nWrite meaningful messages. Not fixed stuff or oops? maybe done?\nThese appear in the log and help you determine what you’ve done.\nThink imperative mood: “add cross validation to simulation”\nBest to have each commit “do one thing”\n\n\n\n\nConventions: (see here for details)\n\nfeat: – a new feature is introduced with the changes\nfix: – a bug fix has occurred\nchore: – changes that do not relate to a fix or feature (e.g., updating dependencies)\nrefactor: – refactored code that neither fixes a bug nor adds a feature\ndocs: – updates to documentation such as a the README or other markdown files\nstyle: – changes that do not affect the function of the code\ntest – including new or correcting previous tests\nperf – performance improvements\nci – continuous integration related\n\ngit commit -m \"feat: add cross validation to simulation, closes #251\""
  },
  {
    "objectID": "schedule/slides/git.html#conflicts",
    "href": "schedule/slides/git.html#conflicts",
    "title": "UBC Stat550",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\nThey look like this:\nHere are lines that are either unchanged\nfrom the common ancestor, or cleanly\nresolved because only one side changed.\n\nBut below we have some troubles\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs:sample.txt\n\nAnd here is another line that is cleanly \nresolved or unmodified.\n\n\nYou decide what to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nAlways delete the &lt;&lt;&lt;&lt;&lt;, ======, and &gt;&gt;&gt;&gt;&gt; lines.\nOnce you’re satisfied, commit to resolve the conflict."
  },
  {
    "objectID": "schedule/slides/git.html#some-other-pointers",
    "href": "schedule/slides/git.html#some-other-pointers",
    "title": "UBC Stat550",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "schedule/slides/git.html#how-to-undo-in-3-scenarios",
    "href": "schedule/slides/git.html#how-to-undo-in-3-scenarios",
    "title": "UBC Stat550",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\nIn RStudio, select the file and click   then select  Revert…\n# grab the old committed version\ngit checkout -- README.md \n2. Staged but not committed\nIn RStudio, uncheck the box by the file, then use the method above.\n# first unstage, then same as 1\ngit reset HEAD README.md\ngit checkout -- README.md\n\n\n3. Committed\nNot easy to do in RStudio…\n# check the log to find the chg \ngit log\n# go one step before that \n# (e.g., to commit 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n\n# alternatively, if it happens\n# to also be on another branch\ngit checkout otherbranch -- README.md"
  },
  {
    "objectID": "schedule/slides/git.html#recovering-from-things",
    "href": "schedule/slides/git.html#recovering-from-things",
    "title": "UBC Stat550",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main,\n\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# find out where to go to\ngit log\n# undo everything after ace2193\ngit reset --hard ace2193\ngit checkout newbranch\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\ngit checkout main\ngit branch -d badbranch\n\nAnything more complicated, either post to Slack or LMGTFY"
  },
  {
    "objectID": "schedule/slides/git.html#rules-for-hw-2",
    "href": "schedule/slides/git.html#rules-for-hw-2",
    "title": "UBC Stat550",
    "section": "Rules for HW 2",
    "text": "Rules for HW 2\n\nEach team has their own repo\nMake a PR against main to submit\nTag me and all the assigned reviewers\nPeer evaluations are done via PR review (also send to Estella)\nYOU must make at least 5 commits (fewer will lead to deductions)\nI review your work and merge the PR\n\n\n\n\n\n\n\nImportant\n\n\n☠️☠️ Read all the instructions in the repo! ☠️☠️"
  },
  {
    "objectID": "schedule/slides/model-selection.html#section",
    "href": "schedule/slides/model-selection.html#section",
    "title": "UBC Stat550",
    "section": "Statistical models and model selection",
    "text": "Statistical models and model selection\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/model-selection.html#what-is-a-model",
    "href": "schedule/slides/model-selection.html#what-is-a-model",
    "title": "UBC Stat550",
    "section": "What is a model?",
    "text": "What is a model?\nIn statistics, “model” has a mathematical meaning.\nDistinct from “algorithm” or “procedure”.\nDefining a model often leads to a procedure/algorithm with good properties.\nSometimes procedure/algorithm \\(\\Rightarrow\\) a specific model.\n\nStatistics (the field) tells me how to understand when different procedures are desirable and the mathematical guarantees that they satisfy.\n\nWhen are certain models appropriate?\n\nOne definition of “Statistical Learning” is the “statistics behind the procedure”."
  },
  {
    "objectID": "schedule/slides/model-selection.html#statistical-models-101",
    "href": "schedule/slides/model-selection.html#statistical-models-101",
    "title": "UBC Stat550",
    "section": "Statistical models 101",
    "text": "Statistical models 101\nWe observe data \\(Z_1,\\ Z_2,\\ \\ldots,\\ Z_n\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\n\nA statistical model is a set of distributions \\(\\mathcal{P}\\).\n\nSome examples:\n\n\\(\\P = \\{ 0 &lt; p &lt; 1 : P(z=1)=p,\\ P(z=0)=1-p\\}\\).\n\\(\\P = \\{ \\beta \\in \\R^p,\\ \\sigma&gt;0 : Y \\sim N(X^\\top\\beta,\\sigma^2),\\  X\\mbox{ fixed}\\}\\).\n\\(\\P = \\{\\mbox{all CDF's }F\\}\\).\n\\(\\P = \\{\\mbox{all smooth functions } f: \\R^p \\rightarrow \\R : Z_i = (X_i, Y_i),\\ E[Y_i] = f(X_i) \\}\\)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#statistical-models",
    "href": "schedule/slides/model-selection.html#statistical-models",
    "title": "UBC Stat550",
    "section": "Statistical models",
    "text": "Statistical models\nWe want to use the data to select a distribution \\(P\\) that probably generated the data.\n\nMy model:\n\\[\n\\P = \\{ P(z=1)=p,\\ P(z=0)=1-p,\\ 0 &lt; p &lt; 1 \\}\n\\]\n\nTo completely characterize \\(P\\), I just need to estimate \\(p\\).\nNeed to assume that \\(P \\in \\P\\).\nThis assumption is mostly empty: need independent, can’t see \\(z=12\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#statistical-models-1",
    "href": "schedule/slides/model-selection.html#statistical-models-1",
    "title": "UBC Stat550",
    "section": "Statistical models",
    "text": "Statistical models\nWe observe data \\(Z_i=(Y_i,X_i)\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\n\nMy model\n\\[\n\\P = \\{ \\beta \\in \\R^p, \\sigma&gt;0 : Y_i \\given X_i=x_i \\sim N(x_i^\\top\\beta,\\ \\sigma^2) \\}.\n\\]\n\nTo completely characterize \\(P\\), I just need to estimate \\(\\beta\\) and \\(\\sigma\\).\nNeed to assume that \\(P\\in\\P\\).\nThis time, I have to assume a lot more: (conditional) Linearity, independence, conditional Gaussian noise, no ignored variables, no collinearity, etc."
  },
  {
    "objectID": "schedule/slides/model-selection.html#statistical-models-unfamiliar-example",
    "href": "schedule/slides/model-selection.html#statistical-models-unfamiliar-example",
    "title": "UBC Stat550",
    "section": "Statistical models, unfamiliar example",
    "text": "Statistical models, unfamiliar example\nWe observe data \\(Z_i \\in \\R\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\nMy model\n\\[\n\\P = \\{ Z_i \\textrm{ has a density function } f \\}.\n\\]\n\nTo completely characterize \\(P\\), I need to estimate \\(f\\).\nIn fact, we can’t hope to do this.\n\nRevised Model 1 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; M \\}\\)\nRevised Model 2 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; K &lt; M \\}\\)\nRevised Model 3 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int |f'| dx &lt;  M \\}\\)\n\nEach of these suggests different ways of estimating \\(f\\)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#assumption-lean-regression",
    "href": "schedule/slides/model-selection.html#assumption-lean-regression",
    "title": "UBC Stat550",
    "section": "Assumption Lean Regression",
    "text": "Assumption Lean Regression\nImagine \\(Z = (Y, \\mathbf{X}) \\sim P\\) with \\(Y \\in \\R\\) and \\(\\mathbf{X} = (1, X_1, \\ldots, X_p)^\\top\\).\nWe are interested in the conditional distribution \\(P_{Y|\\mathbf{X}}\\)\nSuppose we think that there is some function of interest which relates \\(Y\\) and \\(X\\).\nLet’s call this function \\(\\mu(\\mathbf{X})\\) for the moment. How do we estimate \\(\\mu\\)? What is \\(\\mu\\)?\n\n\nTo make this precise, we\n\nHave a model \\(\\P\\).\nNeed to define a “good” functional \\(\\mu\\).\nLet’s loosely define “good” as\n\n\nGiven a new (random) \\(Z\\), \\(\\mu(\\mathbf{X})\\) is “close” to \\(Y\\).\n\n\n\nSee Berk et al. Assumption Lean Regression."
  },
  {
    "objectID": "schedule/slides/model-selection.html#evaluating-close",
    "href": "schedule/slides/model-selection.html#evaluating-close",
    "title": "UBC Stat550",
    "section": "Evaluating “close”",
    "text": "Evaluating “close”\nWe need more functions.\nChoose some loss function \\(\\ell\\) that measures how close \\(\\mu\\) and \\(Y\\) are.\n\n\n\nSquared-error:\n\\(\\ell(y,\\ \\mu) = (y-\\mu)^2\\)\nAbsolute-error:\n\\(\\ell(y,\\ \\mu) = |y-\\mu|\\)\nZero-One:\n\\(\\ell(y,\\ \\mu) = I(y\\neq\\mu)=\\begin{cases} 0 & y=\\mu\\\\1 & \\mbox{else}\\end{cases}\\)\nCauchy:\n\\(\\ell(y,\\ \\mu) = \\log(1 + (y - \\mu)^2)\\)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#start-with-expected-squared-error",
    "href": "schedule/slides/model-selection.html#start-with-expected-squared-error",
    "title": "UBC Stat550",
    "section": "Start with (Expected) Squared Error",
    "text": "Start with (Expected) Squared Error\nLet’s try to minimize the expected squared error (MSE).\nClaim: \\(\\mu(X) = \\Expect{Y\\ \\vert\\ X}\\) minimizes MSE.\nThat is, for any \\(r(X)\\), \\(\\Expect{(Y - \\mu(X))^2} \\leq \\Expect{(Y-r(X))^2}\\).\n\nProof of Claim:\n\\[\\begin{aligned}\n\\Expect{(Y-r(X))^2}\n&= \\Expect{(Y- \\mu(X) + \\mu(X) - r(X))^2}\\\\\n&= \\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2\\Expect{(Y- \\mu(X))(\\mu(X) - r(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2(\\mu(X) - r(X))\\Expect{(Y- \\mu(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} + 0\\\\\n&\\geq \\Expect{(Y- \\mu(X))^2}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/model-selection.html#the-regression-function",
    "href": "schedule/slides/model-selection.html#the-regression-function",
    "title": "UBC Stat550",
    "section": "The regression function",
    "text": "The regression function\nSometimes people call this solution:\n\\[\\mu(X) = \\Expect{Y \\ \\vert\\  X}\\]\nthe regression function. (But don’t forget that it depended on \\(\\ell\\).)\nIf we assume that \\(\\mu(x) = \\Expect{Y \\ \\vert\\  X=x} = x^\\top \\beta\\), then we get back exactly OLS.\n\nBut why should we assume \\(\\mu(x) = x^\\top \\beta\\)?"
  },
  {
    "objectID": "schedule/slides/model-selection.html#brief-aside",
    "href": "schedule/slides/model-selection.html#brief-aside",
    "title": "UBC Stat550",
    "section": "Brief aside",
    "text": "Brief aside\nSome notation / terminology\n\n“Hats” on things mean “estimates”, so \\(\\widehat{\\mu}\\) is an estimate of \\(\\mu\\)\nParameters are “properties of the model”, so \\(f_X(x)\\) or \\(\\mu\\) or \\(\\Var{Y}\\)\nRandom variables like \\(X\\), \\(Y\\), \\(Z\\) may eventually become data, \\(x\\), \\(y\\), \\(z\\), once observed.\n“Estimating” means “using observations to estimate parameters”\n“Predicting” means “using observations to predict future data”\nOften, there is a parameter whose estimate will provide a prediction.\n“Non-parametric” means “we don’t assume a parametric form” for the regression function (or density)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#estimation-vs.-prediction",
    "href": "schedule/slides/model-selection.html#estimation-vs.-prediction",
    "title": "UBC Stat550",
    "section": "Estimation vs. Prediction",
    "text": "Estimation vs. Prediction\n\nIn consulting, you’re usually interested in estimating parameters accurately.\nThis is a departure from machine learning, when you want to predict new data.\nBut to “select a model”, we may have to choose between plausible alternatives.\nThis can be challenging to understand."
  },
  {
    "objectID": "schedule/slides/model-selection.html#prediction-risk-for-regression",
    "href": "schedule/slides/model-selection.html#prediction-risk-for-regression",
    "title": "UBC Stat550",
    "section": "Prediction risk for regression",
    "text": "Prediction risk for regression\nGiven the training data \\(\\mathcal{D}\\), we want to predict some independent test data \\(Z = (X,Y)\\)\nThis means forming a \\(\\hat f\\), which is a function of both the range of \\(X\\) and the training data \\(\\mathcal{D}\\), which provides predictions \\(\\hat Y = \\hat f(X)\\).\nThe quality of this prediction is measured via the prediction risk \\[R(\\hat{f}) = \\Expect{(Y - \\hat{f}(X))^2}.\\]\nWe know that the regression function, \\(\\mu(X) = \\mathbb{E}[Y \\mid X]\\), is the best possible predictor."
  },
  {
    "objectID": "schedule/slides/model-selection.html#model-selection-and-tuning-parameters",
    "href": "schedule/slides/model-selection.html#model-selection-and-tuning-parameters",
    "title": "UBC Stat550",
    "section": "Model selection and tuning parameters",
    "text": "Model selection and tuning parameters\n\nOften “model selection” means “choosing a set of predictors/variables”\n\nE.g. Lasso performs model selection by setting many \\(\\widehat\\beta=0\\)\n\n“Model selection” really means &gt; making any necessary decisions to arrive at a final model\nSometimes this means “choosing predictors”\nIt could also mean “selecting a tuning parameter”\nOr “deciding whether to use LASSO or Ridge” (and picking tuning parameters)\nModel selection means “choose \\(\\mathcal{P}\\)”"
  },
  {
    "objectID": "schedule/slides/model-selection.html#my-pet-peeve",
    "href": "schedule/slides/model-selection.html#my-pet-peeve",
    "title": "UBC Stat550",
    "section": "My pet peeve",
    "text": "My pet peeve\n\nOften people talk about “using LASSO” or “using an SVM”\nThis isn’t quite right.\nLASSO is a regularized procedure that depends on \\(\\lambda\\)\nTo “use LASSO”, you must pick a particular \\(\\lambda\\)\nDifferent ways to pick \\(\\lambda\\) (today’s topic) produce different final estimators\nThus we should say “I used LASSO + CV” or “I used Ridge + GCV”\nProbably also indicate “how” (I used the CV minimum.)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#bias-and-variance",
    "href": "schedule/slides/model-selection.html#bias-and-variance",
    "title": "UBC Stat550",
    "section": "Bias and variance",
    "text": "Bias and variance\nRecall that \\(\\mathcal{D}\\) is the training data.\n\\[R_n(f) := \\Expect{L(Y,f(X))} = \\Expect{\\Expect{L(Y,f(X)) \\given \\mathcal{D}}}\\]\n\nNote the difference between \\(R_n(f)\\;\\;\\textrm{and}\\;\\;\\Expect{L(Y,f(X)) \\given \\mathcal{D}}\\)\nIf you use \\(\\mathcal{D}\\) to choose \\(f\\), then these are different.\nIf you use \\(\\mathcal{D}\\) to choose \\(f\\), then both depend on how much data you have seen."
  },
  {
    "objectID": "schedule/slides/model-selection.html#risk-estimates",
    "href": "schedule/slides/model-selection.html#risk-estimates",
    "title": "UBC Stat550",
    "section": "Risk estimates",
    "text": "Risk estimates\n\n(Hastie, Tibshirani, and Friedman 2009)\nWe can use risk estimates for 2 different goals\n\n\nChoosing between different potential models.\nCharacterizing the out-of-sample performance of the chosen model.\n\n\nI am not generally aware of other methods of accomplishing (1)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#a-model-selection-picture",
    "href": "schedule/slides/model-selection.html#a-model-selection-picture",
    "title": "UBC Stat550",
    "section": "A model selection picture",
    "text": "A model selection picture\n\n(Hastie, Tibshirani, and Friedman 2009)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#why",
    "href": "schedule/slides/model-selection.html#why",
    "title": "UBC Stat550",
    "section": "Why?",
    "text": "Why?\nWe want to do model selection for at least three reasons:\n\nPrediction accuracy\n\nCan essentially always be improved by introducing some bias\n\nInterpretation\n\nA large number of features can sometimes be reduced to an interpretable subset\n\nComputation\n\nA large \\(p\\) can create a huge computational bottleneck."
  },
  {
    "objectID": "schedule/slides/model-selection.html#things-you-shouldnt-do",
    "href": "schedule/slides/model-selection.html#things-you-shouldnt-do",
    "title": "UBC Stat550",
    "section": "Things you shouldn’t do",
    "text": "Things you shouldn’t do\n\nEstimate \\(R_n\\) with \\(\\widehat{R}_n(f) = \\sum_{i=1}^n L(Y_i,\\widehat{f}(X_i))\\).\nThrow away variables with small \\(p\\)-values.\nUse \\(F\\)-tests\nCompare the log-likelihood between different models\n\n\nThese last two can occasionally be ok, but aren’t in general. You should investigate the assumptions that are implicit in them."
  },
  {
    "objectID": "schedule/slides/model-selection.html#unbiased-risk-estimation",
    "href": "schedule/slides/model-selection.html#unbiased-risk-estimation",
    "title": "UBC Stat550",
    "section": "Unbiased risk estimation",
    "text": "Unbiased risk estimation\n\nIt is very hard (impossible?) to estimate \\(R_n\\).\nInstead we focus on\n\n\\[\\overline{R}_n(f) = \\E_{Y_1,\\ldots,Y_n}\\left[\\E_{Y^0}\\left[\\frac{1}{n}\\sum_{i=1}^n L(Y^0_i,\\hat{f}(x_i))\\given \\mathcal{D}\\right]\\right].\\]\n\nThe difference is that \\(\\overline{R}_n(f)\\) averages over the observed \\(x_i\\) rather than taking the expected value over the distribution of \\(X\\).\nIn the “fixed design” setting, these are equal."
  },
  {
    "objectID": "schedule/slides/model-selection.html#unbiased-risk-estimation-1",
    "href": "schedule/slides/model-selection.html#unbiased-risk-estimation-1",
    "title": "UBC Stat550",
    "section": "Unbiased risk estimation",
    "text": "Unbiased risk estimation\nFor many \\(L\\) and some predictor \\(\\hat{f}\\), one can show\n\\[\\overline{R}_n(\\hat{f}) = \\Expect{\\hat{R}_n(\\hat{f})} + \\frac{2}{n} \\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}.\\]\nThis suggests estimating \\(\\overline{R}_n(\\hat{f})\\) with\n\\[\\hat{R}_{\\textrm{gic}} := \\hat{R}_n(\\hat{f}) + \\textrm{pen}.\\]\nIf \\(\\Expect{\\textrm{pen}} = \\frac{2}{n}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}\\), we have an unbiased estimator of \\(\\overline{R}_n(\\hat{f})\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#normal-means-model",
    "href": "schedule/slides/model-selection.html#normal-means-model",
    "title": "UBC Stat550",
    "section": "Normal means model",
    "text": "Normal means model\nSuppose we observe the following data:\n\\[Y_i = \\beta_i + \\epsilon_i, \\quad\\quad i=1,\\ldots,n\\]\nwhere \\(\\epsilon_i\\overset{iid}{\\sim} \\mbox{N}(0,1)\\).\nWe want to estimate \\[\\boldsymbol{\\beta} = (\\beta_1,\\ldots,\\beta_n).\\]\nThe usual estimator (MLE) is \\[\\widehat{\\boldsymbol{\\beta}}^{MLE} = (Y_1,\\ldots,Y_n).\\]\nThis estimator has lots of nice properties: consistent, unbiased, UMVUE, (asymptotic) normality…"
  },
  {
    "objectID": "schedule/slides/model-selection.html#mles-are-bad",
    "href": "schedule/slides/model-selection.html#mles-are-bad",
    "title": "UBC Stat550",
    "section": "MLEs are bad",
    "text": "MLEs are bad\nBut, the standard estimator STINKS! It’s a bad estimator.\nIt has no bias, but big variance.\n\\[R_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = \\mbox{bias}^2 + \\mbox{var} = 0\n+ n\\cdot 1= n\\]\nWhat if we use a biased estimator?\nConsider the following estimator instead: \\[\\widehat{\\beta}_i^S = \\begin{cases} Y_i & i \\in S\\\\ 0 & \\mbox{else}. \\end{cases}\\]\nHere \\(S \\subseteq \\{1,\\ldots,n\\}\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#biased-normal-means",
    "href": "schedule/slides/model-selection.html#biased-normal-means",
    "title": "UBC Stat550",
    "section": "Biased normal means",
    "text": "Biased normal means\nWhat is the risk of this estimator?\n\\[\nR_n(\\widehat{\\boldsymbol{\\beta}}^S) = \\sum_{i\\not\\in S} \\beta_i^2 + |S|.\n\\]\nIn other words, if some \\(|\\beta_i| &lt; 1\\), then don’t bother estimating them!\nIn general, introduced parameters like \\(S\\) will be called tuning parameters.\nOf course we don’t know which \\(|\\beta_i| &lt; 1\\).\nBut we could try to estimate \\(R_n(\\widehat{\\boldsymbol{\\beta}}^S)\\), and choose \\(S\\) to minimize our estimate."
  },
  {
    "objectID": "schedule/slides/model-selection.html#dangers-of-using-the-training-error",
    "href": "schedule/slides/model-selection.html#dangers-of-using-the-training-error",
    "title": "UBC Stat550",
    "section": "Dangers of using the training error",
    "text": "Dangers of using the training error\nAlthough\n\\[\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}})  \\approx R_n(\\widehat{\\boldsymbol{\\beta}}),\\] this approximation can be very bad. In fact:\n\nTraining Error\n\n\\(\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = 0\\)\n\nRisk\n\n\\(R_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = n\\)\n\n\nIn this case, the optimism of the training error is \\(n\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#normal-means",
    "href": "schedule/slides/model-selection.html#normal-means",
    "title": "UBC Stat550",
    "section": "Normal means",
    "text": "Normal means\nWhat about \\(\\widehat{\\boldsymbol{\\beta}}^S\\)?\n\\[\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S) = \\sum_{i=1}^n (\\widehat{\\beta_i}-\n  Y_i)^2 = \\sum_{i \\notin S} Y_i^2 %+ |S|\\sigma^2\\]\nWell \\[\\E\\left[\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S)\\right] =\n  R_n(\\widehat{\\boldsymbol{\\beta}}^S) - 2|S| +n.\\]\nSo I can choose \\(S\\) by minimizing \\(\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S) + 2|S|\\).\n\\[\\mbox{Estimate of Risk} = \\mbox{training error} + \\mbox{penalty}.\\]\nThe penalty term corrects for the optimism."
  },
  {
    "objectID": "schedule/slides/model-selection.html#pen-in-the-nice-cases",
    "href": "schedule/slides/model-selection.html#pen-in-the-nice-cases",
    "title": "UBC Stat550",
    "section": "pen() in the nice cases",
    "text": "pen() in the nice cases\nResult:\nSuppose \\(\\hat{f}(x_i) = HY\\) for some matrix \\(H\\), and \\(Y_i\\)’s are IID. Then\n\\[\\frac{2}{n} \\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)} = \\frac{2}{n} \\sum_{i=1}^n H_{ii} \\Cov{Y_i}{Y_i} = \\frac{2\\Var{Y}}{n} \\tr{H}.\\]\n\nSuch estimators are called “linear smoothers”.\nObvious extension to the heteroskedastic case.\nWe call \\(\\frac{1}{\\Var{Y}}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}\\) the degrees of freedom of \\(\\hat{f}\\).\nLinear smoothers are ubiquitous\nExamples: OLS, ridge regression, KNN, dictionary regression, smoothing splines, kernel regression, etc."
  },
  {
    "objectID": "schedule/slides/model-selection.html#examples-of-df",
    "href": "schedule/slides/model-selection.html#examples-of-df",
    "title": "UBC Stat550",
    "section": "Examples of DF",
    "text": "Examples of DF\n\nOLS\n\n\\[H = X^\\top (X^\\top X)^{-1} X^\\top \\Rightarrow \\tr{H} = \\textrm{rank}(X) = p\\]\n\nRidge (decompose \\(X=UDV^\\top\\))\n\n\\[H = X^\\top (X^\\top X + \\lambda I_p)^{-1} X^\\top \\Rightarrow \\tr{H} = \\sum_{i=1}^p \\frac{d_i^2}{d_i^2 + \\lambda} &lt; \\min\\{p,n\\}\\]\n\nKNN \\(\\textrm{df} = n/K\\) (each point is it’s own nearest neighbor, it gets weight \\(1/K\\))"
  },
  {
    "objectID": "schedule/slides/model-selection.html#finding-risk-estimators",
    "href": "schedule/slides/model-selection.html#finding-risk-estimators",
    "title": "UBC Stat550",
    "section": "Finding risk estimators",
    "text": "Finding risk estimators\nThis isn’t the way everyone introduces/conceptualizes prediction risk.\nFor me, thinking of \\(\\hat{R}_n\\) as overly optimistic and correcting for that optimism is conceptually appealing\nAn alternative approach is to discuss information criteria.\nIn this case one forms a (pseudo)-metric on probability measures."
  },
  {
    "objectID": "schedule/slides/model-selection.html#kullbackleibler",
    "href": "schedule/slides/model-selection.html#kullbackleibler",
    "title": "UBC Stat550",
    "section": "Kullback–Leibler",
    "text": "Kullback–Leibler\nSuppose we have data \\(Y\\) that comes from the probability density function \\(f\\).\nWhat happens if we use the probability density function \\(g\\) instead?\n\nExample\n\nSuppose \\(Y \\sim N(\\mu,\\sigma^2) =: f\\). We want to predict a new \\(Y_*\\), but we model it as \\(Y_* \\sim N(\\mu_*,\\sigma^2) =: g\\)\n\n\nHow far away are we? We can either compare \\(\\mu\\) to \\(\\mu_*\\) or \\(Y\\) to \\(Y^*\\).\nOr, we can compute how far \\(f\\) is from \\(g\\).\nWe need a notion of distance."
  },
  {
    "objectID": "schedule/slides/model-selection.html#kullbackleibler-1",
    "href": "schedule/slides/model-selection.html#kullbackleibler-1",
    "title": "UBC Stat550",
    "section": "Kullback–Leibler",
    "text": "Kullback–Leibler\nKullback–Leibler divergence (or discrepancy)\n\\[\\begin{aligned}\nKL(f\\;\\Vert\\; g) & = \\int \\log\\left( \\frac{f(y)}{g(y)} \\right)f(y) dy \\\\\n& \\propto\n-\\int \\log (g(y)) f(y) dy \\qquad \\textrm{(ignore term without $g$)}\\\\\n& =\n-\\mathbb{E}_f [\\log (g(Y))] \\end{aligned}\\]\n\nMeasures the loss incurred by (incorrectly) using \\(g\\) instead of \\(f\\).\nKL is not symmetric: \\(KL(f\\;\\Vert\\; g) \\neq KL(g\\;\\Vert\\; f)\\), so it’s not a distance, but it is non-negative and satisfies the triangle inequality.\nUsually, \\(f,\\ g\\) will depend on some parameters, call them \\(\\theta\\)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#kl-example",
    "href": "schedule/slides/model-selection.html#kl-example",
    "title": "UBC Stat550",
    "section": "KL example",
    "text": "KL example\n\nIn regression, we can specify \\(f = N(X^{\\top} \\beta_*, \\sigma^2)\\)\nfor a fixed (true) \\(\\beta_*\\),\nlet \\(g_\\theta = N(X^{\\top}\\beta,\\sigma^2)\\) over all \\(\\theta = (\\beta,\\sigma^2) \\in \\mathbb{R}^p\\times\\mathbb{R}^+\\)\n\\(KL(f,g_\\theta) = -\\mathbb{E}_f [\\log (g_\\theta(Y))]\\), we want to minimize this over \\(\\theta\\).\nBut \\(f\\) is unknown, so we minimize \\(-\\log (g_\\theta(Y))\\) instead.\nThis is the maximum likelihood value \\[\\hat{\\theta}_{ML} = \\argmax_\\theta g_\\theta(Y)\\]\nWe don’t actually need to assume things about a true model nor have it be nested in the alternative models to make this work."
  },
  {
    "objectID": "schedule/slides/model-selection.html#operationalizing",
    "href": "schedule/slides/model-selection.html#operationalizing",
    "title": "UBC Stat550",
    "section": "Operationalizing",
    "text": "Operationalizing\n\nNow, to get an operational characterization of the KL divergence at the ML solution \\[-\\mathbb{E}_f [\\log (g_{\\hat\\theta_{ML}}(Y))]\\] we need an approximation (don’t know \\(f\\), still).\n\n\nResult\n\nIf you maximize the likelihood for a finite dimensional parameter vector \\(\\theta\\) of length \\(p\\), then as \\(n\\rightarrow \\infty\\), \\[-\\mathbb{E}_f [\\log (g_{\\hat\\theta_{ML}}(Y))] = -\\log (g_{\\hat\\theta_{ML}}(Y)) + p.\\]\n\n\n\nThis is AIC (originally “an information criterion”, now “Akaike’s information criterion”)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#aic-warnings",
    "href": "schedule/slides/model-selection.html#aic-warnings",
    "title": "UBC Stat550",
    "section": "AIC warnings",
    "text": "AIC warnings\n\nChoose the model with smallest AIC\nOften multiplied by 2 “for historical reasons”.\nSometimes by \\(-2\\) “to be extra annoying”.\nYour estimator for \\(\\theta\\) needs to be the MLE. (or the asymptotics may be wrong)\n\\(p\\) includes all estimated parameters."
  },
  {
    "objectID": "schedule/slides/model-selection.html#back-to-the-ols-example",
    "href": "schedule/slides/model-selection.html#back-to-the-ols-example",
    "title": "UBC Stat550",
    "section": "Back to the OLS example",
    "text": "Back to the OLS example\nSuppose \\(Y\\) comes from the standard normal linear regression model with known variance \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\n-\\log(g_{\\hat{\\theta}}) &\\propto \\log(\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^\\top \\hat{\\beta}_{MLE})^2\\\\ \\Rightarrow AIC &= 2\\frac{n}{2\\sigma^2}\\hat{R}_n + 2p = \\hat{R}_n + \\frac{2\\sigma^2}{n} p.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/model-selection.html#back-to-the-ols-example-1",
    "href": "schedule/slides/model-selection.html#back-to-the-ols-example-1",
    "title": "UBC Stat550",
    "section": "Back to the OLS example",
    "text": "Back to the OLS example\nSuppose \\(Y\\) comes from the standard normal linear regression model with unknown variance \\(\\sigma^2\\).\nNote that \\(\\hat{\\sigma}_{MLE}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i-x_i^\\top\\hat{\\beta}_{MLE})^2\\).\n\\[\n\\begin{aligned}\n-\\log(g_{\\hat{\\theta}}) &\\propto \\frac{n}{2}\\log(\\hat{\\sigma}^2) + \\frac{1}{2\\hat{\\sigma^2}}\\sum_{i=1}^n (y_i - x_i^\\top \\hat{\\beta}_{MLE})^2\\\\ \\Rightarrow AIC &\\propto 2 n\\log(\\hat{\\sigma}^2)/2 + 2(p+1) \\propto \\log(\\hat{R}_n) + \\frac{2(p+1)}{n}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/model-selection.html#mallows-cp",
    "href": "schedule/slides/model-selection.html#mallows-cp",
    "title": "UBC Stat550",
    "section": "Mallow’s Cp",
    "text": "Mallow’s Cp\n\nDefined for linear regression.\nNo likelihood assumptions.\nVariance is known\n\n\\[C_p = \\hat{R}_n + 2\\sigma^2 \\frac{\\textrm{df}}{n} = AIC\\]"
  },
  {
    "objectID": "schedule/slides/model-selection.html#bayes-factor",
    "href": "schedule/slides/model-selection.html#bayes-factor",
    "title": "UBC Stat550",
    "section": "Bayes factor",
    "text": "Bayes factor\nFor Bayesian Analysis, we want the posterior. Suppose we have two models A and B.\n\\[\n\\begin{aligned}\nP(B\\given \\mathcal{D}) &= \\frac{P(\\mathcal{D}\\given B)P(B)}{P(\\mathcal{D})}\n\\propto P(\\mathcal{D}\\given B)P(B)\\\\\nP(A\\given \\mathcal{D}) &= \\frac{P(\\mathcal{D}\\given A)P(A)}{P(\\mathcal{D})}\n\\propto P(\\mathcal{D}\\given A)P(A)\n\\end{aligned}\n\\] We assume that \\(P(A) = P(B)\\). Then to compare, \\[\n\\frac{P(B\\given \\mathcal{D})}{P(A\\given \\mathcal{D})} = \\frac{P(\\mathcal{D}\\given B)}  {P(\\mathcal{D}\\given A)}.\n\\]\n\nCalled the Bayes Factor.\nThis is the ratio of marginal likelihoods under the different models."
  },
  {
    "objectID": "schedule/slides/model-selection.html#bayes-factor-1",
    "href": "schedule/slides/model-selection.html#bayes-factor-1",
    "title": "UBC Stat550",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\nNot easy to calculate generally. ()\nUse the Laplace approximation, some simplifications, assumptions:\n\n\\[\\log P(\\mathcal{D}\\given B) = \\log P(\\mathcal{D} \\given \\hat{\\theta},\\ B) -\\frac{p\\log(n)}{2} + O(1)\n\\]\n\nMultiply through by \\(-2\\): \\[\nBIC = -\\log (g_\\theta(Y)) + p\\log(n) = \\log(\\hat{R}_n) + \\frac{p\\log(n)}{n}\n\\]\nAlso called Schwarz IC. Compare to AIC (variance unknown case)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#sure",
    "href": "schedule/slides/model-selection.html#sure",
    "title": "UBC Stat550",
    "section": "SURE",
    "text": "SURE\n\\[\\hat{R}_{gic} := \\hat{R}_n(\\hat{f}) + \\textrm{pen}.\\]\nIf \\(\\Expect{\\textrm{pen}} = \\frac{2}{n}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}\\), we have an unbiased estimator of \\(\\overline{R}_n(\\hat{f})\\).\n\nResult (Stein’s Lemma)\n\nSuppose \\(Y_i\\sim N(\\mu_i,\\sigma^2)\\) and suppose \\(f\\) is weakly differentiable. Then\n\n\n\\[\\frac{1}{\\sigma^2} \\sum_{i=1}^n\\Cov{Y_i}{\\hat{f}_i(Y)} = \\Expect{\\sum_{i=1}^n \\frac{\\partial f_i}{\\partial y_i} \\hat{f}(Y)}.\\]\n\nNote: Here I’m writing \\(\\hat{f}\\) as a function of \\(Y\\) rather than \\(x\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#sure-1",
    "href": "schedule/slides/model-selection.html#sure-1",
    "title": "UBC Stat550",
    "section": "SURE",
    "text": "SURE\n\nThis gives “Stein’s Unbiased Risk Estimator”\n\n\\[SURE = \\hat{R}_n(\\hat{f}) + 2\\sigma^2 \\sum_{i=1}^n \\frac{\\partial f_i}{\\partial y_i} \\hat{f}(Y) - n\\sigma^2.\\]\n\nIf \\(f(Y) = HY\\) is linear, we’re back to AIC (variance known case)\nIf \\(\\sigma^2\\) is unknown, may not be unbiased anymore. May not care."
  },
  {
    "objectID": "schedule/slides/model-selection.html#intuition-for-cv",
    "href": "schedule/slides/model-selection.html#intuition-for-cv",
    "title": "UBC Stat550",
    "section": "Intuition for CV",
    "text": "Intuition for CV\nOne reason that \\(\\widehat{R}_n(\\widehat{f})\\) is bad is that we are using the same data to pick \\(\\widehat{f}\\) AND to estimate \\(R_n\\).\n“Validation set” fixes this, but holds out a particular, fixed block of data we pretend mimics the “test data”\nWhat if we set aside one observation, say the first one \\((y_1, x_1)\\).\nWe estimate \\(\\widehat{f}^{(1)}\\) without using the first observation.\nThen we test our prediction:\n\\[\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.\\]\n(why the notation \\(\\widetilde{R}_1\\)? Because we’re estimating the risk with 1 observation. )"
  },
  {
    "objectID": "schedule/slides/model-selection.html#keep-going",
    "href": "schedule/slides/model-selection.html#keep-going",
    "title": "UBC Stat550",
    "section": "Keep going",
    "text": "Keep going\nBut that was only one data point \\((y_1, x_1)\\). Why stop there?\nDo the same with \\((y_2, x_2)\\)! Get an estimate \\(\\widehat{f}^{(2)}\\) without using it, then\n\\[\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.\\]\nWe can keep doing this until we try it for every data point.\nAnd then average them! (Averages are good)\n\\[\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n\n(y_i - \\widehat{f}^{(i)}(x_i))^2\\]\nThis is leave-one-out cross validation"
  },
  {
    "objectID": "schedule/slides/model-selection.html#problems-with-loo-cv",
    "href": "schedule/slides/model-selection.html#problems-with-loo-cv",
    "title": "UBC Stat550",
    "section": "Problems with LOO-CV",
    "text": "Problems with LOO-CV\n🤮 Each held out set is small \\((n=1)\\). Therefore, the variance of the Squared Error of each prediction is high.\n🤮 The training sets overlap. This is bad.\n\nUsually, averaging reduces variance: \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.\\)\nBut only if the variables are independent. If not, then \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.\\)\nSince the training sets overlap a lot, that covariance can be pretty big.\n\n🤮 We have to estimate this model \\(n\\) times.\n🎉 Bias is low because we used almost all the data to fit the model: \\(E[\\mbox{LOO-CV}] = R_{n-1}\\)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#k-fold-cv",
    "href": "schedule/slides/model-selection.html#k-fold-cv",
    "title": "UBC Stat550",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nTo alleviate some of these problems, people usually use \\(K\\)-fold cross validation.\nThe idea of \\(K\\)-fold is\n\nDivide the data into \\(K\\) groups.\nLeave a group out and estimate with the rest.\nTest on the held-out group. Calculate an average risk over these \\(\\sim n/K\\) data.\nRepeat for all \\(K\\) groups.\nAverage the average risks.\n\n\n\n🎉 Less overlap, smaller covariance.\n🎉 Larger hold-out sets, smaller variance.\n🎉 Less computations (only need to estimate \\(K\\) times)\n🤮 LOO-CV is (nearly) unbiased for \\(R_n\\)\n🤮 K-fold CV is unbiased for \\(R_{n(1-1/K)}\\)\nThe risk depends on how much data you use to estimate the model. \\(R_n\\) depends on \\(n\\)."
  },
  {
    "objectID": "schedule/slides/model-selection.html#comparison",
    "href": "schedule/slides/model-selection.html#comparison",
    "title": "UBC Stat550",
    "section": "Comparison",
    "text": "Comparison\n\nLOO-CV and AIC are asymptotically equivalent \\(p&lt;n\\), (Stone 1977)\nProperties of AIC/BIC in high dimensions are not well understood.\nIn low dimensions, AIC is minimax optimal for the prediction risk (Yang and Barron 1998)\nCV is consistent for the prediction risk (Dudoit and Laan 2005)\nBoth tend to over-select predictors (unproven, except empirically)\nBIC asymptotically selects the correct linear model in low dimensions (Shao 1997) and in high dimensions (Wang, Li, and Leng 2009)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#comparison-1",
    "href": "schedule/slides/model-selection.html#comparison-1",
    "title": "UBC Stat550",
    "section": "Comparison",
    "text": "Comparison\n\nIn linear regression, it is impossible for a model selection criterion to be minimax optimal and select the correct model asymptotically (Yang 2005)\nIn high dimensions, if the variance is unknown, the “known” variance form of AIC/BIC is disastrous.\nConclusion: your choice of risk estimator impacts results. Thus,\n\nIf you want to select models, you might pick BIC\nIf you want good predictions, you might use CV\nIt’s possible LASSO + CV(1se) picks models better than LASSO + CV(min)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#some-other-lessons",
    "href": "schedule/slides/model-selection.html#some-other-lessons",
    "title": "UBC Stat550",
    "section": "Some other lessons",
    "text": "Some other lessons\n\nThe unknown variance form of AIC fails in high dimensions because you can drive RSS to zero.\nYou need to use a high-dimensional variance estimator instead (Homrighausen and McDonald 2018)\nLASSO + CV “works” in high dimensions (not LOO, but no one uses it anyway)\nUnder very strong conditions it selects the right model at the right rate.\nUnder weaker conditions, it achieves (nearly) minimax optimal prediction risk. (Homrighausen and McDonald 2013, 2014, 2017)"
  },
  {
    "objectID": "schedule/slides/model-selection.html#what-if-we-dont-want-to-choose",
    "href": "schedule/slides/model-selection.html#what-if-we-dont-want-to-choose",
    "title": "UBC Stat550",
    "section": "What if we don’t want to choose?",
    "text": "What if we don’t want to choose?\n\nChoose a risk estimator \\(\\hat{R}\\)\nCalculate weights \\(p_i = \\exp\\left\\{-\\hat{R}(\\textrm{Model}_i)\\right\\}\\)\nCreate final estimator \\(\\hat{f} = \\sum_{\\textrm{models}} \\frac{p_i}{\\sum p_i} \\hat{f}_i\\).\n\n\nIf \\(\\hat{R}\\) is BIC, this is (poor-man’s) Bayesian Model Averaging."
  },
  {
    "objectID": "schedule/slides/model-selection.html#bayesian-model-averaging",
    "href": "schedule/slides/model-selection.html#bayesian-model-averaging",
    "title": "UBC Stat550",
    "section": "Bayesian Model Averaging",
    "text": "Bayesian Model Averaging\n\nReal BMA integrates over the models: \\[P(f \\given \\mathcal{D}) = \\int P(f \\given M_i, \\mathcal{D}) P(M_i \\given \\mathcal{D}) dM\\]\nAveraging + Sparsity is pretty hard.\nInteresting open problem: how can we combine LASSO models over the path?\nIssue with MA: \\(e^{-BIC}\\) can be tiny for all but a few models. You’re not averaging anymore."
  },
  {
    "objectID": "schedule/slides/model-selection.html#references",
    "href": "schedule/slides/model-selection.html#references",
    "title": "UBC Stat550",
    "section": "References",
    "text": "References\n\n\n\nUBC Stat 550 - 2024\n\n\n\n\nDudoit, Sandrine, and Mark J. van der Laan. 2005. “Asymptotics of Cross-Validation Risk Estimation in Estimator Selection and Performance Assessment.” Statistical Methodology, 131–54.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Verlag.\n\n\nHomrighausen, Darren, and Daniel J. McDonald. 2013. “The Lasso, Persistence, and Cross-Validation.” In Proceedings of the \\(30^{th}\\) International Conference on Machine Learning (ICML), edited by Sanjoy Dasgupta and David McAllester, 28:1031–39. PMLR.\n\n\n———. 2014. “Leave-One-Out Cross-Validation Is Risk Consistent for Lasso.” Machine Learning 97 (1-2): 65–78.\n\n\n———. 2017. “Risk Consistency of Cross-Validation for Lasso-Type Procedures.” Statistica Sinica 27 (3): 1017–36.\n\n\n———. 2018. “A Study on Tuning Parameter Selection for the High-Dimensional Lasso.” Journal of Statistical Computation and Simulation 88: 2865–92.\n\n\nShao, Jun. 1997. “An Asymptotic Theory for Linear Model Selection.” Statistica Sinica 7 (2): 221–42.\n\n\nStone, M. 1977. “Asymptotics for and Against Cross-Validation.” Biometrika 64 (1): 29–35.\n\n\nWang, Hansheng, Bo Li, and Chenlei Leng. 2009. “Shrinkage Tuning Parameter Selection with a Diverging Number of Parameters.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71 (3): 671–83.\n\n\nYang, Yuhong. 2005. “Can the Strengths of AIC and BIC Be Shared? A Conflict Between Model Indentification and Regression Estimation.” Biometrika 92 (4): 937–50.\n\n\nYang, Yuhong, and Andrew Barron. 1998. “An Asymptotic Property of Model Selection Criteria.” IEEE Transactions on Information Theory 44: 95–116."
  },
  {
    "objectID": "schedule/slides/pca-intro.html#section",
    "href": "schedule/slides/pca-intro.html#section",
    "title": "UBC Stat550",
    "section": "Principal components analysis",
    "text": "Principal components analysis\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#representation-learning",
    "href": "schedule/slides/pca-intro.html#representation-learning",
    "title": "UBC Stat550",
    "section": "Representation learning",
    "text": "Representation learning\nRepresentation learning is the idea that performance of ML methods is highly dependent on the choice of representation\nFor this reason, much of ML is geared towards transforming the data into the relevant features and then using these as inputs\nThis idea is as old as statistics itself, really,\nHowever, the idea is constantly revisited in a variety of fields and contexts\nCommonly, these learned representations capture low-level information like overall shapes\nIt is possible to quantify this intuition for PCA at least\n\n\nGoal\n\nTransform \\(\\mathbf{X}\\in \\R^{n\\times p}\\) into \\(\\mathbf{Z} \\in \\R^{n \\times ?}\\)\n\n\n?-dimension can be bigger (feature creation) or smaller (dimension reduction) than \\(p\\)"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#pca",
    "href": "schedule/slides/pca-intro.html#pca",
    "title": "UBC Stat550",
    "section": "PCA",
    "text": "PCA\nPrincipal components analysis (PCA) is a dimension reduction technique\nIt solves various equivalent optimization problems\n(Maximize variance, minimize \\(\\ell_2\\) distortions, find closest subspace of a given rank, \\(\\ldots\\))\nAt its core, we are finding linear combinations of the original (centered) data \\[z_{ij} = \\alpha_j^{\\top} x_i\\]"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#lower-dimensional-embeddings",
    "href": "schedule/slides/pca-intro.html#lower-dimensional-embeddings",
    "title": "UBC Stat550",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nSuppose we have predictors \\(\\x_1\\) and \\(\\x_2\\) (columns / features / measurements)\n\nWe more faithfully preserve the structure of this data by keeping \\(\\x_1\\) and setting \\(\\x_2\\) to zero than the opposite"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#lower-dimensional-embeddings-1",
    "href": "schedule/slides/pca-intro.html#lower-dimensional-embeddings-1",
    "title": "UBC Stat550",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nAn important feature of the previous example is that \\(\\x_1\\) and \\(\\x_2\\) aren’t correlated\nWhat if they are?\n\nWe lose a lot of structure by setting either \\(\\x_1\\) or \\(\\x_2\\) to zero"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#lower-dimensional-embeddings-2",
    "href": "schedule/slides/pca-intro.html#lower-dimensional-embeddings-2",
    "title": "UBC Stat550",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nThe only difference is the first is a rotation of the second"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#pca-1",
    "href": "schedule/slides/pca-intro.html#pca-1",
    "title": "UBC Stat550",
    "section": "PCA",
    "text": "PCA\nIf we knew how to rotate our data, then we could more easily retain the structure.\nPCA gives us exactly this rotation\n\nCenter (+scale?) the data matrix \\(\\X\\)\nCompute the SVD of \\(\\X = \\U\\D \\V^\\top\\) (always exists)\nReturn \\(\\U_M\\D_M\\), where \\(\\D_M\\) is the largest \\(M\\) singular values of \\(\\X\\)"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#pca-2",
    "href": "schedule/slides/pca-intro.html#pca-2",
    "title": "UBC Stat550",
    "section": "PCA",
    "text": "PCA"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#pca-on-some-pop-music-data",
    "href": "schedule/slides/pca-intro.html#pca-on-some-pop-music-data",
    "title": "UBC Stat550",
    "section": "PCA on some pop music data",
    "text": "PCA on some pop music data\n\n\n# A tibble: 1,269 × 15\n   artist      danceability energy   key loudness  mode speechiness acousticness\n   &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Taylor Swi…        0.781  0.357     0   -16.4      1      0.912       0.717  \n 2 Taylor Swi…        0.627  0.266     9   -15.4      1      0.929       0.796  \n 3 Taylor Swi…        0.516  0.917    11    -3.19     0      0.0827      0.0139 \n 4 Taylor Swi…        0.629  0.757     1    -8.37     0      0.0512      0.00384\n 5 Taylor Swi…        0.686  0.705     9   -10.8      1      0.249       0.832  \n 6 Taylor Swi…        0.522  0.691     2    -4.82     1      0.0307      0.00609\n 7 Taylor Swi…        0.31   0.374     6    -8.46     1      0.0275      0.761  \n 8 Taylor Swi…        0.705  0.621     2    -8.09     1      0.0334      0.101  \n 9 Taylor Swi…        0.553  0.604     1    -5.30     0      0.0258      0.202  \n10 Taylor Swi…        0.419  0.908     9    -5.16     1      0.0651      0.00048\n# ℹ 1,259 more rows\n# ℹ 7 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, time_signature &lt;int&gt;, duration_ms &lt;int&gt;, explicit &lt;lgl&gt;"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#pca-on-some-pop-music-data-1",
    "href": "schedule/slides/pca-intro.html#pca-on-some-pop-music-data-1",
    "title": "UBC Stat550",
    "section": "PCA on some pop music data",
    "text": "PCA on some pop music data\n\n15 dimensions to 2\ncoloured by artist"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#plotting-the-weights-alpha_j-j12",
    "href": "schedule/slides/pca-intro.html#plotting-the-weights-alpha_j-j12",
    "title": "UBC Stat550",
    "section": "Plotting the weights, \\(\\alpha_j,\\ j=1,2\\)",
    "text": "Plotting the weights, \\(\\alpha_j,\\ j=1,2\\)"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#matrix-decompositions",
    "href": "schedule/slides/pca-intro.html#matrix-decompositions",
    "title": "UBC Stat550",
    "section": "Matrix decompositions",
    "text": "Matrix decompositions\nAt its core, we are finding linear combinations of the original (centered) data \\[z_{ij} = \\alpha_j^{\\top} x_i\\]\nThis is expressed via the SVD: \\(\\X  = \\U\\D\\V^{\\top}\\).\n\n\n\n\n\n\nImportant\n\n\nWe assume throughout that we have centered the data\n\n\n\nThen our new features are\n\\[\\mathbf{Z} = \\X \\V = \\U\\D\\]"
  },
  {
    "objectID": "schedule/slides/pca-intro.html#short-svd-aside",
    "href": "schedule/slides/pca-intro.html#short-svd-aside",
    "title": "UBC Stat550",
    "section": "Short SVD aside",
    "text": "Short SVD aside\n\nAny \\(n\\times p\\) matrix can be decomposed into \\(\\mathbf{UDV}^\\top\\).\nThis is a computational procedure, like inverting a matrix, svd()\nThese have properties:\n\n\n\\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_n\\)\n\\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_p\\)\n\\(\\mathbf{D}\\) is diagonal (0 off the diagonal)\n\nMany methods for dimension reduction use the SVD of some matrix."
  },
  {
    "objectID": "schedule/slides/pca-intro.html#why",
    "href": "schedule/slides/pca-intro.html#why",
    "title": "UBC Stat550",
    "section": "Why?",
    "text": "Why?\n\nGiven \\(\\X\\), find a projection \\(\\mathbf{P}\\) onto \\(\\R^M\\) with \\(M \\leq p\\) that minimizes the reconstruction error \\[\n\\begin{aligned}\n\\min_{\\mathbf{P}} &\\,\\, \\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P} \\rVert^2_F \\,\\,\\, \\textrm{(sum all the elements)}\\\\\n\\textrm{subject to} &\\,\\, \\textrm{rank}(\\mathbf{P}) = M,\\, \\mathbf{P} = \\mathbf{P}^T,\\, \\mathbf{P} = \\mathbf{P}^2\n\\end{aligned}\n\\] The conditions ensure that \\(\\mathbf{P}\\) is a projection matrix onto \\(M\\) dimensions.\nMaximize the variance explained by an orthogonal transformation \\(\\mathbf{A} \\in \\R^{p\\times M}\\) \\[\n\\begin{aligned}\n\\max_{\\mathbf{A}} &\\,\\, \\textrm{trace}\\left(\\frac{1}{n}\\mathbf{A}^\\top \\X^\\top \\X \\mathbf{A}\\right)\\\\\n\\textrm{subject to} &\\,\\, \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}_M\n\\end{aligned}\n\\]\n\n\nIn case one, the minimizer is \\(\\mathbf{P} = \\mathbf{V}_M\\mathbf{V}_M^\\top\\)\nIn case two, the maximizer is \\(\\mathbf{A} = \\mathbf{V}_M\\)."
  },
  {
    "objectID": "schedule/slides/pca-intro.html#code-output-to-look-at",
    "href": "schedule/slides/pca-intro.html#code-output-to-look-at",
    "title": "UBC Stat550",
    "section": "Code output to look at",
    "text": "Code output to look at\n\n\n\nUBC Stat 550 - 2024"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#section",
    "href": "schedule/slides/regularization-lm.html#section",
    "title": "UBC Stat550",
    "section": "Linear models, selection, regularization, and inference",
    "text": "Linear models, selection, regularization, and inference\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#recap",
    "href": "schedule/slides/regularization-lm.html#recap",
    "title": "UBC Stat550",
    "section": "Recap",
    "text": "Recap\nModel Selection means select a family of distributions for your data.\nIdeally, we’d do this by comparing the \\(R_n\\) for one family with that for another.\nWe’d use whichever has smaller \\(R_n\\).\nBut \\(R_n\\) depends on the truth, so we estimate it with \\(\\widehat{R}\\).\nThen we use whichever has smaller \\(\\widehat{R}\\)."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#example",
    "href": "schedule/slides/regularization-lm.html#example",
    "title": "UBC Stat550",
    "section": "Example",
    "text": "Example\nThe truth:\n\ndat &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100),\n  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)\n)\n\nModel 1: \\(y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\), \\(\\quad\\epsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\)\nModel 2: y ~ x1 + x2 + x1*x2 (what’s the math version?)\nModel 3: y ~ x2 + sin(x1 * x2)"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#fit-each-model-and-estimate-r_n",
    "href": "schedule/slides/regularization-lm.html#fit-each-model-and-estimate-r_n",
    "title": "UBC Stat550",
    "section": "Fit each model and estimate \\(R_n\\)",
    "text": "Fit each model and estimate \\(R_n\\)\n\nlist(\"y ~ x1 + x2\", \"y ~ x1 * x2\", \"y ~ x2 + sin(x1*x2)\") |&gt;\n  map(~ {\n    fits &lt;- lm(as.formula(.x), data = dat)\n    tibble(\n      R2 = summary(fits)$r.sq,\n      training_error = mean(residuals(fits)^2),\n      loocv = mean( (residuals(fits) / (1 - hatvalues(fits)))^2 ),\n      AIC = AIC(fits),\n      BIC = BIC(fits)\n    )\n  }) |&gt; list_rbind()\n\n# A tibble: 3 × 5\n     R2 training_error loocv   AIC   BIC\n  &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.589           21.3  22.9  598.  608.\n2 0.595           21.0  23.4  598.  611.\n3 0.586           21.4  23.0  598.  609."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#model-selection-vs.-variable-selection",
    "href": "schedule/slides/regularization-lm.html#model-selection-vs.-variable-selection",
    "title": "UBC Stat550",
    "section": "Model Selection vs. Variable Selection",
    "text": "Model Selection vs. Variable Selection\nModel selection is very comprehensive\nYou choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.\nVariable selection is a subset of this. It means\n\nchoosing which predictors to include in a predictive model\n\nEliminating a predictor, means removing it from the model.\nSome procedures automatically search predictors, and eliminate some.\nWe call this variable selection. But the procedure is implicitly selecting a model as well.\nMaking this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#selecting-variables-predictors-with-linear-methods",
    "href": "schedule/slides/regularization-lm.html#selecting-variables-predictors-with-linear-methods",
    "title": "UBC Stat550",
    "section": "Selecting variables / predictors with linear methods",
    "text": "Selecting variables / predictors with linear methods\nSuppose we have a pile of predictors.\nWe estimate models with different subsets of predictors and use CV / Cp / AIC / BIC to decide which is preferred.\nSometimes you might have a few plausible subsets. Easy enough to choose with our criterion.\nSometimes you might just have a bunch of predictors, then what do you do?"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#best-subsets",
    "href": "schedule/slides/regularization-lm.html#best-subsets",
    "title": "UBC Stat550",
    "section": "Best subsets",
    "text": "Best subsets\nIf we imagine that only a few predictors are relevant, we could solve\n\\[\\min_{\\beta\\in\\R^p} \\frac{1}{2n}\\norm{Y-\\X\\beta}_2^2 + \\lambda\\norm{\\beta}_0\\]\nThe \\(\\ell_0\\)-norm counts the number of non-zero coefficients.\nThis may or may not be a good thing to do.\nIt is computationally infeasible if \\(p\\) is more than about 20.\nTechnically NP-hard (you must find the error of each of the \\(2^p\\) models)\nThough see (Bertsimas, King, and Mazumder 2016) for a method of solving reasonably large cases via mixed integer programming."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#greedy-methods",
    "href": "schedule/slides/regularization-lm.html#greedy-methods",
    "title": "UBC Stat550",
    "section": "Greedy methods",
    "text": "Greedy methods\nBecause this is an NP-hard problem, we fall back on greedy algorithms.\nAll are implemented by the regsubsets function in the leaps package.\n\nAll subsets\n\nestimate model based on every possible subset of size \\(|\\mathcal{S}| \\leq \\min\\{n, p\\}\\), use one with lowest risk estimate\n\nForward selection\n\nstart with \\(\\mathcal{S}=\\varnothing\\), add predictors greedily\n\nBackward selection\n\nstart with \\(\\mathcal{S}=\\{1,\\ldots,p\\}\\), remove greedily\n\nHybrid\n\ncombine forward and backward smartly"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#section-1",
    "href": "schedule/slides/regularization-lm.html#section-1",
    "title": "UBC Stat550",
    "section": "",
    "text": "Note\n\n\nWithin each procedure, we’re comparing nested models."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#costs-and-benefits",
    "href": "schedule/slides/regularization-lm.html#costs-and-benefits",
    "title": "UBC Stat550",
    "section": "Costs and benefits",
    "text": "Costs and benefits\n\nAll subsets\n\n👍 estimates each subset\n💣 takes \\(2^p\\) model fits when \\(p&lt;n\\). If \\(p=50\\), this is about \\(10^{15}\\) models.\n\nForward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n\nBackward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n💣 doesn’t work if \\(p&gt;n\\)\n\nHybrid\n\n👍 visits more models than forward/backward\n💣 slower"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#synthetic-example",
    "href": "schedule/slides/regularization-lm.html#synthetic-example",
    "title": "UBC Stat550",
    "section": "Synthetic example",
    "text": "Synthetic example\n\nset.seed(2024 - 550)\nn &lt;- 550\ndf &lt;- tibble( \n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  x5 = x1 + rnorm(n, sd = .1), # correlated with x1\n  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)\n  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only\n)\n\n\\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) are the true predictors\nBut the rest are correlated with them"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#full-model",
    "href": "schedule/slides/regularization-lm.html#full-model",
    "title": "UBC Stat550",
    "section": "Full model",
    "text": "Full model\n\nfull &lt;- lm(y ~ ., data = df)\nsummary(full)\n\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.120 -1.386 -0.060  1.417  6.536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.17176    0.21823  -0.787  0.43158   \nx1           4.94560    1.62872   3.036  0.00251 **\nx2           1.88209    1.34057   1.404  0.16091   \nx3           0.10755    0.90835   0.118  0.90579   \nx4          -1.51043    0.97746  -1.545  0.12287   \nx5          -1.79872    0.94961  -1.894  0.05874 . \nx6          -0.08277    0.92535  -0.089  0.92876   \nx7          -0.05477    0.90159  -0.061  0.95159   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.176 on 542 degrees of freedom\nMultiple R-squared:  0.6538,    Adjusted R-squared:  0.6494 \nF-statistic: 146.2 on 7 and 542 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#true-model",
    "href": "schedule/slides/regularization-lm.html#true-model",
    "title": "UBC Stat550",
    "section": "True model",
    "text": "True model\n\ntruth &lt;- lm(y ~ x1 + x2, data = df)\nsummary(truth)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0630 -1.4199 -0.0654  1.3871  6.7382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.12389    0.20060  -0.618    0.537    \nx1           2.99853    0.09434  31.783  &lt; 2e-16 ***\nx2           0.44614    0.09257   4.820 1.87e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.179 on 547 degrees of freedom\nMultiple R-squared:  0.6498,    Adjusted R-squared:  0.6485 \nF-statistic: 507.5 on 2 and 547 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#all-subsets",
    "href": "schedule/slides/regularization-lm.html#all-subsets",
    "title": "UBC Stat550",
    "section": "All subsets",
    "text": "All subsets\n\nlibrary(leaps)\ntrythemall &lt;- regsubsets(y ~ ., data = df)\nsummary(trythemall)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df)\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \" \"\n3  ( 1 ) \"*\" \"*\" \" \" \" \" \"*\" \" \" \" \"\n4  ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \" \"\n5  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \"\n6  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#bic-and-cp",
    "href": "schedule/slides/regularization-lm.html#bic-and-cp",
    "title": "UBC Stat550",
    "section": "BIC and Cp",
    "text": "BIC and Cp"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#theory",
    "href": "schedule/slides/regularization-lm.html#theory",
    "title": "UBC Stat550",
    "section": "Theory",
    "text": "Theory\nThis result is due to Foster and George (1994).\n\nIf the truth is linear.\n\\(\\lambda = C\\sigma^2\\log p.\\)\n\\(\\norm{\\beta_*}_0 = s\\)\n\n\\[\\frac{\\Expect{\\norm{\\X\\beta_*-\\X\\hat\\beta}_2^2}/n}{s\\sigma^2/n} \\leq 4\\log p + 2 + o(1).\\]\n\\[\\inf_{\\hat\\beta}\\sup_{\\X,\\beta_*} \\frac{\\Expect{\\norm{\\X\\beta_*-\\X\\hat\\beta}_2^2}/n}{s\\sigma^2/n} \\geq 2\\log p - o(\\log p).\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#section-2",
    "href": "schedule/slides/regularization-lm.html#section-2",
    "title": "UBC Stat550",
    "section": "",
    "text": "Important\n\n\n\neven if we could compute the subset selection estimator at scale, it’s not clear that we would want to\n(Many people assume that we would.)\ntheory provides an understanding of the performance of various estimators under typically idealized conditions"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#regularization-1",
    "href": "schedule/slides/regularization-lm.html#regularization-1",
    "title": "UBC Stat550",
    "section": "Regularization",
    "text": "Regularization\n\nAnother way to control bias and variance is through regularization or shrinkage.\nRather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\nBut, make your estimates of \\(\\beta\\) “smaller”"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#brief-aside-on-optimization",
    "href": "schedule/slides/regularization-lm.html#brief-aside-on-optimization",
    "title": "UBC Stat550",
    "section": "Brief aside on optimization",
    "text": "Brief aside on optimization\n\nAn optimization problem has 2 components:\n\nThe “Objective function”: e.g. \\(\\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\\).\nThe “constraint”: e.g. “fewer than 5 non-zero entries in \\(\\beta\\)”.\n\nA constrained minimization problem is written\n\n\\[\\min_\\beta f(\\beta)\\;\\; \\mbox{ subject to }\\;\\; C(\\beta)\\]\n\n\\(f(\\beta)\\) is the objective function\n\\(C(\\beta)\\) is the constraint"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression-constrained-version",
    "href": "schedule/slides/regularization-lm.html#ridge-regression-constrained-version",
    "title": "UBC Stat550",
    "section": "Ridge regression (constrained version)",
    "text": "Ridge regression (constrained version)\nOne way to do this for regression is to solve (say): \\[\n\\minimize_\\beta \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\sum_j \\beta^2_j &lt; s\n\\] for some \\(s&gt;0\\).\n\nThis is called “ridge regression”.\nWrite the minimizer as \\(\\hat{\\beta}_s\\).\n\n\nCompare this to ordinary least squares:\n\\[\n\\minimize_\\beta \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\beta \\in \\R^p\n\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#geometry-of-ridge-regression-contours",
    "href": "schedule/slides/regularization-lm.html#geometry-of-ridge-regression-contours",
    "title": "UBC Stat550",
    "section": "Geometry of ridge regression (contours)",
    "text": "Geometry of ridge regression (contours)"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#reminder-of-norms-we-should-remember",
    "href": "schedule/slides/regularization-lm.html#reminder-of-norms-we-should-remember",
    "title": "UBC Stat550",
    "section": "Reminder of norms we should remember",
    "text": "Reminder of norms we should remember\n\n\\(\\ell_q\\)-norm\n\n\\(\\left(\\sum_{j=1}^p |z_j|^q\\right)^{1/q}\\)\n\n\\(\\ell_1\\)-norm (special case)\n\n\\(\\sum_{j=1}^p |z_j|\\)\n\n\\(\\ell_0\\)-norm\n\n\\(\\sum_{j=1}^p I(z_j \\neq 0 ) = \\lvert \\{j : z_j \\neq 0 \\}\\rvert\\)\n\n\\(\\ell_\\infty\\)-norm\n\n\\(\\max_{1\\leq j \\leq p} |z_j|\\)\n\n\n\n\nRecall what a norm is: https://en.wikipedia.org/wiki/Norm_(mathematics)"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression",
    "href": "schedule/slides/regularization-lm.html#ridge-regression",
    "title": "UBC Stat550",
    "section": "Ridge regression",
    "text": "Ridge regression\nAn equivalent way to write\n\\[\\hat\\beta_s = \\argmin_{ \\Vert \\beta \\Vert_2^2 \\leq s} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\\]\nis in the Lagrangian form\n\\[\\hat\\beta_\\lambda = \\argmin_{ \\beta} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\frac{\\lambda}{2} \\Vert \\beta \\Vert_2^2.\\]\nFor every \\(\\lambda\\) there is a unique \\(s\\) (and vice versa) that makes\n\\[\\hat\\beta_s = \\hat\\beta_\\lambda\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression-1",
    "href": "schedule/slides/regularization-lm.html#ridge-regression-1",
    "title": "UBC Stat550",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\(\\hat\\beta_s = \\argmin_{ \\Vert \\beta \\Vert_2^2 \\leq s} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\\)\n\\(\\hat\\beta_\\lambda = \\argmin_{ \\beta} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\frac{\\lambda}{2} \\Vert \\beta \\Vert_2^2.\\)\nObserve:\n\n\\(\\lambda = 0\\) (or \\(s = \\infty\\)) makes \\(\\hat\\beta_\\lambda = \\hat\\beta_{ols}\\)\nAny \\(\\lambda &gt; 0\\) (or \\(s &lt;\\infty\\)) penalizes larger values of \\(\\beta\\), effectively shrinking them.\n\n\\(\\lambda\\) and \\(s\\) are known as tuning parameters"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#example-data",
    "href": "schedule/slides/regularization-lm.html#example-data",
    "title": "UBC Stat550",
    "section": "Example data",
    "text": "Example data\nprostate data from [ESL]\n\n\n# A tibble: 97 × 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;lgl&gt;\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ℹ 87 more rows\n\n\n\nUse lpsa as response."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression-path",
    "href": "schedule/slides/regularization-lm.html#ridge-regression-path",
    "title": "UBC Stat550",
    "section": "Ridge regression path",
    "text": "Ridge regression path\n\nY &lt;- prostate$lpsa\nX &lt;- model.matrix(~ ., data = prostate |&gt; dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge &lt;- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel selection here:\n\nmeans choose some \\(\\lambda\\)\nA value of \\(\\lambda\\) is a vertical line.\nThis graphic is a “path” or “coefficient trace”\nCoefficients for varying \\(\\lambda\\)"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#solving-the-minimization",
    "href": "schedule/slides/regularization-lm.html#solving-the-minimization",
    "title": "UBC Stat550",
    "section": "Solving the minimization",
    "text": "Solving the minimization\n\nOne nice thing about ridge regression is that it has a closed-form solution (like OLS)\n\n\\[\\hat\\beta_\\lambda = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nThis is easy to calculate in R for any \\(\\lambda\\).\nHowever, computations and interpretation are simplified if we examine the Singular Value Decomposition of \\(\\X = \\mathbf{UDV}^\\top\\).\nRecall: any matrix has an SVD.\nHere \\(\\mathbf{D}\\) is diagonal and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}\\)."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#solving-the-minization",
    "href": "schedule/slides/regularization-lm.html#solving-the-minization",
    "title": "UBC Stat550",
    "section": "Solving the minization",
    "text": "Solving the minization\n\\[\\hat\\beta_\\lambda = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nNote that \\(\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top\\).\nThen,\n\n\\[\\hat\\beta_\\lambda = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.\\]\n\nFor computations, now we only need to invert \\(\\mathbf{D}\\)."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#comparing-with-ols",
    "href": "schedule/slides/regularization-lm.html#comparing-with-ols",
    "title": "UBC Stat550",
    "section": "Comparing with OLS",
    "text": "Comparing with OLS\n\n\\(\\mathbf{D}\\) is a diagonal matrix\n\n\\[\\hat\\beta_{ols} = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y\\]\n\\[\\hat\\beta_\\lambda = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.\\]\n\nNotice that \\(\\hat\\beta_{ols}\\) depends on \\(d_j/d_j^2\\) while \\(\\hat\\beta_\\lambda\\) depends on \\(d_j/(d_j^2 + \\lambda)\\).\nRidge regression makes the coefficients smaller relative to OLS.\nBut if \\(\\X\\) has small singular values, ridge regression compensates with \\(\\lambda\\) in the denominator."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression-and-multicollinearity",
    "href": "schedule/slides/regularization-lm.html#ridge-regression-and-multicollinearity",
    "title": "UBC Stat550",
    "section": "Ridge regression and multicollinearity",
    "text": "Ridge regression and multicollinearity\nMulticollinearity: a linear combination of predictor variables is nearly equal to another predictor variable."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#multicollinearity-questions",
    "href": "schedule/slides/regularization-lm.html#multicollinearity-questions",
    "title": "UBC Stat550",
    "section": "Multicollinearity questions",
    "text": "Multicollinearity questions\n\nCan I test cor(x1, x2) == 0 to determine if these are collinear?\nWhat plots or summaries can I look at?\nIf multivariate regression or logistic regression is applied on a data set with many explanatory variables, what in the regression output might indicate potential multicollinearity?\nIs there a test or diagnostic procedure for multicollinearity?\n\n\n\nNo. \nCorrelation matrix of continuous \\(x\\).\nLarge standard errors, estimated coefficients with opposite sign. NA estimates. Removing vars brings down SEs without much change in fit.\nBig VIF summary(lm(xj ~ . - xj - y))$r.sq"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#multicollinearity-thoughts",
    "href": "schedule/slides/regularization-lm.html#multicollinearity-thoughts",
    "title": "UBC Stat550",
    "section": "Multicollinearity thoughts",
    "text": "Multicollinearity thoughts\nSome comments:\n\nA better phrase: \\(\\X\\) is ill-conditioned\nAKA “(numerically) rank-deficient”.\n\\(\\X = \\mathbf{U D V}^\\top\\) ill-conditioned \\(\\Longleftrightarrow\\) some elements of \\(\\mathbf{D} \\approx 0\\)\n\\(\\hat\\beta_{ols}= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y\\). Small entries of \\(\\mathbf{D}\\) \\(\\Longleftrightarrow\\) huge elements of \\(\\mathbf{D}^{-1}\\)\nMeans huge variance: \\(\\Var{\\hat\\beta_{ols}} =  \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top\\)\nIf you’re doing prediction, this is a purely computational concern."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-regression-and-ill-posed-x",
    "href": "schedule/slides/regularization-lm.html#ridge-regression-and-ill-posed-x",
    "title": "UBC Stat550",
    "section": "Ridge regression and ill-posed \\(\\X\\)",
    "text": "Ridge regression and ill-posed \\(\\X\\)\nRidge Regression fixes this problem by preventing the division by a near-zero number\n\nConclusion\n\n\\((\\X^{\\top}\\X)^{-1}\\) can be really unstable, while \\((\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}\\) is not.\n\nAside\n\nEngineering approach to solving linear systems is to always do this with small \\(\\lambda\\). The thinking is about the numerics rather than the statistics.\n\n\nWhich \\(\\lambda\\) to use?\n\nComputational\n\nUse CV and pick the \\(\\lambda\\) that makes this smallest.\n\nIntuition (bias)\n\nAs \\(\\lambda\\rightarrow\\infty\\), bias ⬆\n\nIntuition (variance)\n\nAs \\(\\lambda\\rightarrow\\infty\\), variance ⬇\n\n\nYou should think about why."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#can-we-get-the-best-of-both-worlds",
    "href": "schedule/slides/regularization-lm.html#can-we-get-the-best-of-both-worlds",
    "title": "UBC Stat550",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\nTo recap:\n\nDeciding which predictors to include, adding quadratic terms, or interactions is model selection (more precisely variable selection within a linear model).\nRidge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.\nIf the LM is true,\n\nOLS is unbiased, but Variance depends on \\(\\mathbf{D}^{-2}\\). Can be big.\nRidge is biased (can you find the bias?). But Variance is smaller than OLS.\n\nRidge regression does not perform variable selection.\nBut picking \\(\\lambda=3.7\\) and thereby deciding to predict with \\(\\widehat{\\beta}^R_{3.7}\\) is model selection."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#can-we-get-the-best-of-both-worlds-1",
    "href": "schedule/slides/regularization-lm.html#can-we-get-the-best-of-both-worlds-1",
    "title": "UBC Stat550",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\n\nRidge regression\n\n\\(\\minimize \\frac{1}{2n}\\Vert\\y-\\X\\beta\\Vert_2^2 \\ \\st\\ \\snorm{\\beta}_2^2 \\leq s\\)\n\nBest (in-sample) linear regression model of size \\(s\\)\n\n\\(\\minimize \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2 \\ \\st\\ \\snorm{\\beta}_0 \\leq s\\)\n\n\n\\(||\\beta||_0\\) is the number of nonzero elements in \\(\\beta\\)\nFinding the best in-sample linear model (of size \\(s\\), among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\nRidge regression is convex (easy to solve), but doesn’t do variable selection\nCan we somehow “interpolate” to get both?\nNote: selecting \\(\\lambda\\) is still model selection, but we’ve included all the variables."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-theory",
    "href": "schedule/slides/regularization-lm.html#ridge-theory",
    "title": "UBC Stat550",
    "section": "Ridge theory",
    "text": "Ridge theory\nRecalling that \\(\\beta^\\top_*x\\) is the best linear approximation to \\(f_*(x)\\)\nIf \\(\\norm{x}_\\infty&lt; r\\), (Hsu, Kakade, and Zhang 2014), \\[R(\\hat\\beta_\\lambda) - R(\\beta_*) \\leq \\left(1+ O\\left(\\frac{1+r^2/\\lambda}{n}\\right)\\right)\n\\frac{\\lambda\\norm{\\beta_*}_2^2}{2} + \\frac{\\sigma^2\\tr{\\Sigma}}{2n\\lambda}\\]\nOptimizing over \\(\\lambda\\), and setting \\(B=\\norm{\\beta_*}\\) gives\n\\[R(\\hat\\beta_\\lambda) - R(\\beta_*) \\leq \\sqrt{\\frac{\\sigma^2r^2B^2}{n}\\left(1+O(1/n)\\right)} +\nO\\left(\\frac{r^2B^2}{n}\\right)\\]\n\\[\\inf_{\\hat\\beta}\\sup_{\\beta_*} R(\\hat\\beta) - R(\\beta_*) \\geq C\\sqrt{\\frac{\\sigma^2r^2B^2}{n}}\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ridge-theory-1",
    "href": "schedule/slides/regularization-lm.html#ridge-theory-1",
    "title": "UBC Stat550",
    "section": "Ridge theory",
    "text": "Ridge theory\nWe call this behavior rate minimax: essential meaning, \\[R(\\hat\\beta) - R(\\beta_*) = O\\left(\\inf_{\\hat\\beta}\\sup_{\\beta_*} R(\\hat\\beta) - R(\\beta_*)\\right)\\]\nIn this setting, Ridge regression does as well as we could hope, up to constants."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#bayes-interpretation",
    "href": "schedule/slides/regularization-lm.html#bayes-interpretation",
    "title": "UBC Stat550",
    "section": "Bayes interpretation",
    "text": "Bayes interpretation\nIf\n\n\\(Y=X'\\beta + \\epsilon\\),\n\\(\\epsilon\\sim N(0,\\sigma^2)\\)\n\\(\\beta\\sim N(0,\\tau^2 I_p)\\),\n\nThen, the posterior mean (median, mode) is the ridge estimator with \\(\\lambda=\\sigma^2/\\tau^2\\)."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#geometry",
    "href": "schedule/slides/regularization-lm.html#geometry",
    "title": "UBC Stat550",
    "section": "Geometry",
    "text": "Geometry\n\n\nCode\nlibrary(mvtnorm)\nnormBall &lt;- function(q = 1, len = 1000) {\n  tg &lt;- seq(0, 2 * pi, length = len)\n  out &lt;- data.frame(x = cos(tg)) %&gt;%\n    mutate(b = (1 - abs(x)^q)^(1 / q), bm = -b) %&gt;%\n    gather(key = \"lab\", value = \"y\", -x)\n  out$lab &lt;- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipseData &lt;- function(n = 100, xlim = c(-2, 3), ylim = c(-2, 3),\n                        mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  df &lt;- expand.grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)\n  )\n  df$z &lt;- dmvnorm(df, mean, Sigma)\n  df\n}\n\nlballmax &lt;- function(ed, q = 1, tol = 1e-6) {\n  ed &lt;- filter(ed, x &gt; 0, y &gt; 0)\n  for (i in 1:20) {\n    ff &lt;- abs((ed$x^q + ed$y^q)^(1 / q) - 1) &lt; tol\n    if (sum(ff) &gt; 0) break\n    tol &lt;- 2 * tol\n  }\n  best &lt;- ed[ff, ]\n  best[which.max(best$z), ]\n}\n\nnb &lt;- normBall(1)\ned &lt;- ellipseData()\nbols &lt;- data.frame(x = 1, y = 1)\nbhat &lt;- lballmax(ed, 1)\nggplot(nb, aes(x, y)) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  geom_label(\n    data = bols, mapping = aes(label = bquote(\"hat(beta)[ols]\")), parse = TRUE,\n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  geom_label(\n    data = bhat, mapping = aes(label = bquote(\"hat(beta)[s]^L\")), parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#ell_1-regularized-regression",
    "href": "schedule/slides/regularization-lm.html#ell_1-regularized-regression",
    "title": "UBC Stat550",
    "section": "\\(\\ell_1\\)-regularized regression",
    "text": "\\(\\ell_1\\)-regularized regression\nKnown as\n\n“lasso”\n“basis pursuit”\n\nThe estimator satisfies\n\\[\\hat\\beta_s = \\argmin_{ \\snorm{\\beta}_1 \\leq s}  \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2\\]\nIn its corresponding Lagrangian dual form:\n\\[\\hat\\beta_\\lambda = \\argmin_{\\beta} \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1\\]"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#lasso-1",
    "href": "schedule/slides/regularization-lm.html#lasso-1",
    "title": "UBC Stat550",
    "section": "Lasso",
    "text": "Lasso\nWhile the ridge solution can be easily computed\n\\[\\argmin_{\\beta} \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_2^2 = (\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1} \\X^{\\top}\\y\\]\nthe lasso solution\n\\[\\argmin_{\\beta} \\frac 1n\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1 = \\; ??\\]\ndoesn’t have a closed-form solution.\nHowever, because the optimization problem is convex, there exist efficient algorithms for computing it\n\n\nThe best are Iterative Soft Thresholding or Coordinate Descent. Gradient Descent doesn’t work very well in practice."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#coefficient-path-ridge-vs-lasso",
    "href": "schedule/slides/regularization-lm.html#coefficient-path-ridge-vs-lasso",
    "title": "UBC Stat550",
    "section": "Coefficient path: ridge vs lasso",
    "text": "Coefficient path: ridge vs lasso\n\n\nCode\nlibrary(glmnet)\ndata(prostate, package = \"ElemStatLearn\")\nX &lt;- prostate |&gt; dplyr::select(-train, -lpsa) |&gt;  as.matrix()\nY &lt;- prostate$lpsa\nlasso &lt;- glmnet(x = X, y = Y) # alpha = 1 by default\nridge &lt;- glmnet(x = X, y = Y, alpha = 0)\nop &lt;- par()"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#additional-intuition-for-why-lasso-selects-variables",
    "href": "schedule/slides/regularization-lm.html#additional-intuition-for-why-lasso-selects-variables",
    "title": "UBC Stat550",
    "section": "Additional intuition for why Lasso selects variables",
    "text": "Additional intuition for why Lasso selects variables\nSuppose, for a particular \\(\\lambda\\), I have solutions for \\(\\widehat{\\beta}_k\\), \\(k = 1,\\ldots,j-1, j+1,\\ldots,p\\).\nLet \\(\\widehat{\\y}_{-j} = \\X_{-j}\\widehat{\\beta}_{-j}\\), and assume WLOG \\(\\overline{\\X}_k = 0\\), \\(\\X_k^\\top\\X_k = 1\\ \\forall k\\)\nOne can show that:\n\\[\n\\widehat{\\beta}_j = S\\left(\\mathbf{X}^\\top_j(\\y - \\widehat{\\y}_{-j}),\\ \\lambda\\right).\n\\]\n\\[\nS(z, \\gamma) = \\textrm{sign}(z)(|z| - \\gamma)_+ = \\begin{cases} z - \\gamma & z &gt; \\gamma\\\\\nz + \\gamma & z &lt; -\\gamma \\\\ 0 & |z| \\leq \\gamma \\end{cases}\n\\]\n\nIterating over this is called coordinate descent and gives the solution\n\n\n\n\nIf I were told all the other coefficient estimates.\nThen to find this one, I’d shrink when the gradient is big, or set to 0 if it gets too small.\n\n\n\nSee for example, https://doi.org/10.18637/jss.v033.i01"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "href": "schedule/slides/regularization-lm.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "title": "UBC Stat550",
    "section": "{glmnet} version (same procedure for lasso or ridge)",
    "text": "{glmnet} version (same procedure for lasso or ridge)\n\nlasso &lt;- cv.glmnet(X, Y) # estimate full model and CV no good reason to call glmnet() itself\n# 2. Look at the CV curve. If the dashed lines are at the boundaries, redo and adjust lambda\nlambda_min &lt;- lasso$lambda.min # the value, not the location (or use lasso$lambda.1se)\ncoeffs &lt;- coefficients(lasso, s = \"lambda.min\") # s can be string or a number\npreds &lt;- predict(lasso, newx = X, s = \"lambda.1se\") # must supply `newx`\n\n\n\\(\\widehat{R}_{CV}\\) is an estimator of \\(R_n\\), it has bias and variance\nBecause we did CV, we actually have 10 \\(\\widehat{R}\\) values, 1 per split.\nCalculate the mean (that’s what we’ve been using), but what about SE?"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#other-flavours",
    "href": "schedule/slides/regularization-lm.html#other-flavours",
    "title": "UBC Stat550",
    "section": "Other flavours",
    "text": "Other flavours\n\nThe elastic net\n\ngenerally used for correlated variables that combines a ridge/lasso penalty. Use glmnet(..., alpha = a) (0 &lt; a &lt; 1).\n\nGrouped lasso\n\nwhere variables are included or excluded in groups. Required for factors (1-hot encoding)\n\nRelaxed lasso\n\nTakes the estimated model from lasso and fits the full least squares solution on the selected covariates (less bias, more variance). Use glmnet(..., relax = TRUE).\n\nDantzig selector\n\na slightly modified version of the lasso"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#lasso-cinematic-universe",
    "href": "schedule/slides/regularization-lm.html#lasso-cinematic-universe",
    "title": "UBC Stat550",
    "section": "Lasso cinematic universe",
    "text": "Lasso cinematic universe\n\n\n\nSCAD\n\na non-convex version of lasso that adds a more severe variable selection penalty\n\n\\(\\sqrt{\\textrm{lasso}}\\)\n\nclaims to be tuning parameter free (but isn’t). Uses \\(\\Vert\\cdot\\Vert_2\\) instead of \\(\\Vert\\cdot\\Vert_1\\) for the loss.\n\nGeneralized lasso\n\nAdds various additional matrices to the penalty term (e.g. \\(\\Vert D\\beta\\Vert_1\\)).\n\nArbitrary combinations\n\ncombine the above penalties in your favourite combinations"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#warnings-on-regularized-regression",
    "href": "schedule/slides/regularization-lm.html#warnings-on-regularized-regression",
    "title": "UBC Stat550",
    "section": "Warnings on regularized regression",
    "text": "Warnings on regularized regression\n\nThis isn’t a method unless you say how to choose \\(\\lambda\\).\nThe intercept is never penalized. Adds an extra degree-of-freedom.\nPredictor scaling is very important.\nDiscrete predictors need groupings.\nCentering the predictors may be necessary\n(These all work with other likelihoods.)\n\n\nSoftware handles most of these automatically, but not always. (No Lasso with factor predictors.)"
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#lasso-theory-under-strong-conditions",
    "href": "schedule/slides/regularization-lm.html#lasso-theory-under-strong-conditions",
    "title": "UBC Stat550",
    "section": "Lasso theory under strong conditions",
    "text": "Lasso theory under strong conditions\nSupport recovery: (Wainwright 2009), see also (Meinshausen and Bühlmann 2006; Zhao and Yu 2006)\n\nThe truth is linear.\n\\(\\norm{\\X'_{S^c}\\X_S (\\X'_S\\X_S)^{-1}}_\\infty &lt; 1-\\epsilon.\\)\n\\(\\lambda_{\\min} (\\X'_S\\X_S) \\geq C_{\\min} &gt; 0\\).\nThe columns of \\(\\X\\) have 2-norm \\(n\\).\nThe noise is iid Normal.\n\\(\\lambda_n\\) satisfies \\(\\frac{n\\lambda^2}{\\log(p-s)} \\rightarrow \\infty\\).\n\\(\\min_j \\{ |\\beta_j| : j \\in S\\} \\geq \\rho_n &gt; 0\\) and \\[\\rho_n^{-1} \\left( \\sqrt{\\frac{\\log s}{n}}+ \\lambda_n\\norm{(\\X'_S\\X_S)^{-1}}_\\infty \\right)\\rightarrow 0\\]\n\nThen, \\(P(\\textrm{supp}(\\hat\\beta_\\lambda) = \\textrm{supp}(\\beta_*))\\rightarrow 1\\)."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#lasso-theory-under-strong-conditions-1",
    "href": "schedule/slides/regularization-lm.html#lasso-theory-under-strong-conditions-1",
    "title": "UBC Stat550",
    "section": "Lasso theory under strong conditions",
    "text": "Lasso theory under strong conditions\nEstimation consistency: (Negahban et al. 2012) also (Meinshausen and Yu 2009)\n\nThe truth is linear.\n\\(\\exists \\kappa\\) such that for all vectors \\(\\theta\\in\\R^p\\) that satisfy \\(\\norm{\\theta_{S^C}}_1 \\leq 3\\norm{\\theta_S}_1\\), we have \\(\\norm{X\\theta}_2^2/n \\geq \\kappa\\norm{\\theta}_2^2\\) (Compatibility)\nThe columns of \\(\\X\\) have 2-norm \\(n\\).\nThe noise is iid sub-Gaussian.\n\\(\\lambda_n &gt;4\\sigma \\sqrt{\\log (p)/n}\\).\n\nThen, with probability at least \\(1-c\\exp(-c'n\\lambda_n^2)\\),\n\\[\\norm{\\hat\\beta_\\lambda-\\beta_*}_2^2 \\leq \\frac{64\\sigma^2}{\\kappa^2}\\frac{s\\log p}{n}.\\]\n\n\n\n\n\n\nImportant\n\n\nThese conditions are very strong, uncheckable in practice, unlikely to be true for real datasets. But theory of this type is the standard for these procedures."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#lasso-under-weak-no-conditions",
    "href": "schedule/slides/regularization-lm.html#lasso-under-weak-no-conditions",
    "title": "UBC Stat550",
    "section": "Lasso under weak / no conditions",
    "text": "Lasso under weak / no conditions\nIf \\(Y\\) and \\(X\\) are bounded by \\(B\\), then with probability at least \\(1-\\delta^2\\), \\[R_n(\\hat\\beta_\\lambda) - R_n(\\beta_*) \\leq \\sqrt{\\frac{16(t+1)^4B^2}{n}\\log\\left(\\frac{\\sqrt{2}p}{\\delta}\\right)}.\\]\nThis is a simple version of a result in (Greenshtein and Ritov 2004).\nNote that it applies to the constrained version.\n(Bartlett, Mendelson, and Neeman 2012) derives the same rate for the Lagrangian version\nAgain, this rate is (nearly) optimal: \\[c\\sqrt{\\frac{s}{n}} &lt; R_n(\\hat\\beta_\\lambda) - R_n(\\beta_*) &lt; C\\sqrt{\\frac{s\\log p}{n}}.\\]\n\\(\\log p\\) is the penalty you pay for selection."
  },
  {
    "objectID": "schedule/slides/regularization-lm.html#references",
    "href": "schedule/slides/regularization-lm.html#references",
    "title": "UBC Stat550",
    "section": "References",
    "text": "References\n\n\n\nUBC Stat 550 - 2024\n\n\n\n\nBartlett, Peter L., Shahar Mendelson, and Joseph Neeman. 2012. “\\(\\ell_1\\)-Regularized Linear Regression: Persistence and Oracle Inequalities.” Probability Theory and Related Fields 154 (1-2): 193–224.\n\n\nBertsimas, Dimitris, Angela King, and Rahul Mazumder. 2016. “Best Subset Selection via a Modern Optimization Lens.” Annals of Statistics 44: 813–52.\n\n\nFoster, Dean P., and Edward I. George. 1994. “The Risk Inflation Criterion for Multiple Regression.” Annals of Statistics 22 (4): 1947–75.\n\n\nGreenshtein, Eitan, and Ya’acov Ritov. 2004. “Persistence in High-Dimensional Linear Predictor Selection and the Virtue of Overparametrization.” Bernoulli 10 (6): 971–88.\n\n\nHsu, Daniel, Sham M Kakade, and Tong Zhang. 2014. “Random Design Analysis of Ridge Regression.” Foundations of Computational Mathematics 14 (3): 569–600.\n\n\nMeinshausen, Nicolai, and Peter Bühlmann. 2006. “High-Dimensional Graphs and Variable Selection with the Lasso.” The Annals of Statistics 34 (3): 1436–62.\n\n\nMeinshausen, Nicolai, and Bin Yu. 2009. “Lasso-Type Recovery of Sparse Representations for High-Dimensional Data.” The Annals of Statistics 37 (1): 246–70.\n\n\nNegahban, Sahand, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. 2012. “A Unified Framework for High-Dimensional Analysis of \\(M\\)-Estimators with Decomposable Regularizers.” Statistical Science 27: 538–337.\n\n\nWainwright, Martin J. 2009. “Sharp Thresholds for High-Dimensional and Noisy Sparsity Recovery Using \\(\\ell_1\\)-Constrained Quadratic Programming (Lasso).” IEEE Transactions on Information Theory 55 (5): 2183–2202.\n\n\nZhao, Peng, and Bin Yu. 2006. “On Model Selection Consistency of Lasso.” The Journal of Machine Learning Research 7: 2541–63."
  },
  {
    "objectID": "schedule/slides/time-series.html#section",
    "href": "schedule/slides/time-series.html#section",
    "title": "UBC Stat550",
    "section": "Time series, a whirlwind",
    "text": "Time series, a whirlwind\nStat 550\nDaniel J. McDonald\nLast modified – 03 April 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#the-general-linear-process",
    "href": "schedule/slides/time-series.html#the-general-linear-process",
    "title": "UBC Stat550",
    "section": "The general linear process",
    "text": "The general linear process\n\nImagine that there is a noise process\n\n\\[\\epsilon_j \\sim \\textrm{N}(0, 1),\\ \\textrm{i.i.d.}\\]\n\nAt time \\(i\\), we observe the sum of all past noise\n\n\\[y_i = \\sum_{j=-\\infty}^0 a_{i+j} \\epsilon_j\\]\n\nWithout some conditions on \\(\\{a_k\\}_{k=-\\infty}^0\\) this process will “run away”\nThe result is “non-stationary” and difficult to analyze.\nStationary means (roughly) that the marginal distribution of \\(y_i\\) does not change with \\(i\\)."
  },
  {
    "objectID": "schedule/slides/time-series.html#chasing-stationarity",
    "href": "schedule/slides/time-series.html#chasing-stationarity",
    "title": "UBC Stat550",
    "section": "Chasing stationarity",
    "text": "Chasing stationarity\n\nn &lt;- 1000\nnseq &lt;- 5\ngenerate_ar &lt;- function(n, b) {\n  y &lt;- double(n)\n  y[1] &lt;- rnorm(1)\n  for (i in 2:n) y[i] &lt;- b * y[i - 1] + rnorm(1)\n  tibble(time = 1:n, y = y)\n}\nstationary &lt;- map(1:nseq, ~ generate_ar(n, .99)) |&gt; list_rbind(names_to = \"id\")\nnon_stationary &lt;- map(1:nseq, ~ generate_ar(n, 1.01)) |&gt;\n  list_rbind(names_to = \"id\")"
  },
  {
    "objectID": "schedule/slides/time-series.html#uses-of-stationarity",
    "href": "schedule/slides/time-series.html#uses-of-stationarity",
    "title": "UBC Stat550",
    "section": "Uses of stationarity",
    "text": "Uses of stationarity\n\nLots of types (weak, strong, in-mean, wide-sense,…)\nnot required for modelling / forecasting\nBut assuming stationarity gives some important guarantees\nUsually work with stationary processes"
  },
  {
    "objectID": "schedule/slides/time-series.html#standard-models",
    "href": "schedule/slides/time-series.html#standard-models",
    "title": "UBC Stat550",
    "section": "Standard models",
    "text": "Standard models\nAR(p)\nSuppose \\(\\epsilon_i\\) are i.i.d. N(0, 1) (distn is convenient, but not required)\n\\[y_i = \\mu + a_1 y_{i-1} + \\cdots + a_p y_{i-p} + \\epsilon_i\\]\n\nThis is a special case of the general linear process\nYou can recursively substitute this defn into itself to get that equation\n\nEasy to estimate the a’s given a realization.\n\ny &lt;- arima.sim(list(ar = c(.7, -.1)), n = 1000)\nY &lt;- y[3:1000]\nX &lt;- cbind(lag1 = y[2:999], lag2 = y[1:998])\nsummary(lm(Y ~ X + 0))\n\n\nCall:\nlm(formula = Y ~ X + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6164 -0.6638  0.0271  0.6456  3.8367 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nXlag1  0.66931    0.03167  21.134   &lt;2e-16 ***\nXlag2 -0.04856    0.03167  -1.533    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9899 on 996 degrees of freedom\nMultiple R-squared:  0.4085,    Adjusted R-squared:  0.4073 \nF-statistic:   344 on 2 and 996 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/time-series.html#arp-1",
    "href": "schedule/slides/time-series.html#arp-1",
    "title": "UBC Stat550",
    "section": "AR(p)",
    "text": "AR(p)\n\nThe estimate isn’t that accurate because the residuals (not the \\(\\epsilon\\)’s) are correlated.\n(Usually, you get 1/n convergence, here you don’t.)\nAlso, this isn’t the MLE. The likelihood includes \\(p(y_1)\\), \\(p(y_2 | y_1)\\) which lm() ignored.\nThe Std. Errors are unjustified.\nBut that was easy to do.\nThe correct way is\n\n\n\n\nCall:\narima(x = y, order = c(2, 0, 0), include.mean = FALSE)\n\nCoefficients:\n         ar1      ar2\n      0.6686  -0.0485\ns.e.  0.0316   0.0316\n\nsigma^2 estimated as 0.9765:  log likelihood = -1407.34,  aic = 2820.67\n\n\n\nThe resulting estimates and SEs are identical, AFAICS."
  },
  {
    "objectID": "schedule/slides/time-series.html#maq",
    "href": "schedule/slides/time-series.html#maq",
    "title": "UBC Stat550",
    "section": "MA(q)",
    "text": "MA(q)\nStart with the general linear process, but truncate the infinite sum.\n\\[y_i = \\sum_{j=-q}^0 a_{i+j} \\epsilon_j\\]\n\nThis is termed a “moving average” process.\nthough \\(a_0 + \\cdots a_{-q}\\) don’t sum to 1.\nCan’t write this easily as a lm()\n\n\ny &lt;- arima.sim(list(ma = c(.9, .6, .1)), n = 1000)\narima(y, c(0, 0, 3), include.mean = FALSE)\n\n\nCall:\narima(x = y, order = c(0, 0, 3), include.mean = FALSE)\n\nCoefficients:\n         ma1     ma2     ma3\n      0.9092  0.6069  0.1198\ns.e.  0.0313  0.0380  0.0311\n\nsigma^2 estimated as 0.8763:  log likelihood = -1353.41,  aic = 2714.82"
  },
  {
    "objectID": "schedule/slides/time-series.html#maq-as-an-ar1-hidden-process",
    "href": "schedule/slides/time-series.html#maq-as-an-ar1-hidden-process",
    "title": "UBC Stat550",
    "section": "MA(q) as an AR(1) hidden process",
    "text": "MA(q) as an AR(1) hidden process\nLet \\(X_j = [\\epsilon_{j-1},\\ \\ldots,\\  \\epsilon_{j-q}]\\) and write\n\\[\n\\begin{aligned}\nX_i &= \\begin{bmatrix} a_{i-1} & a_{i-2} & \\cdots & a_{i-q}\\\\ 1 & 0 & \\cdots & 0\\\\ & & \\ddots \\\\ 0 & 0 & \\cdots & 1\\end{bmatrix} X_{i-1} +\n\\begin{bmatrix} a_{i}\\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\epsilon_i\\\\\ny_i &= \\begin{bmatrix} 1 & 0 & \\cdots 0 \\end{bmatrix} X_i\n\\end{aligned}\n\\]\n\nNow \\(X\\) is a \\(q\\)-dimensional AR(1) (but we don’t see it)\n\\(y\\) is deterministic conditional on \\(X\\)\nThis is the usual way these are estimated using a State-Space Model\nMany time series models have multiple equivalent representations"
  },
  {
    "objectID": "schedule/slides/time-series.html#arima",
    "href": "schedule/slides/time-series.html#arima",
    "title": "UBC Stat550",
    "section": "ARIMA",
    "text": "ARIMA\n\nWe’ve been using arima() and arima.sim(), so what is left?\nThe “I” means “integrated”\nIf, for example, we can write \\(z_i = y_i - y_{i-1}\\) and \\(z\\) follows an ARMA(p, q), we say \\(y\\) follows an ARIMA(p, 1, q).\nThe middle term is the degree of differencing"
  },
  {
    "objectID": "schedule/slides/time-series.html#other-standard-models",
    "href": "schedule/slides/time-series.html#other-standard-models",
    "title": "UBC Stat550",
    "section": "Other standard models",
    "text": "Other standard models\nSuppose we can write\n\\[\ny_i = T_i + S_i + W_i\n\\]\nThis is the “classical” decomposition of \\(y\\) into a Trend + Seasonal + Noise.\nYou can estimate this with a “Basic Structural Time Series Model” using StrucTS().\nA related, though slightly different model is called the STL decomposition, estimated with stl().\nThis is “Seasonal Decomposition of Time Series by Loess”\n(LOESS is “locally estimated scatterplot smoothing” named/proposed independently by Bill Cleveland though originally proposed about 15 years earlier and called the Savitsky-Golay Filter)"
  },
  {
    "objectID": "schedule/slides/time-series.html#quick-example",
    "href": "schedule/slides/time-series.html#quick-example",
    "title": "UBC Stat550",
    "section": "Quick example",
    "text": "Quick example\n\nsts &lt;- StructTS(AirPassengers)\nbc &lt;- stl(AirPassengers, \"periodic\") # use sin/cos to represent the seasonal\ntibble(\n  time = seq(as.Date(\"1949-01-01\"), as.Date(\"1960-12-31\"), by = \"month\"),\n  AP = AirPassengers, \n  StrucTS = fitted(sts)[, 1], \n  STL = rowSums(bc$time.series[, 1:2])\n) |&gt;\n  pivot_longer(-time) |&gt;\n  ggplot(aes(time, value, color = name)) +\n  geom_line() +\n  theme_bw(base_size = 24) +\n  scale_color_viridis_d(name = \"\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "schedule/slides/time-series.html#generic-state-space-model",
    "href": "schedule/slides/time-series.html#generic-state-space-model",
    "title": "UBC Stat550",
    "section": "Generic state space model",
    "text": "Generic state space model\n\n\n\n\\[\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}\\]\n\n\n\\(x_k\\) is unobserved, dimension \\(n\\)\n\\(y_k\\) is observed, dimension \\(m\\)\n\\(x\\) process is the transition or process equation\n\\(y\\) is the observation or measurement equation\nBoth are probability distributions that can depend on parameters \\(\\theta\\)\nFor now, assume \\(\\theta\\) is KNOWN\nWe can allow the densities to vary with time."
  },
  {
    "objectID": "schedule/slides/time-series.html#goals",
    "href": "schedule/slides/time-series.html#goals",
    "title": "UBC Stat550",
    "section": "GOAL(s)",
    "text": "GOAL(s)\n\nFiltering: given observations, find \\[p(x_k | y_1,\\ldots y_k)\\]\nSmoothing: given observations, find \\[p(x_k | y_1,\\ldots y_T), \\;\\;\\ k &lt; T\\]\nForecasting: given observations, find \\[p(y_{k+1} | y_1,\\ldots,y_k)\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#using-bayes-rule",
    "href": "schedule/slides/time-series.html#using-bayes-rule",
    "title": "UBC Stat550",
    "section": "Using Bayes Rule",
    "text": "Using Bayes Rule\nAssume \\(p(x_0)\\) is known\n\\[\n\\begin{aligned}\np(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T) &= \\prod_{k=1}^T p(y_k | x_k)\\\\\np(x_0,\\ldots,x_T) &= p(x_0) \\prod_{k=1}^T p(x_k | x_{k-1})\\\\\np(x_0,\\ldots,x_T\\ |\\ y_1,\\ldots,y_T) &= \\frac{p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)}{p(y_1,\\ldots,y_T)}\\\\ &\\propto p(y_1,\\ldots,y_T\\ |\\ x_1, \\ldots, x_T)p(x_0,\\ldots,x_T)\\end{aligned}\n\\]\nIn principle, if things are nice, you can compute this posterior (thinking of \\(x\\) as unknown parameters)\nBut in practice, computing a big multivariate posterior like this is computationally ill-advised."
  },
  {
    "objectID": "schedule/slides/time-series.html#generic-filtering",
    "href": "schedule/slides/time-series.html#generic-filtering",
    "title": "UBC Stat550",
    "section": "Generic filtering",
    "text": "Generic filtering\n\nRecursively build up \\(p(x_k | y_1,\\ldots y_k)\\).\nWhy? Because if we’re collecting data in real time, this is all we need to make forecasts for future data.\n\n\\[\\begin{aligned} &p(y_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1}, y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | y_1,\\ldots,y_T)\\\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | x_T) p(x_T | y_1,\\ldots,y_T)\\end{aligned}\\]\n\nCan continue to iterate if I want to predict \\(h\\) steps ahead\n\n\\[\\begin{aligned} &p(y_{T+h} | y_1,\\ldots,y_T)= p(y_{T+h} | x_{T+h} )\\prod_{j=0}^{h-1} p(x_{T+j+1} | x_{T+j}) p(x_T | y_1,\\ldots,y_T)\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#the-filtering-recursion",
    "href": "schedule/slides/time-series.html#the-filtering-recursion",
    "title": "UBC Stat550",
    "section": "The filtering recursion",
    "text": "The filtering recursion\n\nInitialization. Fix \\(p(x_0)\\).\n\nIterate the following for \\(k=1,\\ldots,T\\):\n\nPredict. \\[p(x_k | y_{k-1}) = \\int p(x_k | x_{k-1}) p(x_{k-1} | y_1,\\ldots, y_{k-1})dx_{k-1}.\\]\nUpdate. \\[p(x_k | y_1,\\ldots,y_k) = \\frac{p(y_k | x_k)p(x_k | y_1,\\ldots,y_{k-1})}{p(y_1,\\ldots,y_k)}\\]\n\nIn general, this is somewhat annoying because these integrals may be challenging to solve.\nBut with some creativity, we can use Monte Carlo for everything."
  },
  {
    "objectID": "schedule/slides/time-series.html#what-if-we-make-lots-of-assumptions",
    "href": "schedule/slides/time-series.html#what-if-we-make-lots-of-assumptions",
    "title": "UBC Stat550",
    "section": "What if we make lots of assumptions?",
    "text": "What if we make lots of assumptions?\nAssume that \\[\\begin{aligned}p(x_0) &= N(m_0, P_0) \\\\ p_k(x_k\\ |\\ x_{k-1}) &= N(A_{k-1}x_{k-1},\\ Q_{k-1})\\\\ p_k(y_k\\ |\\ x_k) &= N(H_k x_k,\\ R_k)\\end{aligned}.\\]\nThen all the ugly integrals have closed-form representations by properties of conditional Gaussian distributions."
  },
  {
    "objectID": "schedule/slides/time-series.html#closed-form-representations",
    "href": "schedule/slides/time-series.html#closed-form-representations",
    "title": "UBC Stat550",
    "section": "Closed-form representations",
    "text": "Closed-form representations\n\n\nDistributions:\n\\[\n\\begin{aligned}\np(x_k | y_1,\\ldots,y_{k-1}) &= N(m^{-}_k, P^{-}_k)\\\\\np(x_k | y_1,\\ldots,y_{k}) &= N(m_k, P_k)\\\\\np(y_{k} | y_1,\\ldots,y_{k-1}) &= N(H_k m^-_k, S_k)\\\\\n\\end{aligned}\n\\] Prediction: \\[\n\\begin{aligned}\nm^-_k &= A_{k-1}m_{k-1}\\\\\nP^-_k &= A_{k-1}P_{k-1}A^\\mathsf{T}_{k-1} + Q_{k-1}\n\\end{aligned}\n\\]\n\nUpdate: \\[\n\\begin{aligned}\nv_k &= y_k - H_k m_k^-\\\\\nS_k &= H_k P_k^- H_k^\\mathsf{T} + R_k\\\\\nK_k &= P^-_k H_k^\\mathsf{T} S_k^{-1}\\\\\nm_k &= m^-_k + K_{k}v_{k}\\\\\nP_k &= P^-_k - K_k S_k K_k^\\mathsf{T}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#code-or-it-isnt-real-kalman-filter",
    "href": "schedule/slides/time-series.html#code-or-it-isnt-real-kalman-filter",
    "title": "UBC Stat550",
    "section": "Code or it isn’t real (Kalman Filter)",
    "text": "Code or it isn’t real (Kalman Filter)\n\n\n\nkalman &lt;- function(y, m0, P0, A, Q, H, R) {\n  n &lt;- length(y)\n  m &lt;- double(n + 1)\n  P &lt;- double(n + 1)\n  m[1] &lt;- m0\n  P[1] &lt;- P0\n  for (k in seq(n)) {\n    mm &lt;- A * m[k]\n    Pm &lt;- A * P[k] * A + Q\n    v &lt;- y[k] - H * mm\n    S &lt;- H * Pm * H + R\n    K &lt;- Pm * H / S\n    m[k + 1] &lt;- mm + K * v\n    P[k + 1] &lt;- Pm - K * S * K\n  }\n  tibble(t = 1:n, m = m[-1], P = P[-1])\n}\n\nset.seed(2022 - 06 - 01)\nx &lt;- double(100)\nfor (k in 2:100) x[k] &lt;- x[k - 1] + rnorm(1)\ny &lt;- x + rnorm(100, sd = 1)\nkf &lt;- kalman(y, 0, 5, 1, 1, 1, 1)"
  },
  {
    "objectID": "schedule/slides/time-series.html#important-notes",
    "href": "schedule/slides/time-series.html#important-notes",
    "title": "UBC Stat550",
    "section": "Important notes",
    "text": "Important notes\n\nSo far, we assumed all parameters were known.\nIn reality, we had 6: m0, P0, A, Q, H, R\nI sort of also think of x as “parameters” in the Bayesian sense\nBy that I mean, “latent variables for which we have prior distributions”\nWhat if we want to estimate them?\n\nBayesian way: m0 and P0 are already the parameters of for the prior on x1. Put priors on the other 4.\nFrequentist way: Just maximize the likelihood. Can technically take P0 \\(\\rightarrow\\infty\\) to remove it and m0\n\nThe Likelihood is produced as a by-product of the Kalman Filter.\n\\[-\\ell(\\theta) = \\sum_{k=1}^T \\left(v_k^\\mathsf{T}S_k^{-1}v_k + \\log |S_k| + m \\log 2\\pi\\right)\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#smoothing",
    "href": "schedule/slides/time-series.html#smoothing",
    "title": "UBC Stat550",
    "section": "Smoothing",
    "text": "Smoothing\n\nWe also want \\(p(x_k | y_1,\\ldots,y_{T})\\)\nFiltering went “forward” in time. At the end we got, \\(p(x_T | y_1,\\ldots,y_{T})\\). Smoothing starts there and goes “backward”\nFor “everything linear Gaussian”, this is again “easy”\nSet \\(m_T^s = m_T\\), \\(P_T^s = P_T\\).\nFor \\(k = T-1,\\ldots,1\\),\n\n\\[\\begin{aligned}\nG_k &= P_k A_k^\\mathsf{T} [P_{k+1}^-]^{-1}\\\\\nm_k^s &= m_k + G_k(m_{k+1}^s - m_{k+1}^-)\\\\\nP_k^s &= P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^\\mathsf{T}\\\\\nx_k | y_1,\\ldots,y_T &= N(m^s_k, P_k^s)\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/time-series.html#comparing-the-filter-and-the-smoother",
    "href": "schedule/slides/time-series.html#comparing-the-filter-and-the-smoother",
    "title": "UBC Stat550",
    "section": "Comparing the filter and the smoother",
    "text": "Comparing the filter and the smoother\n\nSame data, different code (using a package)\n\n\nlibrary(FKF)\nfilt &lt;- fkf(\n  a0 = 0, P0 = matrix(5), dt = matrix(0), ct = matrix(0),\n  Tt = matrix(1), Zt = matrix(1), HHt = matrix(1), GGt = matrix(1),\n  yt = matrix(y, ncol = length(y))\n)\nsmo &lt;- fks(filt)"
  },
  {
    "objectID": "schedule/slides/time-series.html#what-about-non-linear-andor-non-gaussian",
    "href": "schedule/slides/time-series.html#what-about-non-linear-andor-non-gaussian",
    "title": "UBC Stat550",
    "section": "What about non-linear and/or non-Gaussian",
    "text": "What about non-linear and/or non-Gaussian\n\\[\\begin{aligned} x_k &\\sim p(x_k | x_{k-1}) \\\\ y_k &\\sim p(y_k | x_k)\\end{aligned}\\]\nThen we need to solve integrals. This is a pain. We approximate them.\nThese all give approximations to the filtering distribution\n\nExtended Kalman filter - basically do a Taylor approximation, then do Kalman like\nUncented Kalman filter - Approximate integrals with Sigma points\nParticle filter - Sequential Monte Carlo\nBootstrap filter (simple version of SMC)\nLaplace Gaussian filter - Do a Laplace approximation to the distributions"
  },
  {
    "objectID": "schedule/slides/time-series.html#the-bootstrap-filter",
    "href": "schedule/slides/time-series.html#the-bootstrap-filter",
    "title": "UBC Stat550",
    "section": "The bootstrap filter",
    "text": "The bootstrap filter\n\nNeed to simulate from the transition distribution (rtrans)\nNeed to evaluate the observation distribution (dobs)\n\n\nboot_filter &lt;-\n  function(y, B = 1000, rtrans, dobs, a0 = 0, P0 = 1, perturb = function(x) x) {\n    n &lt;- length(y)\n    filter_est &lt;- matrix(0, n, B)\n    predict_est &lt;- matrix(0, n, B)\n    init &lt;- rnorm(B, a0, P0)\n    filter_est[1, ] &lt;- init\n    for (i in seq(n)) {\n      raw_w &lt;- dobs(y[i], filter_est[i, ])\n      w &lt;- raw_w / sum(raw_w)\n      selection &lt;- sample.int(B, replace = TRUE, prob = w)\n      filter_est[i, ] &lt;- perturb(filter_est[i, selection])\n      predict_est[i, ] &lt;- rtrans(filter_est[i, ])\n      if (i &lt; n) filter_est[i + 1, ] &lt;- predict_est[i, ]\n    }\n    list(filt = filter_est, pred = predict_est)\n  }\n\n\n\n\nUBC Stat 550 - 2024"
  }
]