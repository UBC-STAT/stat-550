{
  "hash": "da189046925c9ca229d4d2ef3c3ee2f1",
  "result": {
    "markdown": "---\nlecture: \"Linear models, selection, regularization, and inference\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\nbibliography: refs.bib\n---\n## {{< meta lecture >}} {.large background-image=\"img/consult.jpeg\" background-opacity=\"0.3\"}\n\n[Stat 550]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 02 April 2024\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n$$\n\n\n\n\n## Recap\n\nModel Selection means [select a family of distributions for your data]{.secondary}.\n\nIdeally, we'd do this by comparing the $R_n$ for one family with that for\nanother.\n\nWe'd use whichever has smaller $R_n$.\n\nBut $R_n$ depends on the truth, so we estimate it with $\\widehat{R}$.\n\nThen we use whichever has smaller $\\widehat{R}$.\n\n## Example\n\nThe truth:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100),\n  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)\n)\n```\n:::\n\n\nModel 1: $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i$, $\\quad\\epsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)$\n\nModel 2: `y ~ x1 + x2 + x1*x2` (what's the math version?)\n\nModel 3: `y ~ x2 + sin(x1 * x2)`\n\n\n## Fit each model and estimate $R_n$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlist(\"y ~ x1 + x2\", \"y ~ x1 * x2\", \"y ~ x2 + sin(x1*x2)\") |>\n  map(~ {\n    fits <- lm(as.formula(.x), data = dat)\n    tibble(\n      R2 = summary(fits)$r.sq,\n      training_error = mean(residuals(fits)^2),\n      loocv = mean( (residuals(fits) / (1 - hatvalues(fits)))^2 ),\n      AIC = AIC(fits),\n      BIC = BIC(fits)\n    )\n  }) |> list_rbind()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 5\n     R2 training_error loocv   AIC   BIC\n  <dbl>          <dbl> <dbl> <dbl> <dbl>\n1 0.589           21.3  22.9  598.  608.\n2 0.595           21.0  23.4  598.  611.\n3 0.586           21.4  23.0  598.  609.\n```\n:::\n:::\n\n\n# Greedy selection\n\n::: {.callout-note}\nI'm doing everything for linear models, but applies to generalized linear models.\n:::\n\n## Model Selection vs. Variable Selection\n\nModel selection is very comprehensive\n\nYou choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.\n\nVariable selection is a subset of this. It means \n\n> choosing which predictors to include in a predictive model\n\nEliminating a predictor, means removing it from the model.\n\nSome [procedures]{.hand} automatically search predictors, and eliminate some.\n\nWe call this variable selection. But the procedure is implicitly selecting a model\nas well.\n\n\nMaking this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them.\n\n## Selecting variables / predictors with linear methods\n\n\nSuppose we have a pile of predictors.\n\nWe estimate models with different subsets of predictors and use CV / Cp / AIC \n/ BIC to decide which is preferred.\n\nSometimes you might have a few plausible subsets. Easy enough to choose with our criterion.\n\nSometimes you might just have a bunch of predictors, then what do you do?\n\n## Best subsets\n\nIf we imagine that only a few predictors are relevant, we could solve\n\n$$\\min_{\\beta\\in\\R^p} \\frac{1}{2n}\\norm{Y-\\X\\beta}_2^2 + \\lambda\\norm{\\beta}_0$$\n\n\nThe $\\ell_0$-norm counts the number of non-zero coefficients.\n\nThis may or may not be a good thing to do.\n\nIt is computationally infeasible if $p$ is more than about 20.\n\nTechnically NP-hard (you must find the error of each of the $2^p$ models)\n\nThough see [@BertsimasKing2016] for a method of solving reasonably large cases via mixed integer programming.\n\n## Greedy methods\n\nBecause this is an NP-hard problem, we fall back on greedy algorithms.\n\nAll are implemented by the `regsubsets` function in the `leaps` package. \n\nAll subsets\n: estimate model based on every possible subset of size $|\\mathcal{S}| \\leq \\min\\{n, p\\}$, use one with \nlowest risk estimate\n\nForward selection\n: start with $\\mathcal{S}=\\varnothing$, add predictors greedily\n\nBackward selection\n: start with $\\mathcal{S}=\\{1,\\ldots,p\\}$, remove greedily\n\nHybrid\n: combine forward and backward smartly\n\n##\n\n::: {.callout-note}\nWithin each procedure, we're comparing _nested_ models.\n:::\n\n\n## Costs and benefits\n\n\nAll subsets\n: üëç estimates each subset  \nüí£ takes $2^p$ model fits when $p<n$. If $p=50$, this is about $10^{15}$ models. \n\nForward selection\n: üëç computationally feasible  \nüí£ ignores some models, correlated predictors means bad performance\n\nBackward selection\n: üëç computationally feasible  \nüí£ ignores some models, correlated predictors means bad performance  \nüí£ doesn't work if $p>n$\n\nHybrid\n: üëç visits more models than forward/backward  \nüí£ slower\n\n\n## Synthetic example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2024 - 550)\nn <- 550\ndf <- tibble( \n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  x5 = x1 + rnorm(n, sd = .1), # correlated with x1\n  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)\n  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only\n)\n```\n:::\n\n\n$\\mathbf{x}_1$ and $\\mathbf{x}_2$ are the true predictors\n\nBut the rest are correlated with them\n\n\n## Full model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull <- lm(y ~ ., data = df)\nsummary(full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.120 -1.386 -0.060  1.417  6.536 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -0.17176    0.21823  -0.787  0.43158   \nx1           4.94560    1.62872   3.036  0.00251 **\nx2           1.88209    1.34057   1.404  0.16091   \nx3           0.10755    0.90835   0.118  0.90579   \nx4          -1.51043    0.97746  -1.545  0.12287   \nx5          -1.79872    0.94961  -1.894  0.05874 . \nx6          -0.08277    0.92535  -0.089  0.92876   \nx7          -0.05477    0.90159  -0.061  0.95159   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.176 on 542 degrees of freedom\nMultiple R-squared:  0.6538,\tAdjusted R-squared:  0.6494 \nF-statistic: 146.2 on 7 and 542 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\n## True model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntruth <- lm(y ~ x1 + x2, data = df)\nsummary(truth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0630 -1.4199 -0.0654  1.3871  6.7382 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.12389    0.20060  -0.618    0.537    \nx1           2.99853    0.09434  31.783  < 2e-16 ***\nx2           0.44614    0.09257   4.820 1.87e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.179 on 547 degrees of freedom\nMultiple R-squared:  0.6498,\tAdjusted R-squared:  0.6485 \nF-statistic: 507.5 on 2 and 547 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\n## All subsets\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(leaps)\ntrythemall <- regsubsets(y ~ ., data = df)\nsummary(trythemall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df)\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \" \"\n3  ( 1 ) \"*\" \"*\" \" \" \" \" \"*\" \" \" \" \"\n4  ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \" \"\n5  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \"\n6  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n```\n:::\n:::\n\n\n\n## BIC and Cp\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/more-all-subsets1-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Theory\n\nThis result is due to @FosterGeorge1994.\n\n1. If the truth is linear.\n2. $\\lambda = C\\sigma^2\\log p.$\n3. $\\norm{\\beta_*}_0 = s$\n\n$$\\frac{\\Expect{\\norm{\\X\\beta_*-\\X\\hat\\beta}_2^2}/n}{s\\sigma^2/n} \\leq 4\\log p + 2 + o(1).$$\n\n\n$$\\inf_{\\hat\\beta}\\sup_{\\X,\\beta_*} \\frac{\\Expect{\\norm{\\X\\beta_*-\\X\\hat\\beta}_2^2}/n}{s\\sigma^2/n} \\geq 2\\log p - o(\\log p).$$\n\n\n##\n\n::: {.callout-important}\n\n- even if we could compute the subset selection estimator at scale, it‚Äôs not clear that we would want to\n- (Many people assume that we would.) \n- theory provides an understanding of the performance of various estimators under typically idealized conditions\n\n:::\n\n\n\n# Regularization\n\n## Regularization\n\n\n* Another way to control bias and variance is through [regularization]{.secondary} or\n[shrinkage]{.secondary}.  \n\n\n* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\n\n\n* But, make your estimates of $\\beta$ \"smaller\"\n\n\n\n## Brief aside on optimization\n\n* An optimization problem has 2 components:\n\n    1. The \"Objective function\": e.g. $\\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2$.\n    2. The \"constraint\": e.g. \"fewer than 5 non-zero entries in $\\beta$\".\n    \n* A constrained minimization problem is written\n\n\n$$\\min_\\beta f(\\beta)\\;\\; \\mbox{ subject to }\\;\\; C(\\beta)$$\n\n* $f(\\beta)$ is the objective function\n* $C(\\beta)$ is the constraint\n\n\n## Ridge regression (constrained version)\n\nOne way to do this for regression is to solve (say):\n$$\n\\minimize_\\beta \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\sum_j \\beta^2_j < s\n$$\nfor some $s>0$.\n\n* This is called \"ridge regression\".\n* Write the minimizer as $\\hat{\\beta}_s$.\n\n. . .\n\nCompare this to ordinary least squares:\n\n$$\n\\minimize_\\beta \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2 \n\\quad \\st \\beta \\in \\R^p\n$$\n\n\n\n## Geometry of ridge regression (contours)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/plotting-functions-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Reminder of norms we should remember\n\n$\\ell_q$-norm\n: $\\left(\\sum_{j=1}^p |z_j|^q\\right)^{1/q}$\n\n$\\ell_1$-norm (special case)\n: $\\sum_{j=1}^p |z_j|$\n\n$\\ell_0$-norm\n: $\\sum_{j=1}^p I(z_j \\neq 0 ) = \\lvert \\{j : z_j \\neq 0 \\}\\rvert$\n\n$\\ell_\\infty$-norm\n: $\\max_{1\\leq j \\leq p} |z_j|$\n\n::: aside\nRecall what a norm is: <https://en.wikipedia.org/wiki/Norm_(mathematics)>\n:::\n\n\n## Ridge regression\n\nAn equivalent way to write\n\n$$\\hat\\beta_s = \\argmin_{ \\Vert \\beta \\Vert_2^2 \\leq s} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2$$\n\n\nis in the [Lagrangian]{.secondary} form\n\n\n$$\\hat\\beta_\\lambda = \\argmin_{ \\beta} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\frac{\\lambda}{2} \\Vert \\beta \\Vert_2^2.$$\n\n\n\n\nFor every $\\lambda$ there is a unique $s$ (and vice versa) that makes \n\n$$\\hat\\beta_s = \\hat\\beta_\\lambda$$\n\n## Ridge regression\n\n$\\hat\\beta_s = \\argmin_{ \\Vert \\beta \\Vert_2^2 \\leq s} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2$\n\n$\\hat\\beta_\\lambda = \\argmin_{ \\beta} \\frac{1}{2n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\frac{\\lambda}{2} \\Vert \\beta \\Vert_2^2.$\n\nObserve:\n\n* $\\lambda = 0$ (or $s = \\infty$) makes $\\hat\\beta_\\lambda = \\hat\\beta_{ols}$\n* Any $\\lambda > 0$ (or $s <\\infty$)  penalizes larger values of $\\beta$, effectively shrinking them.\n\n\n$\\lambda$ and $s$ are known as [tuning parameters]{.secondary}\n\n\n\n\n## Example data\n\n`prostate` data from [ESL]\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 97 √ó 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    <dbl>   <dbl> <int>  <dbl> <int> <dbl>   <int> <int>  <dbl> <lgl>\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ‚Ñπ 87 more rows\n```\n:::\n:::\n\n\n::: notes\n\nUse `lpsa` as response.\n\n:::\n\n\n## Ridge regression path\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nY <- prostate$lpsa\nX <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\n```\n:::\n\n\n::: flex\n::: w-60\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n::: w-35\n\nModel selection here: \n\n* means [choose]{.secondary} some $\\lambda$ \n\n* A value of $\\lambda$ is a vertical line.\n\n* This graphic is a \"path\" or \"coefficient trace\"\n\n* Coefficients for varying $\\lambda$\n:::\n:::\n\n\n## Solving the minimization\n\n* One nice thing about ridge regression is that it has a closed-form solution (like OLS)\n\n\n$$\\hat\\beta_\\lambda = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n* This is easy to calculate in `R` for any $\\lambda$.\n\n* However, computations and interpretation are simplified if we examine the \n[Singular Value Decomposition]{.secondary} of $\\X = \\mathbf{UDV}^\\top$.\n\n* Recall: any matrix has an SVD.\n\n* Here $\\mathbf{D}$ is diagonal and $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal: $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$.\n\n## Solving the minization\n\n$$\\hat\\beta_\\lambda = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n* Note that $\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top$.\n\n\n* Then,\n\n\n$$\\hat\\beta_\\lambda = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.$$\n\n* For computations, now we only need to invert $\\mathbf{D}$.\n\n\n## Comparing with OLS\n\n\n* $\\mathbf{D}$ is a diagonal matrix\n\n$$\\hat\\beta_{ols} = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y$$\n\n$$\\hat\\beta_\\lambda = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.$$\n\n\n* Notice that $\\hat\\beta_{ols}$ depends on $d_j/d_j^2$ while $\\hat\\beta_\\lambda$ depends on $d_j/(d_j^2 + \\lambda)$.\n\n* Ridge regression makes the coefficients smaller relative to OLS.\n\n* But if $\\X$ has small singular values, ridge regression compensates with $\\lambda$ in the denominator.\n\n# Multicollinearity\n\n## Ridge regression and multicollinearity\n\n[Multicollinearity:]{.secondary} a linear combination of predictor variables is nearly equal to another predictor variable. \n\n## Multicollinearity questions\n\n1. Can I test `cor(x1, x2) == 0` to determine if these are collinear?\n2. What plots or summaries can I look at?\n3. If multivariate regression or logistic regression is applied on a data set with many explanatory variables, what in the regression output might indicate potential multicollinearity?\n4. Is there a test or diagnostic procedure for multicollinearity? \n\n\n::: notes\n1. No. \n2. Correlation matrix of continuous $x$. \n3. Large standard errors, estimated coefficients with opposite sign. `NA` estimates. Removing vars brings down SEs without much change in fit.\n4. Big VIF `summary(lm(xj ~ . - xj - y))$r.sq`\n:::\n\n\n\n## Multicollinearity thoughts\n\nSome comments:\n\n* A better phrase: $\\X$ is ill-conditioned\n\n* AKA \"(numerically) rank-deficient\".\n\n* $\\X = \\mathbf{U D V}^\\top$ ill-conditioned $\\Longleftrightarrow$ some elements of $\\mathbf{D} \\approx 0$\n\n* $\\hat\\beta_{ols}= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y$. Small entries of $\\mathbf{D}$ $\\Longleftrightarrow$ huge elements of $\\mathbf{D}^{-1}$\n\n* Means huge variance: $\\Var{\\hat\\beta_{ols}} =  \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top$\n\n* If you're doing prediction, this is a purely computational concern.\n\n\n## Ridge regression and ill-posed $\\X$\n\n\nRidge Regression fixes this problem by preventing the division by a near-zero number\n\nConclusion\n: $(\\X^{\\top}\\X)^{-1}$ can be really unstable, while $(\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}$ is not.\n\nAside\n: Engineering approach to solving linear systems is to always do this with small $\\lambda$. The thinking is about the numerics rather than the statistics.\n\n### Which $\\lambda$ to use?\n\nComputational\n: Use CV and pick the $\\lambda$ that makes this smallest.\n\nIntuition (bias)\n: As $\\lambda\\rightarrow\\infty$, bias ‚¨Ü\n\nIntuition (variance)\n: As $\\lambda\\rightarrow\\infty$, variance ‚¨á\n\nYou should think about why.\n\n\n\n## Can we get the best of both worlds?\n\nTo recap:\n\n* Deciding which predictors to include, adding quadratic terms, or interactions is [model selection]{.secondary} (more precisely variable selection within a linear model).\n\n* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  \n\n* If the LM is **true**, \n    1. OLS is unbiased, but Variance depends on $\\mathbf{D}^{-2}$. Can be big.\n    2. Ridge is biased (can you find the bias?). But Variance is smaller than OLS.\n\n* Ridge regression does not perform variable selection.\n\n* But [picking]{.hand} $\\lambda=3.7$ and thereby [deciding]{.hand} to predict with $\\widehat{\\beta}^R_{3.7}$ is [model selection]{.secondary}.\n\n\n\n## Can we get the best of both worlds?\n\nRidge regression \n: $\\minimize \\frac{1}{2n}\\Vert\\y-\\X\\beta\\Vert_2^2 \\ \\st\\ \\snorm{\\beta}_2^2 \\leq s$ \n\nBest (in-sample) linear regression model of size $s$\n: $\\minimize \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2 \\ \\st\\ \\snorm{\\beta}_0 \\leq s$\n\n\n$||\\beta||_0$ is the number of nonzero elements in $\\beta$\n\nFinding the best in-sample linear model (of size $s$, among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\n\nRidge regression is convex (easy to solve), but doesn't do __variable__ selection\n\nCan we somehow \"interpolate\" to get both?\n\n\nNote: selecting $\\lambda$ is still __model__ selection, but we've included __all__ the variables.\n\n\n## Ridge theory\n\nRecalling that $\\beta^\\top_*x$ is the best linear approximation to $f_*(x)$\n\nIf $\\norm{x}_\\infty< r$,  [@HsuKakade2014],\n$$R(\\hat\\beta_\\lambda) - R(\\beta_*) \\leq \\left(1+ O\\left(\\frac{1+r^2/\\lambda}{n}\\right)\\right)\n\\frac{\\lambda\\norm{\\beta_*}_2^2}{2} + \\frac{\\sigma^2\\tr{\\Sigma}}{2n\\lambda}$$\n\n\nOptimizing over $\\lambda$, and setting $B=\\norm{\\beta_*}$ gives\n\n$$R(\\hat\\beta_\\lambda) - R(\\beta_*) \\leq \\sqrt{\\frac{\\sigma^2r^2B^2}{n}\\left(1+O(1/n)\\right)} + \nO\\left(\\frac{r^2B^2}{n}\\right)$$\n\n\n$$\\inf_{\\hat\\beta}\\sup_{\\beta_*} R(\\hat\\beta) - R(\\beta_*) \\geq C\\sqrt{\\frac{\\sigma^2r^2B^2}{n}}$$\n\n## Ridge theory\n\nWe call this behavior _rate minimax_: essential meaning, \n$$R(\\hat\\beta) - R(\\beta_*) = O\\left(\\inf_{\\hat\\beta}\\sup_{\\beta_*} R(\\hat\\beta) - R(\\beta_*)\\right)$$\n\nIn this setting, Ridge regression does as well as we could hope, up to constants.\n\n## Bayes interpretation\n\nIf \n\n1. $Y=X'\\beta + \\epsilon$, \n2. $\\epsilon\\sim N(0,\\sigma^2)$ \n3. $\\beta\\sim N(0,\\tau^2 I_p)$,\n\nThen, the posterior mean (median, mode) is the ridge estimator with $\\lambda=\\sigma^2/\\tau^2$.\n\n\n# Lasso\n\n## Geometry\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(mvtnorm)\nnormBall <- function(q = 1, len = 1000) {\n  tg <- seq(0, 2 * pi, length = len)\n  out <- data.frame(x = cos(tg)) %>%\n    mutate(b = (1 - abs(x)^q)^(1 / q), bm = -b) %>%\n    gather(key = \"lab\", value = \"y\", -x)\n  out$lab <- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipseData <- function(n = 100, xlim = c(-2, 3), ylim = c(-2, 3),\n                        mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  df <- expand.grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)\n  )\n  df$z <- dmvnorm(df, mean, Sigma)\n  df\n}\n\nlballmax <- function(ed, q = 1, tol = 1e-6) {\n  ed <- filter(ed, x > 0, y > 0)\n  for (i in 1:20) {\n    ff <- abs((ed$x^q + ed$y^q)^(1 / q) - 1) < tol\n    if (sum(ff) > 0) break\n    tol <- 2 * tol\n  }\n  best <- ed[ff, ]\n  best[which.max(best$z), ]\n}\n\nnb <- normBall(1)\ned <- ellipseData()\nbols <- data.frame(x = 1, y = 1)\nbhat <- lballmax(ed, 1)\nggplot(nb, aes(x, y)) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  geom_label(\n    data = bols, mapping = aes(label = bquote(\"hat(beta)[ols]\")), parse = TRUE,\n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  geom_label(\n    data = bhat, mapping = aes(label = bquote(\"hat(beta)[s]^L\")), parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )\n```\n\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/ball-plotting-functions-1.svg){fig-align='center'}\n:::\n:::\n\n\n## $\\ell_1$-regularized regression\n\nKnown as \n\n* \"lasso\"\n* \"basis pursuit\"\n\nThe estimator satisfies\n\n$$\\hat\\beta_s = \\argmin_{ \\snorm{\\beta}_1 \\leq s}  \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2$$\n\n\nIn its corresponding Lagrangian dual form:\n\n$$\\hat\\beta_\\lambda = \\argmin_{\\beta} \\frac{1}{2n}\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1$$\n\n\n## Lasso\n\nWhile the ridge solution can be easily computed \n\n$$\\argmin_{\\beta} \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_2^2 = (\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1} \\X^{\\top}\\y$$\n\n\nthe lasso solution\n\n\n$$\\argmin_{\\beta} \\frac 1n\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1 = \\; ??$$\n\ndoesn't have a closed-form solution.\n\n\nHowever, because the optimization problem is convex, there exist efficient algorithms for computing it\n\n::: aside\nThe best are Iterative Soft Thresholding or Coordinate Descent. Gradient Descent doesn't work very well in practice.\n:::\n\n\n## Coefficient path: ridge vs lasso\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(glmnet)\ndata(prostate, package = \"ElemStatLearn\")\nX <- prostate |> dplyr::select(-train, -lpsa) |>  as.matrix()\nY <- prostate$lpsa\nlasso <- glmnet(x = X, y = Y) # alpha = 1 by default\nridge <- glmnet(x = X, y = Y, alpha = 0)\nop <- par()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Additional intuition for why Lasso selects variables\n\nSuppose, for a particular $\\lambda$, I have solutions for $\\widehat{\\beta}_k$, $k = 1,\\ldots,j-1, j+1,\\ldots,p$.\n\nLet $\\widehat{\\y}_{-j} = \\X_{-j}\\widehat{\\beta}_{-j}$, and assume WLOG $\\overline{\\X}_k = 0$, $\\X_k^\\top\\X_k = 1\\ \\forall k$\n\nOne can show that:\n\n$$\n\\widehat{\\beta}_j = S\\left(\\mathbf{X}^\\top_j(\\y - \\widehat{\\y}_{-j}),\\ \\lambda\\right).\n$$\n\n$$\nS(z, \\gamma) = \\textrm{sign}(z)(|z| - \\gamma)_+ = \\begin{cases} z - \\gamma & z > \\gamma\\\\\nz + \\gamma & z < -\\gamma \\\\ 0 & |z| \\leq \\gamma \\end{cases}\n$$\n\n* Iterating over this is called [coordinate descent]{.secondary} and gives the solution\n\n::: aside\nSee for example, <https://doi.org/10.18637/jss.v033.i01>\n:::\n\n\n::: notes\n* If I were told all the other coefficient estimates.\n* Then to find this one, I'd shrink when the gradient is big, or set to 0 if it\ngets too small.\n:::\n\n## `{glmnet}` version (same procedure for lasso or ridge)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1|2|3|4|5|\"}\nlasso <- cv.glmnet(X, Y) # estimate full model and CV no good reason to call glmnet() itself\n# 2. Look at the CV curve. If the dashed lines are at the boundaries, redo and adjust lambda\nlambda_min <- lasso$lambda.min # the value, not the location (or use lasso$lambda.1se)\ncoeffs <- coefficients(lasso, s = \"lambda.min\") # s can be string or a number\npreds <- predict(lasso, newx = X, s = \"lambda.1se\") # must supply `newx`\n```\n:::\n\n\n* $\\widehat{R}_{CV}$ is an estimator of $R_n$, it has bias and variance\n* Because we did CV, we actually have 10 $\\widehat{R}$ values, 1 per split.\n* Calculate the mean (that's what we've been using), but what about SE?\n\n##\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regularization-lm_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Other flavours\n\nThe elastic net\n: generally used for correlated variables that\ncombines a ridge/lasso penalty.  Use `glmnet(..., alpha = a)` (0 < `a` < 1). \n\nGrouped lasso\n: where variables are included or excluded in groups. Required for factors (1-hot encoding)\n\nRelaxed lasso\n: Takes the estimated model from lasso and fits the full least squares solution on the selected covariates (less bias, more variance). Use `glmnet(..., relax = TRUE)`.\n\nDantzig selector\n: a slightly modified version of the lasso\n\n## Lasso cinematic universe\n\n::: flex\n::: w-60\n\nSCAD\n: a non-convex version of lasso that adds a more severe variable selection penalty\n\n$\\sqrt{\\textrm{lasso}}$\n: claims to be tuning parameter free (but isn't).  Uses $\\Vert\\cdot\\Vert_2$\ninstead of $\\Vert\\cdot\\Vert_1$ for the loss.\n\nGeneralized lasso\n: Adds various additional matrices to the penalty term (e.g. $\\Vert D\\beta\\Vert_1$).  \n\nArbitrary combinations\n: combine the above penalties in your favourite combinations\n:::\n\n::: w-40\n\n![](https://sportshub.cbsistatic.com/i/2022/08/10/d348f903-585f-4aa6-aebc-d05173761065/brett-goldstein-hercules.jpg)\n\n:::\n:::\n\n## Warnings on regularized regression\n\n1. This isn't a method unless you say how to choose $\\lambda$.\n1. The intercept is never penalized. Adds an extra degree-of-freedom.\n1. Predictor scaling is [very]{.secondary} important.\n1. Discrete predictors need groupings.\n1. Centering the predictors may be necessary\n1. (These all work with other likelihoods.)\n\n. . .\n\nSoftware handles most of these automatically, but not always. (No Lasso with factor predictors.)\n\n## Lasso theory under strong conditions {.smaller}\n\n[Support recovery:]{.tertiary} [@Wainwright2009], see also [@MeinshausenBuhlmann2006; @ZhaoYu2006]\n\n1. The truth is linear.\n2. $\\norm{\\X'_{S^c}\\X_S (\\X'_S\\X_S)^{-1}}_\\infty < 1-\\epsilon.$\n3. $\\lambda_{\\min} (\\X'_S\\X_S) \\geq C_{\\min} > 0$.\n4. The columns of $\\X$ have 2-norm $n$.\n5. The noise is iid Normal.\n6. $\\lambda_n$ satisfies $\\frac{n\\lambda^2}{\\log(p-s)} \\rightarrow \\infty$.\n7. $\\min_j \\{ |\\beta_j| : j \\in S\\} \\geq \\rho_n > 0$ and \n$$\\rho_n^{-1} \\left( \\sqrt{\\frac{\\log s}{n}}+ \\lambda_n\\norm{(\\X'_S\\X_S)^{-1}}_\\infty \\right)\\rightarrow 0$$\n\n\nThen, $P(\\textrm{supp}(\\hat\\beta_\\lambda) = \\textrm{supp}(\\beta_*))\\rightarrow 1$.\n\n## Lasso theory under strong conditions {.smaller}\n\n[Estimation consistency:]{.tertiary} [@negahban2010unified] also [@MeinshausenYu2009]\n\n1. The truth is linear.\n2. $\\exists \\kappa$ such that for all vectors $\\theta\\in\\R^p$ that satisfy \n$\\norm{\\theta_{S^C}}_1 \\leq 3\\norm{\\theta_S}_1$, we have $\\norm{X\\theta}_2^2/n \\geq \\kappa\\norm{\\theta}_2^2$ (Compatibility)\n3. The columns of $\\X$ have 2-norm $n$.\n4. The noise is iid sub-Gaussian.\n5. $\\lambda_n >4\\sigma \\sqrt{\\log (p)/n}$.\n\nThen, with probability at least $1-c\\exp(-c'n\\lambda_n^2)$,  \n$$\\norm{\\hat\\beta_\\lambda-\\beta_*}_2^2 \\leq \\frac{64\\sigma^2}{\\kappa^2}\\frac{s\\log p}{n}.$$\n\n::: {.callout-important}\nThese conditions are very strong, uncheckable in practice, unlikely to be true for real datasets. But theory of this type is the standard for these procedures.\n:::\n\n## Lasso under weak / no conditions\n\nIf $Y$ and $X$ are bounded by $B$, then with probability at least $1-\\delta^2$,\n$$R_n(\\hat\\beta_\\lambda) - R_n(\\beta_*) \\leq \\sqrt{\\frac{16(t+1)^4B^2}{n}\\log\\left(\\frac{\\sqrt{2}p}{\\delta}\\right)}.$$\n\n\nThis is a simple version of a result in [@GreenshteinRitov2004].\n\nNote that it applies to the constrained version.\n\n[@bartlett2012] derives the same rate for the Lagrangian version\n\nAgain, this rate is (nearly) optimal:\n$$c\\sqrt{\\frac{s}{n}} < R_n(\\hat\\beta_\\lambda) - R_n(\\beta_*) < C\\sqrt{\\frac{s\\log p}{n}}.$$\n\n\n$\\log p$ is the penalty you pay for selection.\n\n\n\n\n## References",
    "supporting": [
      "regularization-lm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}