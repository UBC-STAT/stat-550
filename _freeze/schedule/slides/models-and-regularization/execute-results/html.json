{
  "hash": "13092bb9fe16c1a3717d29793c5265f2",
  "result": {
    "markdown": "---\nlecture: \"Statistical models and model selection\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\nbibliography: booth-refs.bib\n---\n## {{< meta lecture >}} {.large background-image=\"img/consult.jpeg\" background-opacity=\"0.3\"}\n\n[Stat 550]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 01 April 2024\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\mid}\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\renewcommand{\\hat}{\\widehat}\n$$\n\n\n\n\n## What is a model?\n\nIn statistics, \"model\" has a mathematical meaning.\n\nDistinct from \"algorithm\" or \"procedure\".\n\nDefining a model often leads to a procedure/algorithm with good properties.\n\nSometimes procedure/algorithm $\\Rightarrow$ a specific model.\n\n> Statistics (the field) tells me how to understand when different procedures\n> are desirable and the mathematical guarantees that they satisfy.\n\nWhen are certain models appropriate?\n\n> One definition of \"Statistical Learning\" is the \"statistics behind the procedure\".\n\n## Statistical models 101\n\nWe observe data $Z_1,\\ Z_2,\\ \\ldots,\\ Z_n$ generated by some probability\ndistribution $P$. We want to use the data to learn about $P$. \n\n> A [statistical model]{.secondary} is a set of distributions $\\mathcal{P}$.\n\n\nSome examples:\n\n  1. $\\P = \\{ 0 < p < 1 : P(z=1)=p,\\ P(z=0)=1-p\\}$.\n  2. $\\P = \\{ \\beta \\in \\R^p,\\ \\sigma>0 : Y \\sim N(X^\\top\\beta,\\sigma^2),\\  X\\mbox{ fixed}\\}$.\n  2. $\\P = \\{\\mbox{all CDF's }F\\}$.\n  3. $\\P = \\{\\mbox{all smooth functions } f: \\R^p \\rightarrow \\R : Z_i = (X_i, Y_i),\\ E[Y_i] = f(X_i) \\}$\n  \n## Statistical models \n\nWe want to use the data to [select]{.secondary} a distribution $P$ that probably \ngenerated the data.\n\n. . . \n\n#### My model:\n\n$$\n\\P = \\{ P(z=1)=p,\\ P(z=0)=1-p,\\ 0 < p < 1 \\}\n$$\n  \n* To completely characterize $P$, I just need to estimate $p$.\n\n* Need to assume that $P \\in \\P$. \n\n* This assumption is mostly empty: _need independent, can't see $z=12$._\n\n## Statistical models \n\nWe observe data $Z_i=(Y_i,X_i)$ generated by some probability\ndistribution $P$. We want to use the data to learn about $P$. \n\n. . . \n\n#### My model\n\n$$\n\\P = \\{ \\beta \\in \\R^p, \\sigma>0 : Y_i \\given X_i=x_i \\sim N(x_i^\\top\\beta,\\ \\sigma^2) \\}.\n$$\n\n  \n* To completely characterize $P$, I just need to estimate $\\beta$ and $\\sigma$.\n\n* Need to assume that $P\\in\\P$.\n\n* This time, I have to assume a lot more: \n_(conditional) Linearity, independence, conditional Gaussian noise,_\n_no ignored variables, no collinearity, etc._\n\n\n## Statistical models, unfamiliar example\n\nWe observe data $Z_i \\in \\R$ generated by some probability\ndistribution $P$. We want to use the data to learn about $P$. \n\n#### My model\n\n$$\n\\P = \\{ Z_i \\textrm{ has a density function } f \\}.\n$$\n\n  \n* To completely characterize $P$, I need to estimate $f$.\n\n* In fact, we can't hope to do this.\n\n\n[Revised Model 1]{.secondary} - $\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx < M \\}$\n\n[Revised Model 2]{.secondary} - $\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx < K < M \\}$\n\n[Revised Model 3]{.secondary} - $\\P=\\{ Z_i \\textrm{ has a density function } f : \\int |f'| dx <  M \\}$\n\n* Each of these suggests different ways of estimating $f$\n\n\n## Assumption Lean Regression\n\nImagine $Z = (Y, \\mathbf{X}) \\sim P$ with $Y \\in \\R$ and $\\mathbf{X} = (1, X_1, \\ldots, X_p)^\\top$.\n\nWe are interested in the _conditional_ distribution $P_{Y|\\mathbf{X}}$\n\nSuppose we think that there is _some_ function of interest which relates $Y$ and $X$.\n\nLet's call this function $\\mu(\\mathbf{X})$ for the moment. How do we estimate $\\mu$? What is $\\mu$?\n\n::: aside\nSee [Berk et al. _Assumption Lean Regression_](https://doi.org/10.1080/00031305.2019.1592781).\n:::\n\n\n. . . \n\nTo make this precise, we \n\n* Have a model $\\P$.\n* Need to define a \"good\" functional $\\mu$.\n* Let's loosely define \"good\" as\n\n> Given a new (random) $Z$, $\\mu(\\mathbf{X})$ is \"close\" to $Y$.\n\n## Evaluating \"close\"\n\nWe need more functions.\n  \nChoose some _loss function_  $\\ell$ that measures how close $\\mu$ and $Y$ are.\n\n\n::: flex\n\n::: w-50\n\n* _Squared-error:_   \n$\\ell(y,\\ \\mu) = (y-\\mu)^2$\n\n* _Absolute-error:_  \n$\\ell(y,\\ \\mu) = |y-\\mu|$\n\n* _Zero-One:_         \n$\\ell(y,\\ \\mu) = I(y\\neq\\mu)=\\begin{cases} 0 & y=\\mu\\\\1 & \\mbox{else}\\end{cases}$\n\n* _Cauchy:_  \n$\\ell(y,\\ \\mu) = \\log(1 + (y - \\mu)^2)$\n\n:::\n\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](models-and-regularization_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n## Start with (Expected) Squared Error\n\nLet's try to minimize the _expected_ squared error (MSE).\n\nClaim: $\\mu(X) = \\Expect{Y\\ \\vert\\ X}$ minimizes MSE.\n\nThat is, for any $r(X)$, $\\Expect{(Y - \\mu(X))^2} \\leq \\Expect{(Y-r(X))^2}$.\n\n\n. . .\n\nProof of Claim:\n\n\n\\begin{aligned}\n\\Expect{(Y-r(X))^2} \n&= \\Expect{(Y- \\mu(X) + \\mu(X) - r(X))^2}\\\\\n&= \\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2\\Expect{(Y- \\mu(X))(\\mu(X) - r(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2(\\mu(X) - r(X))\\Expect{(Y- \\mu(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} + 0\\\\\n&\\geq \\Expect{(Y- \\mu(X))^2}\n\\end{aligned}\n\n\n\n\n## The regression function\n\nSometimes people call this solution:\n\n\n$$\\mu(X) = \\Expect{Y \\ \\vert\\  X}$$\n\n\nthe regression function. (But don't forget that it depended on $\\ell$.)\n\nIf we [assume]{.secondary} that $\\mu(x) = \\Expect{Y \\ \\vert\\  X=x} = x^\\top \\beta$, then we get back exactly OLS.\n\n. . .\n\nBut why should we assume $\\mu(x) = x^\\top \\beta$?\n\n\n## Brief aside {background-color=\"#97D4E9\"}\n\nSome notation / terminology\n\n* \"Hats\" on things mean \"estimates\", so $\\widehat{\\mu}$ is an estimate of $\\mu$\n\n* Parameters are \"properties of the model\", so $f_X(x)$ or $\\mu$ or $\\Var{Y}$\n\n* Random variables like $X$, $Y$, $Z$ may eventually become data, $x$, $y$, $z$, once observed.\n\n* \"Estimating\" means \"using observations to estimate _parameters_\"\n\n* \"Predicting\" means \"using observations to predict _future data_\"\n\n* Often, there is a parameter whose estimate will provide a prediction.\n\n* \"Non-parametric\" means \"we donâ€™t assume a parametric form\" for the regression function (or density)\n\n## Estimation vs. Prediction\n\n* In consulting, you're usually interested in estimating parameters accurately.\n\n* This is a departure from machine learning, when you want to predict new data.\n\n* But to \"select a model\", we may have to choose between plausible alternatives.\n\n* This can be challenging to understand.\n\n## Prediction risk for regression\n\nGiven the _training data_ $\\mathcal{D}$, we\nwant to predict some independent _test data_\n$Z = (X,Y)$\n\nThis means forming a $\\hat f$, which is a function of both the range of\n$X$ and the training data $\\mathcal{D}$, which provides predictions\n$\\hat Y = \\hat f(X)$.\n\n\nThe quality of this prediction is measured via the prediction risk\n$$R(\\hat{f}) = \\Expect{(Y - \\hat{f}(X))^2}.$$\n\nWe know that the _regression function_,\n$\\mu(X) = \\mathbb{E}[Y \\mid X]$, is the best possible predictor.\n\n## Model selection and tuning parameters\n\n* Often \"model selection\" means \"choosing a set of predictors/variables\"\n    - E.g. Lasso performs model selection by setting many $\\widehat\\beta=0$\n* \"Model selection\" [really]{.secondary} means\n> making any necessary decisions to arrive at a final model\n* Sometimes this means \"choosing predictors\"\n* It could also mean \"selecting a tuning parameter\"\n* Or \"deciding whether to use LASSO or Ridge\" (and picking tuning parameters)\n* Model selection means \"choose $\\mathcal{P}$\"\n \n\n## My pet peeve\n\n* Often people talk about \"using LASSO\" or \"using an SVM\"\n* This isn't quite right.\n* LASSO is a regularized procedure that depends on $\\lambda$\n* To \"use LASSO\", you must pick a particular $\\lambda$\n* Different ways to pick $\\lambda$ (today's topic) produce different final \nestimators\n* Thus we should say \"I used LASSO + CV\" or \"I used Ridge + GCV\"\n* Probably also indicate \"how\" (I used the CV minimum.)\n\n## Bias and variance\n\nRecall that $\\mathcal{D}$ is the training data.\n\n\n$$R_n(f) := \\Expect{L(Y,f(X))} = \\Expect{\\Expect{L(Y,f(X)) \\given \\mathcal{D}}}$$\n\n\n* Note the difference between \n$R_n(f)\\;\\;\\textrm{and}\\;\\;\\Expect{L(Y,f(X)) \\given \\mathcal{D}}$\n* If you use $\\mathcal{D}$ to choose $f$, then these are different.\n* If you use $\\mathcal{D}$ to choose $f$, then both depend on how much data you have seen.\n\n\n\n## Risk estimates\n\n![[@HastieTibshirani2009]](gfx/bias-var.jpg)\n\n* We can use risk estimates for 2 different goals\n\n1. Choosing between different potential models.\n2. Characterizing the out-of-sample performance of the chosen model.\n\n* I am not generally aware of other methods of accomplishing (1).\n\n## A model selection picture\n\n![[@HastieTibshirani2009]](gfx/model-space.jpg)\n\n## Why?\n\nWe want to do model selection for at least three reasons:\n\nPrediction accuracy\n: Can essentially *always* be improved by introducing some bias\n\nInterpretation\n: A large number of features can sometimes be reduced to an interpretable subset\n\nComputation\n: A large $p$ can create a huge computational bottleneck.\n\n## Things you shouldn't do\n\n* Estimate $R_n$ with $\\widehat{R}_n(f) = \\sum_{i=1}^n L(Y_i,\\widehat{f}(X_i))$.\n* Throw away variables with small $p$-values.\n* Use $F$-tests\n* Compare the log-likelihood between different models\n\n> These last two can occasionally be ok, but aren't in general. You should investigate the assumptions that are implicit in them.\n\n# Risk estimators\n\n## Unbiased risk estimation\n\n* It is very hard (impossible?) to estimate $R_n$.\n* Instead we focus on \n\n$$\\overline{R}_n(f) = \\E_{Y_1,\\ldots,Y_n}\\left[\\E_{Y^0}\\left[\\frac{1}{n}\\sum_{i=1}^n L(Y^0_i,\\hat{f}(x_i))\\given \\mathcal{D}\\right]\\right].$$\n\n* The difference is that $\\overline{R}_n(f)$ averages over the observed $x_i$ rather than taking the expected value over the distribution of $X$.\n* In the \"fixed design\" setting, these are equal.\n\n## Unbiased risk estimation\n\nFor many $L$ and some predictor $\\hat{f}$, one can show\n\n$$\\overline{R}_n(\\hat{f}) = \\Expect{\\hat{R}_n(\\hat{f})} + \\frac{2}{n} \\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}.$$\n\n\nThis suggests estimating $\\overline{R}_n(\\hat{f})$ with\n\n$$\\hat{R}_{\\textrm{gic}} := \\hat{R}_n(\\hat{f}) + \\textrm{pen}.$$\n\n\nIf $\\Expect{\\textrm{pen}} = \\frac{2}{n}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}$, we have an unbiased estimator of $\\overline{R}_n(\\hat{f})$.\n\n\n## Normal means model\n\n\nSuppose we observe the following data:\n\n$$Y_i = \\beta_i + \\epsilon_i, \\quad\\quad i=1,\\ldots,n$$\n\nwhere $\\epsilon_i\\overset{iid}{\\sim} \\mbox{N}(0,1)$.\n  \n  \nWe want to estimate \n$$\\boldsymbol{\\beta} = (\\beta_1,\\ldots,\\beta_n).$$\n\n\nThe usual estimator (MLE) is $$\\widehat{\\boldsymbol{\\beta}}^{MLE} = (Y_1,\\ldots,Y_n).$$\n\n\nThis estimator has lots of nice properties: __consistent, unbiased, UMVUE, (asymptotic) normality...__\n\n## MLEs are bad\n\n  \nBut, the standard estimator __STINKS!__ It's a bad estimator. \n  \nIt has no bias, but big variance.\n\n\n$$R_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = \\mbox{bias}^2 + \\mbox{var} = 0\n+ n\\cdot 1= n$$\n\n\nWhat if we use a biased estimator?\n\nConsider the following estimator instead:\n$$\\widehat{\\beta}_i^S = \\begin{cases} Y_i & i \\in S\\\\ 0 & \\mbox{else}. \\end{cases}$$\n\n  \nHere $S \\subseteq \\{1,\\ldots,n\\}$. \n\n\n## Biased normal means\n\n\nWhat is the risk of this estimator?\n\n$$\nR_n(\\widehat{\\boldsymbol{\\beta}}^S) = \\sum_{i\\not\\in S} \\beta_i^2 + |S|.\n$$\n\nIn other words, if some $|\\beta_i| < 1$, then don't bother estimating them!\n\nIn general, introduced parameters like $S$ will be called __tuning parameters__.\n\nOf course we don't know which $|\\beta_i| < 1$.\n\nBut we could try to estimate $R_n(\\widehat{\\boldsymbol{\\beta}}^S)$, and choose $S$ to minimize our estimate.\n\n\n\n\n## Dangers of using the training error\n\n\nAlthough\n\n$$\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}})  \\approx R_n(\\widehat{\\boldsymbol{\\beta}}),$$\nthis approximation can be very bad.  In fact:\n\n\nTraining Error\n: $\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = 0$\n\nRisk\n: $R_n(\\widehat{\\boldsymbol{\\beta}}^{MLE}) = n$\n\nIn this case, the  __optimism__ of the training error is $n$. \n\n\n## Normal means\n\nWhat about $\\widehat{\\boldsymbol{\\beta}}^S$?\n\n$$\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S) = \\sum_{i=1}^n (\\widehat{\\beta_i}-\n  Y_i)^2 = \\sum_{i \\notin S} Y_i^2 %+ |S|\\sigma^2$$\n\nWell\n  $$\\E\\left[\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S)\\right] =\n  R_n(\\widehat{\\boldsymbol{\\beta}}^S) - 2|S| +n.$$\n\nSo I can choose $S$ by minimizing $\\widehat{R}_n(\\widehat{\\boldsymbol{\\beta}}^S) + 2|S|$. \n  \n\n$$\\mbox{Estimate of Risk} = \\mbox{training error} + \\mbox{penalty}.$$\n  \nThe penalty term corrects for the optimism.\n\n\n\n\n## `pen()` in the nice cases\n\n__Result:__  \nSuppose $\\hat{f}(x_i) = HY$ for some matrix $H$, and $Y_i$'s are IID. Then \n\n$$\\frac{2}{n} \\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)} = \\frac{2}{n} \\sum_{i=1}^n H_{ii} \\Cov{Y_i}{Y_i} = \\frac{2\\Var{Y}}{n} \\tr{H}.$$\n\n* Such estimators are called \"linear smoothers\".\n* Obvious extension to the heteroskedastic case.\n* We call $\\frac{1}{\\Var{Y}}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}$ the [degrees of freedom]{.secondary} of $\\hat{f}$.\n* Linear smoothers are ubiquitous\n* Examples: OLS, ridge regression, KNN, dictionary regression, smoothing splines, kernel regression, etc.\n\n\n## Examples of DF\n\n* OLS\n\n$$H = X^\\top (X^\\top X)^{-1} X^\\top \\Rightarrow \\tr{H} = \\textrm{rank}(X) = p$$\n\n* Ridge (decompose $X=UDV^\\top$)\n\n$$H = X^\\top (X^\\top X + \\lambda I_p)^{-1} X^\\top \\Rightarrow \\tr{H} = \\sum_{i=1}^p \\frac{d_i^2}{d_i^2 + \\lambda} < \\min\\{p,n\\}$$\n\n\n* KNN $\\textrm{df} = n/K$ (each point is it's own nearest neighbor, it gets weight $1/K$)\n\n\n## Finding risk estimators\n\nThis isn't the way everyone introduces/conceptualizes prediction risk.\n\nFor me, thinking of $\\hat{R}_n$ as\noverly optimistic and correcting for that\noptimism is conceptually appealing\n\nAn alternative approach is to discuss [information criteria]{.secondary}.\n\nIn this case one forms a (pseudo)-metric on probability measures.\n\n\n# Comparing probability measures\n\n## Kullback&ndash;Leibler\n\nSuppose we have data $Y$ that comes from the probability density\nfunction $f$.\n\n\nWhat happens if we use the probability density function $g$ instead?\n\n\nExample\n: Suppose\n$Y \\sim N(\\mu,\\sigma^2) =: f$. We want to predict a new $Y_*$, but we\nmodel it as $Y_* \\sim N(\\mu_*,\\sigma^2) =: g$\n\nHow far away are we? We can either compare\n$\\mu$ to $\\mu_*$ or $Y$ to $Y^*$.\n\nOr, we can compute how far $f$ is from $g$.\n\nWe need a notion of distance.\n\n## Kullback&ndash;Leibler\n\n[Kullback&ndash;Leibler] divergence (or discrepancy)\n\n$$\\begin{aligned}\nKL(f\\;\\Vert\\; g) & = \\int \\log\\left( \\frac{f(y)}{g(y)} \\right)f(y) dy \\\\\n& \\propto\n-\\int \\log (g(y)) f(y) dy \\qquad \\textrm{(ignore term without $g$)}\\\\\n& = \n-\\mathbb{E}_f [\\log (g(Y))] \\end{aligned}$$\n\n* Measures the loss incurred\nby (incorrectly) using $g$ instead of $f$.\n\n* KL is not symmetric: $KL(f\\;\\Vert\\; g) \\neq KL(g\\;\\Vert\\; f)$, so it's not a distance, but it is non-negative and satisfies the triangle inequality.\n\n* Usually, $f,\\ g$ will depend on some parameters, call them $\\theta$\n\n\n## KL example\n\n\n* In regression, we can specify $f = N(X^{\\top} \\beta_*, \\sigma^2)$ \n* for a fixed (true) $\\beta_*$, \n* let $g_\\theta = N(X^{\\top}\\beta,\\sigma^2)$ over all $\\theta = (\\beta,\\sigma^2) \\in \\mathbb{R}^p\\times\\mathbb{R}^+$\n* $KL(f,g_\\theta) = -\\mathbb{E}_f [\\log (g_\\theta(Y))]$, we want to minimize\nthis over $\\theta$.\n* But $f$ is unknown, so we minimize $-\\log (g_\\theta(Y))$\ninstead. \n* This is the maximum likelihood value\n$$\\hat{\\theta}_{ML} = \\argmax_\\theta g_\\theta(Y)$$\n* We don't actually need to assume things about a true model nor have it be nested in\nthe alternative models to make this work.\n\n## Operationalizing\n\n* Now, to get an operational characterization of the KL divergence at the\nML solution $$-\\mathbb{E}_f [\\log (g_{\\hat\\theta_{ML}}(Y))]$$ we need an\napproximation (don't know $f$, still).\n\n\nResult\n:  If you maximize the likelihood for a finite dimensional parameter vector $\\theta$ of length $p$, then as $n\\rightarrow \\infty$, $$-\\mathbb{E}_f [\\log (g_{\\hat\\theta_{ML}}(Y))] = -\\log (g_{\\hat\\theta_{ML}}(Y)) + p.$$\n\n* This is AIC (originally \"an information criterion\", now \"Akaike's information criterion\").\n\n## AIC warnings\n\n* Choose the model with smallest AIC\n* Often multiplied by 2 \"for historical reasons\". \n* Sometimes by $-2$ \"to be extra annoying\".\n* Your estimator for $\\theta$ needs to be the MLE. (or the asymptotics may be wrong)\n* $p$ includes all estimated parameters.\n\n## Back to the OLS example\n\nSuppose $Y$ comes from the standard normal linear regression model with [known]{.secondary} variance $\\sigma^2$. \n\n$$\n\\begin{aligned}\n-\\log(g_{\\hat{\\theta}}) &\\propto \\log(\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - x_i^\\top \\hat{\\beta}_{MLE})^2\\\\ \\Rightarrow AIC &= 2\\frac{n}{2\\sigma^2}\\hat{R}_n + 2p = \\hat{R}_n + \\frac{2\\sigma^2}{n} p.\n\\end{aligned}\n$$\n\n\n## Back to the OLS example\n\nSuppose $Y$ comes from the standard normal linear regression model with [unknown]{.secondary} variance $\\sigma^2$. \n\nNote that $\\hat{\\sigma}_{MLE}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i-x_i^\\top\\hat{\\beta}_{MLE})^2$. \n\n$$\n\\begin{aligned}\n-\\log(g_{\\hat{\\theta}}) &\\propto \\frac{n}{2}\\log(\\hat{\\sigma}^2) + \\frac{1}{2\\hat{\\sigma^2}}\\sum_{i=1}^n (y_i - x_i^\\top \\hat{\\beta}_{MLE})^2\\\\ \\Rightarrow AIC &\\propto 2 n\\log(\\hat{\\sigma}^2)/2 + 2(p+1) \\propto \\log(\\hat{R}_n) + \\frac{2(p+1)}{n}.\n\\end{aligned}\n$$\n\n\n## Mallow's Cp\n\n* Defined for linear regression.\n* No likelihood assumptions.\n* Variance is known\n\n$$C_p = \\hat{R}_n + 2\\sigma^2 \\frac{\\textrm{df}}{n} = AIC$$\n\n\n## Bayes factor\n\nFor Bayesian Analysis, we want the posterior. Suppose we have two models A and B.\n\n$$\n\\begin{aligned}\nP(B\\given \\mathcal{D}) &= \\frac{P(\\mathcal{D}\\given B)P(B)}{P(\\mathcal{D})} \n\\propto P(\\mathcal{D}\\given B)P(B)\\\\\nP(A\\given \\mathcal{D}) &= \\frac{P(\\mathcal{D}\\given A)P(A)}{P(\\mathcal{D})} \n\\propto P(\\mathcal{D}\\given A)P(A)\n\\end{aligned}\n$$\nWe assume that $P(A) = P(B)$. Then to compare, \n$$\n\\frac{P(B\\given \\mathcal{D})}{P(A\\given \\mathcal{D})} = \\frac{P(\\mathcal{D}\\given B)}  {P(\\mathcal{D}\\given A)}.\n$$\n\n* Called the [Bayes Factor]{.secondary}.\n* This is the ratio of marginal likelihoods under the different models. \n\n## Bayes Factor\n\n* Not easy to calculate generally. ()\n* Use the Laplace approximation, some simplifications, assumptions:\n\n$$\\log P(\\mathcal{D}\\given B) = \\log P(\\mathcal{D} \\given \\hat{\\theta},\\ B) -\\frac{p\\log(n)}{2} + O(1)\n$$\n\n* Multiply through by $-2$:\n$$\nBIC = -\\log (g_\\theta(Y)) + p\\log(n) = \\log(\\hat{R}_n) + \\frac{p\\log(n)}{n}\n$$\n\n* Also called Schwarz IC. Compare to AIC (variance unknown case)\n\n## SURE\n\n\n$$\\hat{R}_{gic} := \\hat{R}_n(\\hat{f}) + \\textrm{pen}.$$\n\nIf $\\Expect{\\textrm{pen}} = \\frac{2}{n}\\sum_{i=1}^n \\Cov{Y_i}{\\hat{f}(x_i)}$, we have an unbiased estimator of $\\overline{R}_n(\\hat{f})$.\n\nResult (Stein's Lemma)  \n: Suppose $Y_i\\sim N(\\mu_i,\\sigma^2)$ and suppose $f$ is weakly differentiable. Then\n\n$$\\frac{1}{\\sigma^2} \\sum_{i=1}^n\\Cov{Y_i}{\\hat{f}_i(Y)} = \\Expect{\\sum_{i=1}^n \\frac{\\partial f_i}{\\partial y_i} \\hat{f}(Y)}.$$\n\n* Note: Here I'm writing $\\hat{f}$ as a function of $Y$ rather than $x$. \n\n## SURE\n\n* This gives \"Stein's Unbiased Risk Estimator\"\n\n$$SURE = \\hat{R}_n(\\hat{f}) + 2\\sigma^2 \\sum_{i=1}^n \\frac{\\partial f_i}{\\partial y_i} \\hat{f}(Y) - n\\sigma^2.$$\n\n* If $f(Y) = HY$ is linear, we're back to AIC (variance known case)\n\n* If $\\sigma^2$ is unknown, may not be unbiased anymore. May not care.\n\n# CV\n\n\n## Intuition for CV\n\n\nOne reason that $\\widehat{R}_n(\\widehat{f})$ is bad is that we are using the same data to pick $\\widehat{f}$ __AND__ to estimate $R_n$.\n\n\"Validation set\" fixes this, but holds out a particular, fixed block of data we pretend mimics the \"test data\"\n\n\nWhat if we set aside one observation, say the first one $(y_1, x_1)$.\n\nWe estimate $\\widehat{f}^{(1)}$ without using the first observation.\n\nThen we test our prediction:\n\n$$\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.$$\n\n\n(why the notation $\\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )\n\n\n## Keep going\n\nBut that was only one data point $(y_1, x_1)$. Why stop there?\n\nDo the same with $(y_2, x_2)$! Get an estimate $\\widehat{f}^{(2)}$ \nwithout using it, then\n\n$$\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.$$\n\nWe can keep doing this until we try it for every data point.\n\nAnd then average them! (Averages are good)\n\n\n$$\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n \n(y_i - \\widehat{f}^{(i)}(x_i))^2$$\n\n\nThis is [__leave-one-out cross validation__]{.secondary}\n\n\n## Problems with LOO-CV\n\nðŸ¤® Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.\n\nðŸ¤® The training sets overlap. This is bad. \n\n- Usually, averaging reduces variance: $\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.$\n- But only if the variables are independent. If not, then $\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.$\n- Since the training sets overlap a lot, that covariance can be pretty big.\n    \nðŸ¤® We have to estimate this model $n$ times.\n\nðŸŽ‰ Bias is low because we used almost all the data to fit the model: $E[\\mbox{LOO-CV}] = R_{n-1}$ \n\n  \n## K-fold CV\n\n::: flex\n::: w-50\nTo alleviate some of these problems, people usually use $K$-fold cross validation.\n\nThe idea of $K$-fold is \n\n1. Divide the data into $K$ groups. \n1. Leave a group out and estimate with the rest.\n1. Test on the held-out group. Calculate an average risk over these $\\sim n/K$ data.\n1. Repeat for all $K$ groups.\n1. Average the average risks.\n\n\n:::\n\n\n::: w-50\nðŸŽ‰ Less overlap, smaller covariance.\n\nðŸŽ‰ Larger hold-out sets, smaller variance.\n\nðŸŽ‰ Less computations (only need to estimate $K$ times)\n\nðŸ¤® LOO-CV is (nearly) unbiased for $R_n$\n\nðŸ¤® K-fold CV is unbiased for $R_{n(1-1/K)}$\n\nThe risk depends on how much data you use to estimate the model. $R_n$ depends on $n$.\n\n:::\n:::\n\n\n## Comparison\n\n* LOO-CV and AIC are asymptotically equivalent $p<n$, [@Stone1977]\n* Properties of AIC/BIC in high dimensions are not well understood.\n* In low dimensions, AIC is minimax optimal for the prediction risk [@YangBarron1998]\n* CV is consistent for the prediction risk [@Dudoit2005]\n* Both tend to over-select predictors (unproven, except empirically)\n* BIC asymptotically selects the correct linear model in low dimensions [@shao1997asymptotic] and in high dimensions [@wang2009shrinkage]\n\n## Comparison\n\n* In linear regression, it is impossible for a model selection criterion to be minimax optimal and select the correct model asymptotically [@yang2005can]\n* In high dimensions, if the variance is unknown, the \"known\" variance form of AIC/BIC is disastrous.\n* __Conclusion:__ your choice of risk estimator impacts results. Thus, \n    1. If you want to select models, you might pick BIC\n    2. If you want good predictions, you might use CV\n    3. It's possible LASSO + CV(1se) picks models better than LASSO + CV(min)\n    \n## Some other lessons\n\n* The unknown variance form of AIC fails in high dimensions because you can drive RSS to zero.\n* You need to use a high-dimensional variance estimator instead [@HomrighausenMcDonald2015a]\n* LASSO + CV \"works\" in high dimensions (not LOO, but no one uses it anyway)\n* Under _very_ strong conditions it selects the right model at the right rate.\n* Under weaker conditions, it achieves (nearly) minimax optimal prediction risk.\n[@HomrighausenMcDonald2013;@HomrighausenMcDonald2014;@HomrighausenMcDonald2015]\n\n## What if we don't want to choose?\n\n1. Choose a risk estimator $\\hat{R}$\n2. Calculate weights $p_i = \\exp\\left\\{-\\hat{R}(\\textrm{Model}_i)\\right\\}$\n3. Create final estimator $\\hat{f} = \\sum_{\\textrm{models}} \\frac{p_i}{\\sum p_i} \\hat{f}_i$.\n\n* If $\\hat{R}$ is BIC, this is (poor-man's) Bayesian Model Averaging.\n\n## Bayesian Model Averaging\n\n* Real BMA integrates over the models: $$P(f \\given \\mathcal{D}) = \\int P(f \\given M_i, \\mathcal{D}) P(M_i \\given \\mathcal{D}) dM$$\n* Averaging + Sparsity is pretty hard.\n* Interesting open problem: how can we combine LASSO models over the path?\n* Issue with MA: $e^{-BIC}$ can be tiny for all but a few models. You're not averaging anymore.\n\n\n## References",
    "supporting": [
      "models-and-regularization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}